{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU device count: {torch.cuda.device_count()}\")\n",
    "print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "print(f\"Device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets transformers peft nltk rouge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import BartTokenizer\n",
    "\n",
    "\n",
    "def sample_dataset(dataset, fraction=0.3, seed=42):\n",
    "    \"\"\"\n",
    "    Sample a fraction of the dataset.\n",
    "    \"\"\"\n",
    "    sampled_dataset = dataset.shuffle(seed=seed)\n",
    "    num_samples = int(len(dataset) * fraction)\n",
    "    return sampled_dataset.select(range(num_samples))\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=MAX_TARGET_LENGTH, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Create the custom dataset class and data loaders\n",
    "class MultiNewsDataset(Dataset):\n",
    "    def __init__(self, tokenized_datasets, split):\n",
    "        self.dataset = tokenized_datasets[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        return {\n",
    "            'input_ids': torch.tensor(item['input_ids']),\n",
    "            'attention_mask': torch.tensor(item['attention_mask']),\n",
    "            'labels': torch.tensor(item['labels'])\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = pad_sequence([item['input_ids'] for item in batch], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = pad_sequence([item['attention_mask'] for item in batch], batch_first=True, padding_value=0)\n",
    "    labels = pad_sequence([item['labels'] for item in batch], batch_first=True, padding_value=-100)  # Assuming -100 is ignore index for labels\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "\n",
    "MODEL_NAME = 'facebook/bart-large-cnn'\n",
    "\n",
    "# 1. Load the dataset\n",
    "dataset = load_dataset(\"multi_news\")\n",
    "\n",
    "# 2. Set up the tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 3. Preprocess the dataset\n",
    "MAX_INPUT_LENGTH = 1024\n",
    "MAX_TARGET_LENGTH = 100\n",
    "\n",
    "# Sample 10% of each split\n",
    "train_dataset = sample_dataset(dataset['train'])\n",
    "validation_dataset = sample_dataset(dataset['validation'])\n",
    "test_dataset = sample_dataset(dataset['test'])\n",
    "\n",
    "# Create a new dataset dictionary with the sampled splits\n",
    "dataset = {\n",
    "    'train': train_dataset,\n",
    "    'validation': validation_dataset,\n",
    "    'test': test_dataset\n",
    "}\n",
    "\n",
    "\n",
    "tokenized_datasets = {}\n",
    "for split in dataset.keys():\n",
    "    tokenized_datasets[split] = dataset[split].map(preprocess_function, batched=True)\n",
    "\n",
    "train_dataset = MultiNewsDataset(tokenized_datasets, 'train')\n",
    "val_dataset = MultiNewsDataset(tokenized_datasets, 'validation')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, get_linear_schedule_with_warmup, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_summarizer(MODEL_NAME):\n",
    "    # Load pre-trained model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "    return model, tokenizer\n",
    "\n",
    "def summarize_news(model, tokenizer, news_article):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer([news_article], max_length=2048, return_tensors='pt', truncation=True)\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=100, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "model, tokenizer = load_summarizer(MODEL_NAME, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune only encoder layer from 0-5\n",
    "target_modules=[]\n",
    "for i in range(0,5):\n",
    "    target_modules += [\n",
    "            f\"model.encoder.layers.{i}.self_attn.k_proj\",\n",
    "            f\"model.encoder.layers.{i}.self_attn.q_proj\",\n",
    "            f\"model.encoder.layers.{i}.self_attn.v_proj\"\n",
    "            ]\n",
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=target_modules\n",
    ")\n",
    "\n",
    "# Add LoRA adaptor\n",
    "model_lora = get_peft_model(model, lora_config)\n",
    "model_lora.print_trainable_parameters()\n",
    "\n",
    "# Move model to device\n",
    "model_lora.to(device)\n",
    "\n",
    "# Set up optimizer and scheduler\n",
    "optimizer = optim.AdamW(model_lora.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "num_epochs = 2\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=1,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Set CUDA_LAUNCH_BLOCKING for synchronous CUDA errors\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "def train_lora(model, train_dataloader, val_dataloader, optimizer, scheduler, num_epochs, device):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader, desc=\"Validation\"):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        print(f\"Average validation loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Run the training\n",
    "model_lora = train_lora(model_lora, train_dataloader, val_dataloader, optimizer, scheduler, num_epochs, device)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model_lora.save_pretrained(\"./model/bart_large_cnn_lora_finetuned_multinews_v3\")\n",
    "tokenizer.save_pretrained(\"./model/bart_large_cnn_lora_finetuned_multinews_v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge with base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Load the base model\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Load the LoRA config\n",
    "lora_config = PeftConfig.from_pretrained(\"./model/bart_large_cnn_lora_finetuned_multinews_v2\")\n",
    "\n",
    "# Load the LoRA model\n",
    "lora_model = PeftModel.from_pretrained(base_model, \"./model/bart_large_cnn_lora_finetuned_multinews_v2\")\n",
    "\n",
    "# Merge the LoRA weights with the base model\n",
    "merged_model = lora_model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "merged_model.save_pretrained(\"./model/bart_large_cnn_lora_merged_202408\")\n",
    "tokenizer.save_pretrained(\"./model/bart_large_cnn_lora_merged_202408\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Path to your saved merged model\n",
    "merged_model = \"./model/bart_large_cnn_lora_merged_202408\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(merged_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(merged_model)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to generate summary\n",
    "def generate_summary(model, tokenizer, text, max_length=400, min_length=130):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "        max_length=max_length,\n",
    "        min_length=min_length,\n",
    "    )\n",
    "\n",
    "    # Decode the generated summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Base Model\n",
    "MODEL_NAME = \"facebook/bart_large_cnn\"\n",
    "\n",
    "model_original = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)\n",
    "tokenizer_original = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model_original.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=target_modules  # Targeting only the encoder\n",
    ")\n",
    "\n",
    "\n",
    "# Add LoRA adaptor\n",
    "model_lora = get_peft_model(model_original, lora_config)\n",
    "model_lora.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_summarize = '''\n",
    "The widespread anti-immigrant riots in the United Kingdom of the past week, and the false viral claims that fueled them, may be the clearest, most direct example yet of the way unchecked misinformation on social media can produce violence and harm in the real world.\n",
    "\n",
    "Even after authorities identified a UK national as the suspect behind a series of deadly stabbings targeting children, false claims about the attacker’s name and origins continued to stoke anti-immigrant fervor and propel far-right demonstrations.\n",
    "\n",
    "The fake claims have circulated widely, particularly on X, the platform formerly known as Twitter, extremism researchers said. And police have openly blamed that misinformation for the violence that has wracked the country in recent days, with rioters throwing bricks at mosques, setting cars on fire and chanting anti-Islamic slogans while clashing with officers in riot gear.\n",
    "\n",
    "The events of the past week are hardly the only example of the link between online misinformation and politically motivated violence: From the Rohingya genocide to the attack on the US Capitol on January 6, 2021, false and misleading claims have consistently been at the center of high-profile incidences of political unrest and violence.\n",
    "\n",
    "It is a pattern that keeps repeating despite years of calls by governments and civil society groups for social media platforms to rein in inflammatory, hateful posts, as well as pledges by companies themselves to do more.\n",
    "\n",
    "A recent retreat from content moderation by some major platforms, however, suggests that the problem of violence fueled by misinformation may well get worse before it gets better.\n",
    "\n",
    "For nearly a decade, governments and civil rights groups have increasingly argued that online platforms have created enormous societal costs.\n",
    "\n",
    "Critics of social media have repeatedly accused the industry of putting corporate profits before users’ mental health, or opening the door to foreign meddling, without doing enough to shield the world from those risks.\n",
    "\n",
    "An economist might call these negative externalities – like pollution, they are byproducts of a profit-seeking business that, left unaddressed, everyone else must either learn to live with or mitigate, usually at great collective expense. The consequences tend to play out over long timeframes and with large-scale, systemic effects.\n",
    "\n",
    "Police hold back rioters near a burning police vehicle after disorder broke out on July 30, 2024, in Southport, England.\n",
    "Related article\n",
    "Elon Musk says ‘civil war is inevitable’ as UK rocked by far-right riots. He’s part of the problem\n",
    "This week, it is hard to avoid wondering whether politically motivated violence based on nothing more than bad-faith, evidence-free speculation has become a permanent fixture among social media’s various externalities, and if we are being asked to make peace with it as a condition of living in a digitally connected world.\n",
    "\n",
    "Many social media companies have invested heavily in content moderation over the years. But the industry’s recent track record hints at a bet – or perhaps a hope – that just maybe, the public will tolerate a bit more pollution.\n",
    "\n",
    "There are some signs of pushback. In the European Union, officials are looking to hold social media companies accountable for spreading misinformation under the new Digital Services Act. In the UK, the Online Safety Act could take effect as soon as this year, requiring, among other things, social media platforms to remove illegal content.\n",
    "\n",
    "And even tougher rules may be on the way as a result of the riots. “We’re going to have to look more broadly at social media after this disorder,” UK Prime Minister Keir Starmer said in a video distributed to media Friday.\n",
    "\n",
    "But punishments for online wrongdoing are already being handed out to individual perpetrators. On Friday, Jordan Parlour from Leeds, England, was sentenced to 20 months in jail after being convicted of publishing written material intended to stir racial hatred. The 28-year-old had posted the material on Facebook.\n",
    "\n",
    "The United States has lagged on platform regulation, partly due to congressional dysfunction and partly because of legal and constitutional differences that grant online platforms more freedom to manage their own websites.\n",
    "\n",
    "Still, lawmakers made some moves last month when the US Senate passed the Kids Online Safety Act, which aims to combat mental health harms for teens linked to social media.\n",
    "\n",
    "It may be tempting to dismiss social media’s role in the UK riots as merely a reflection of latent political trends or the result of activism that would have happened on other platforms anyway.\n",
    "\n",
    "But that distracts from the calculation that some platforms appear to have made: At least some of the time, some amount of misinformation-fueled violence is a reasonable cost for society to pay.\n",
    "\n",
    "Olesya Dmitracova and Kara Fox contributed reporting.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = generate_summary(model_original, tokenizer_original, text_to_summarize)\n",
    "print(\"Generated Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge model\n",
    "summary = generate_summary(model, tokenizer_original, text_to_summarize)\n",
    "print(\"Generated Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "test_dataset = sample_dataset(dataset['test'])\n",
    "test_dataset_tokenized = MultiNewsDataset(tokenized_datasets, 'test')\n",
    "\n",
    "generated_summaries = [generate_summary(model, tokenizer_original, doc) for doc in test_dataset['document']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \n",
    "generated_summaries_original = [generate_summary(model_original, tokenizer_original, doc) for doc in test_dataset['document']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with ROUGE\n",
    "rouge = load_metric('rouge',trust_remote_code=True)\n",
    "\n",
    "references = test_dataset['summary']\n",
    "result_1 = rouge.compute(predictions=generated_summaries, references=references)\n",
    "result_2 = rouge.compute(predictions=generated_summaries_original, references=references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with News Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_text = pd.read_csv('./practical_dl_final_project/scraped_texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_result = []\n",
    "summary_result_cnn = []\n",
    "for item in df_text[df_text['url'].str.contains('cnn')]['txt']:\n",
    "    summary_result.append(generate_summary(model, tokenizer_original, item))\n",
    "    summary_result_cnn.append(generate_summary(model_original, tokenizer_original, item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cnn = df_text[df_text['url'].str.contains('cnn')]['txt'].copy()\n",
    "df_cnn['summary_lora'] = summary_result\n",
    "df_cnn['summary_cnn'] = summary_result_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_result[2])\n",
    "print(summary_result_cnn[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
