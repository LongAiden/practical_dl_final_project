# Project Description
This project demonstrates fine-tuning large language models using Low-Rank Adaptation (LoRA) for text summarization tasks. LoRA reduces memory requirements while maintaining model performance by adding trainable rank decomposition matrices.

# Key Features
- PyTorch-based implementation:
    - Model: facebook/bart_large_cnn
    - Data for fine-tuning: multi_news from HuggingFace
- CUDA-enabled training
- Low-memory footprint using LoRA
- Text summarization optimization
