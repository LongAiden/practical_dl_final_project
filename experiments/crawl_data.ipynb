{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl data from Goodreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodread_df = pd.read_csv(\n",
    "    r'D:\\Online_Learning\\Practical_DL\\final_project\\books.csv', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "def fetch_data_for_isbn(isbn):\n",
    "    return get_reviews_from_isbn(url, isbn)\n",
    "\n",
    "\n",
    "def get_reviews_from_isbn(url, isbn):\n",
    "    full_url = url+f\"{isbn}\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    # Retry mechanism\n",
    "    for attempt in range(3):  # Retry up to 3 times\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                full_url, headers=headers, allow_redirects=True, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                break\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching data for ISBN {isbn}: {e}\")\n",
    "            time.sleep(2 ** attempt)  # Exponential backoff\n",
    "        else:\n",
    "            print(\n",
    "                f\"Failed to fetch data after {attempt + 1} attempts. Status Code: {response.status_code}\")\n",
    "            return pd.DataFrame()  # Return empty DataFrame if unsuccessful\n",
    "\n",
    "    # Parsing logic as before\n",
    "    reviews = soup.find_all('article', class_='ReviewCard')\n",
    "    all_reviews = []\n",
    "\n",
    "    for review in reviews:\n",
    "        # Modularize parsing into a function\n",
    "        review_data = parse_review(review, isbn)\n",
    "        all_reviews.append(review_data)\n",
    "\n",
    "    return pd.DataFrame(all_reviews)\n",
    "\n",
    "\n",
    "def parse_review(review, isbn):\n",
    "    try:\n",
    "        reviewer_name = review.find(\n",
    "            'div', {'data-testid': 'name'}).get_text(strip=True)\n",
    "        rating_section = review.find('div', class_='ShelfStatus')\n",
    "        rating = rating_section.find('span', {'role': 'img'}).get(\n",
    "            'aria-label', '').split()[1]\n",
    "        comment_section = review.find(\n",
    "            'div', {'data-testid': 'contentContainer'})\n",
    "        comment = comment_section.get_text(\n",
    "            strip=True) if comment_section else \"No comment provided\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing review for ISBN {isbn}: {e}\")\n",
    "        return {'isbn': isbn, 'reviewer': '', 'rating': '', 'comment': ''}\n",
    "\n",
    "    return {\n",
    "        'isbn': isbn,\n",
    "        'reviewer': reviewer_name,\n",
    "        'rating': rating,\n",
    "        'comment': comment\n",
    "    }\n",
    "\n",
    "\n",
    "def fetch_reviews_parallel(isbn_list):\n",
    "    reviews_df = []\n",
    "    # Reduced number of workers to lessen the load\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        future_to_isbn = {executor.submit(\n",
    "            fetch_data_for_isbn, isbn): isbn for isbn in isbn_list}\n",
    "        for future in as_completed(future_to_isbn):\n",
    "            isbn = future_to_isbn[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "                reviews_df.append(data)\n",
    "                print(f\"Data fetched for ISBN {isbn} with shape {data.shape}\")\n",
    "                time.sleep(1)  # Throttle requests\n",
    "            except Exception as exc:\n",
    "                print(f\"{isbn} generated an exception: {exc}\")\n",
    "    return reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"https://www.goodreads.com/book/isbn/\"\n",
    "isbn_list = goodread_df['isbn'].unique()\n",
    "\n",
    "reviews_dataframes = fetch_reviews_parallel(isbn_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat(reviews_dataframes)\n",
    "test = test[test['reviewer'] != \"\"]\n",
    "test.to_csv('reviews_goodread.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(273442, 4)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\n",
    "    r'D:\\Online_Learning\\Practical_DL\\final_project\\reviews_goodread.csv')\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>rating</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0439358078</td>\n",
       "      <td>Jayson</td>\n",
       "      <td>5</td>\n",
       "      <td>(A) 86%| ExtraordinaryNotes:An angsty apprehen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0439358078</td>\n",
       "      <td>Navessa</td>\n",
       "      <td>5</td>\n",
       "      <td>Seriously, don't read this review if you haven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0439358078</td>\n",
       "      <td>Diane ϟ [ Lestrange ]</td>\n",
       "      <td>5</td>\n",
       "      <td>Interview with JK Rowling...Stephen Fry:Can we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0439358078</td>\n",
       "      <td>Jayson</td>\n",
       "      <td>5</td>\n",
       "      <td>(A) 86%| ExtraordinaryNotes:It's a transitiona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0439358078</td>\n",
       "      <td>Hannah Azerang</td>\n",
       "      <td>5</td>\n",
       "      <td>I had to re read it. I was in such a nostalgic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         isbn               reviewer  rating  \\\n",
       "0  0439358078                 Jayson       5   \n",
       "1  0439358078                Navessa       5   \n",
       "2  0439358078  Diane ϟ [ Lestrange ]       5   \n",
       "3  0439358078                 Jayson       5   \n",
       "4  0439358078         Hannah Azerang       5   \n",
       "\n",
       "                                             comment  \n",
       "0  (A) 86%| ExtraordinaryNotes:An angsty apprehen...  \n",
       "1  Seriously, don't read this review if you haven...  \n",
       "2  Interview with JK Rowling...Stephen Fry:Can we...  \n",
       "3  (A) 86%| ExtraordinaryNotes:It's a transitiona...  \n",
       "4  I had to re read it. I was in such a nostalgic...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl news from ABC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec856898c594fe09a2aa9684cb99f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from urllib.robotparser import RobotFileParser\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# User-agent rotation\n",
    "user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:85.0) Gecko/20100101 Firefox/85.0',\n",
    "]\n",
    "\n",
    "def check_robots_txt(url):\n",
    "    rp = RobotFileParser()\n",
    "    rp.set_url(url + \"/robots.txt\")\n",
    "    rp.read()\n",
    "    return rp\n",
    "\n",
    "def get_links_from_sgtimes(category_sgt, headers, num_limit, rp):\n",
    "    full_url = \"https://thesaigontimes.vn\" + category_sgt\n",
    "\n",
    "    if not rp.can_fetch(headers['User-Agent'], full_url):\n",
    "        return []\n",
    "\n",
    "    response = requests.get(full_url, headers=headers, timeout=10)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    links = soup.find_all(\n",
    "        'h3', class_='entry-title td-module-title', limit=num_limit)\n",
    "    href_link = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            href_link.append(link.find('a')['href'])\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return list(set(href_link))\n",
    "\n",
    "def get_links_from_cnn(category, headers, num_limit, rp):\n",
    "    full_url = \"https://edition.cnn.com\" + category\n",
    "\n",
    "    if not rp.can_fetch(headers['User-Agent'], full_url):\n",
    "        return []\n",
    "\n",
    "    response = requests.get(full_url, headers=headers, timeout=10)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    links = set()\n",
    "    page_number = 1\n",
    "\n",
    "    while len(links) < num_limit:\n",
    "        response = requests.get(full_url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        for item in soup.find_all('a', class_='container__link'):\n",
    "            links.add(\"https://edition.cnn.com\" + item['href'])\n",
    "            if len(links) >= num_limit:\n",
    "                break\n",
    "\n",
    "        page_number += 1\n",
    "        time.sleep(1)  # Throttle requests\n",
    "\n",
    "    return list(links)[:num_limit]\n",
    "\n",
    "def get_links_from_bbc(category, headers, num_limit, rp):\n",
    "    full_url = \"https://www.bbc.com\" + category\n",
    "\n",
    "    if not rp.can_fetch(headers['User-Agent'], full_url):\n",
    "        return []\n",
    "\n",
    "    response = requests.get(full_url, headers=headers, timeout=10)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    links = soup.find_all('a', {'data-testid':'internal-link'}, limit=num_limit)\n",
    "    href_link = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            href_link.append(\"https://www.bbc.com\" + link['href'])\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return list(set(href_link))\n",
    "\n",
    "def get_text_from_link(url, headers, failed_links):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            failed_links.append(url)\n",
    "            return np.nan\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        paragraphs = soup.find_all('p')\n",
    "        text_content = ' '.join(paragraph.get_text(strip=True) for paragraph in paragraphs)\n",
    "        return text_content\n",
    "    except Exception as e:\n",
    "        failed_links.append(url)\n",
    "        return np.nan\n",
    "\n",
    "def get_texts_from_links_parallel(links, headers):\n",
    "    failed_links = []\n",
    "    results_dict = {}\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        future_to_url = {executor.submit(\n",
    "            get_text_from_link, url, headers, failed_links): url for url in links}\n",
    "        for future in as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results_dict[url] = result\n",
    "                # Random delay to throttle requests\n",
    "                time.sleep(random.uniform(1, 3))\n",
    "            except Exception as exc:\n",
    "                failed_links.append(url)\n",
    "                results_dict[url] = np.nan\n",
    "    results = [results_dict[url] for url in links]\n",
    "    return results, failed_links\n",
    "\n",
    "def run_parallel_processing(links, headers):\n",
    "    texts, failed_links = get_texts_from_links_parallel(links, headers)\n",
    "    df = pd.DataFrame({'url': links, 'txt': texts})\n",
    "    return df, failed_links\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    category_cnn = '/business'  # Example category for CNN\n",
    "    category_sgt = '/tai-chinh-ngan-hang'  # Example category for Saigon Times\n",
    "    category_bbc = '/innovation'  # Example category for BBC\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': random.choice(user_agents)\n",
    "    }\n",
    "    num_limit = 10\n",
    "\n",
    "    # Check robots.txt\n",
    "    cnn_rp = check_robots_txt(\"https://edition.cnn.com\")\n",
    "    sgt_rp = check_robots_txt(\"https://thesaigontimes.vn\")\n",
    "    bbc_rp = check_robots_txt(\"https://www.bbc.com\")\n",
    "\n",
    "    links_cnn = get_links_from_cnn(category_cnn, headers, num_limit, cnn_rp)\n",
    "    links_sgt = get_links_from_sgtimes(category_sgt, headers, num_limit, sgt_rp)\n",
    "    links_bbc = get_links_from_bbc(category_bbc, headers, num_limit, bbc_rp)\n",
    "\n",
    "    df_txt, failed_links = run_parallel_processing(links_sgt+links_cnn+links_bbc, headers)\n",
    "    df_txt.to_csv('scraped_texts.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://thesaigontimes.vn/cong-ty-chung-khoan-...</td>\n",
       "      <td>Kinh tế Sài Gòn Online Kinh tế Sài Gòn Online ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://thesaigontimes.vn/chinh-thuc-chuyen-gi...</td>\n",
       "      <td>Kinh tế Sài Gòn Online Kinh tế Sài Gòn Online ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://thesaigontimes.vn/do-dau-vang-nhan-vuo...</td>\n",
       "      <td>Kinh tế Sài Gòn Online Kinh tế Sài Gòn Online ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://thesaigontimes.vn/pho-thu-tuong-ho-duc...</td>\n",
       "      <td>Kinh tế Sài Gòn Online Kinh tế Sài Gòn Online ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://thesaigontimes.vn/vi-sao-chinh-phu-kho...</td>\n",
       "      <td>Kinh tế Sài Gòn Online Kinh tế Sài Gòn Online ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://thesaigontimes.vn/cong-ty-chung-khoan-...   \n",
       "1  https://thesaigontimes.vn/chinh-thuc-chuyen-gi...   \n",
       "2  https://thesaigontimes.vn/do-dau-vang-nhan-vuo...   \n",
       "3  https://thesaigontimes.vn/pho-thu-tuong-ho-duc...   \n",
       "4  https://thesaigontimes.vn/vi-sao-chinh-phu-kho...   \n",
       "\n",
       "                                                 txt  \n",
       "0  Kinh tế Sài Gòn Online Kinh tế Sài Gòn Online ...  \n",
       "1  Kinh tế Sài Gòn Online Kinh tế Sài Gòn Online ...  \n",
       "2  Kinh tế Sài Gòn Online Kinh tế Sài Gòn Online ...  \n",
       "3  Kinh tế Sài Gòn Online Kinh tế Sài Gòn Online ...  \n",
       "4  Kinh tế Sài Gòn Online Kinh tế Sài Gòn Online ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('scraped_texts.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://edition.cnn.com/2024/09/17/economy/us-retail-sales-august/index.html\n",
      "\n",
      "Markets Hot Stocks Fear & Greed Index Latest Market News Hot Stocks A key driver of the US economy remains solid. Spending at US retailers rose 0.1% in August from the prior month, the Commerce Department reported Tuesday. That’s a much slower pace than July’s upwardly revised 1.1% gain, but well above the 0.2% decline economists projected in a FactSet poll. The figures are adjusted for seasonal swings but not inflation. It’s an encouraging sign for America’s economy, since consumer spending represents two-thirds of US economic output. Retail sales make up a sizable chunk of overall spending. Tuesday’s report is the final major economic release beforethe Federal Reserve announces its latest interest-rate moveon Wednesday. The numbers do little to influence the size of the expected rate cut. The debate over whether the Fed will roll out a quarter-point rate cut, or a larger, half-point cut has intensified recently. The health of the US economy, especially the job market, is top of mind for the Fed and Wall Street, which is betting the central bank will start cutting aggressively. Employers are hiring fewer workers these days and it’s become a lot tougher for workers to find a new job. The unemployment rate has ratcheted up quickly over the past year, reaching a 4.2% rate last month from 3.8% a year earlier. If the job market falters, that could translate into a sharp pullback in consumer spending, spelling trouble for the US economy. Businesses would be forced to adjust their hiring plans accordingly, with American shoppers spending less, possibly igniting a negative feedback loop in which consumers spend even less because they got laid off, according to economists. The Fed could step in to prevent that by lowering borrowing costs. While consumer spending remains above pre-pandemic levels, two separate surveys released recently point to a slight pullback in the months to come. In August, consumers surveyed by the Federal Reserve Bank of New York reported a 5% annual increase in nominal household spending, up from 4.6% in April. However, median expected monthly overall spending growth slowed a tick to 3% last month, the New York Fed survey found. That increase is well below the 5.4% high hit in April 2022 but remains above the range seen in 2019. A separate survey from Bank of America released Friday painted a similar picture: a pullback in spending expectations from May 2024 on both the three- and 12-month horizons. “It’s pointing to a picture of a consumer that, incrementally, is getting even more discerning about where they spend their money,” said Robert F. Ohmes, research analyst at Bank of America Securities. The increasingly cautious approach to spending is likely more “wallet shift” versus fears of unemployment, he said, noting food prices as an example. In the decade leading up to the pandemic, food prices held relatively steady, with a 5-year growth rate below 1%, he said. “Today, it’s a 27% increase versus what you were paying five years ago,” he said. “If you’re making 15% more money, but your grocery prices are up 27%, you have to buy less of something to be in the same place.” Food price inflation has moderated substantially during the past year, Consumer Price Index data shows. As of August,grocery prices were rising at a pace of 0.9% annually, landing in line with the average increase seen in 2019, according to Bureau of Labor Statistics data. CNN’s Alicia Wallace contributed to this report. Most stock quote data provided by BATS. US market indices are shown in real time, except for the S&P 500 which is refreshed every two minutes. All times are ET. Factset: FactSet Research Systems Inc. All rights reserved. Chicago Mercantile: Certain market data is the property of Chicago Mercantile Exchange Inc. and its licensors. All rights reserved. Dow Jones: The Dow Jones branded indices are proprietary to and are calculated, distributed and marketed by DJI Opco, a subsidiary of S&P Dow Jones Indices LLC and have been licensed for use to S&P Opco, LLC and CNN. Standard & Poor’s and S&P are registered trademarks of Standard & Poor’s Financial Services LLC and Dow Jones is a registered trademark of Dow Jones Trademark Holdings LLC. All content of the Dow Jones branded indices Copyright S&P Dow Jones Indices LLC and/or its affiliates. Fair value provided by IndexArb.com. Market holidays and trading hours provided by Copp Clark Limited. © 2024 Cable News Network. A Warner Bros. Discovery Company. All Rights Reserved.CNN Sans ™ & © 2016 Cable News Network.\n"
     ]
    }
   ],
   "source": [
    "print(test['url'][12])\n",
    "print()\n",
    "print(test['txt'][12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "## Base Model\n",
    "MODEL_NAME = r\"D:\\Online_Learning\\Practical_DL\\bart_large_cnn\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Function to generate summary\n",
    "def generate_summary(model, tokenizer, text, max_length=400, min_length=130):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "        max_length=max_length,\n",
    "        min_length=min_length,\n",
    "    )\n",
    "\n",
    "    # Decode the generated summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: ['https://thesaigontimes.vn/de-xuat-ho-tro-tai-chinh-linh-hoat-cho-dia-phuong-bi-anh-huong-bao/']\n",
      "Generated Summary:\n",
      "Bộ Tài chính sẽ xem xét và đề xuất phương án hỗ trợ tài. Bão do bão và hoàn lưu bão, ăn bị thiệt hại\n",
      "cho các  các  ĉnh giá lửa 2024. Các āXVN đã thực hiỉn đông cân  năm 2024. Trong tổng số 26\n",
      " ĉn hưởng, có 12  ‘’’ ‘”’, “”, ” ”   ,  h”.\n",
      "\n",
      "\n",
      "Original Text: ['https://thesaigontimes.vn/vang-nhan-tung-buoc-xo-do-cac-ky-luc-ve-gia/']\n",
      "Generated Summary:\n",
      "Kinh tế Sài Gòn Online (KTSG Online) – Kim loại quý toàn cục mới. Giá vàng cũng duy trì vào sáng hôm\n",
      "nay (15-9) Tỉnh rõ ràng về lãi suất. Hồng niễm yἿt giá  vàng nhẫn 77,88 – 79,08 triệu  mua  bán ra.\n",
      "\n",
      "\n",
      "Original Text: ['https://thesaigontimes.vn/ty-gia-bien-dong-kho-luong-he-qua-nao-cho-doanh-nghiep/']\n",
      "Generated Summary:\n",
      "Kinh tế Sài Gòn Online (KTSG) – Trong bối cảnh đô la Mỹ, giá trở lại trong hai tháng qua. Ngoài ra,\n",
      "sự biỉn động khó lường của yen Nhật cũng công cây lớn. 27,7% là mức tăng trưng ứng vồng và tơn\n",
      "124.000 t-shirts.\n",
      "\n",
      "\n",
      "Original Text: ['https://thesaigontimes.vn/thong-tu-12-va-viec-coi-troi-tin-dung-tieu-dung/']\n",
      "Generated Summary:\n",
      "Kinh tế Sài Gòn Online (KTSG) – “Tín dụng tiêu dùng có tính thực tiễn hơn”, PGS.TS. Việt Nam nên học\n",
      "theo mô hình đánh giá tín nhi  của Trung Quốc. KTSG: “Thúc đẩy tín d dùg phải song song với các biỉn\n",
      "pháp kiểm soát rịi ro.”\n",
      "\n",
      "\n",
      "Original Text: ['https://thesaigontimes.vn/facebook-tiktok-va-loat-ong-lon-cong-nghe-nop-thue-hon-6-200-ti-dong-', 'trong-8-thang/']\n",
      "Generated Summary:\n",
      "KTSG Online is an online store for Sài Gòn Online. Tỉ đồng tiền thuế trong 8 tháng qua, tăng 24% so\n",
      "với cùng kỳ 2023. Những tập đoàn công nghệ là Meta (Facebook), Google, Microsoft, TikTok, Netflix và\n",
      "Apple đang chiỿm 90% thị phần dụch.\n",
      "\n",
      "\n",
      "Original Text: ['https://thesaigontimes.vn/doanh-nghiep-nguoi-dan-co-the-duoc-vay-von-khong-lai-suat-de-khoi-phuc-', 'san-xuat/']\n",
      "Generated Summary:\n",
      "Kinh tế Sài Gòn Online (KTSG Online) – Chính phủ yêu cầu Ngân hàng Nhà nước nghiên cứu đưa ra các\n",
      "chính sách tín dụng ưu  hãi như giãn, hoãn nợ hay có gói lãi suất 0 đồng. Bão số 3 (bão Yagi) gây ra\n",
      "những thiệt hại do bão s  sản xuἥt sau nh  nh�i thi-thi-t hâm mái.\n",
      "\n",
      "\n",
      "Original Text: ['https://edition.cnn.com/2024/09/17/food/lunchly-logan-paul-mrbeast/index.html']\n",
      "Generated Summary:\n",
      "MrBeast, Logan Paul and KSI are teaming up on a “groundbreaking new better-for-you lunch option”\n",
      "called Lunchly. The all-in-one meal includes a bottle of Prime Hydration, a low-sugar energy drink\n",
      "and a Feastables chocolate bar. “Our end game has always been to not only provide high quality\n",
      "products, but also healthier alternatives,” Lunchly cofounder Logan Paul said in a press release.\n",
      "The launch comes when Lunchable, owned by Kraft Heinz, announced a major initiative last year to be\n",
      "apart of school lunch programs, but had to reformulate the ingredients to meet federal guidelines.\n",
      "\n",
      "\n",
      "Original Text: ['https://edition.cnn.com/2024/09/17/business/pagers-cell-phones-batteries/index.html']\n",
      "Generated Summary:\n",
      "At least nine people, including an eight-year-old girl, were killed, and around 2,800 were wounded.\n",
      "The exact cause of the explosions hasn’t been confirmed yet, and the news is developing by the\n",
      "minute. Experts who spoke to CNN said the explosions were most likely triggered by hardware\n",
      "tampering rather than another theory of a cybersecurity breach causing lithium batteries to heat up\n",
      "and explode. The explosions came as Israel and Hezbollah have engaged intit-for-tat strikesfor\n",
      "months in the wake of the Israel-Hamas war. It’s unclear what kind of battery the pagers in question\n",
      "had. A Lebanese security source told CNN that Hezbollah had recently purchased the devices.\n",
      "\n",
      "\n",
      "Original Text: ['https://edition.cnn.com/2024/09/17/business/alaska-airlines-hawaiian-can-close-merger-deal-dot-', 'says/index.html']\n",
      "Generated Summary:\n",
      "The Justice Department in August opted not to block the deal that was announced in December by\n",
      "Alaska, the fifth-largest domestic U.S. airline, to Hawaiian, the 10th-largest carrier. The carriers\n",
      "said on Tuesday that they expect to close the deal in the coming days. The agreement came after\n",
      "weeks of discussions between Alaska and DOT, which had sought wide-ranging concessions that went\n",
      "beyond what is in the agreement announced on Tuesday. Alaska said the commitments align with plans\n",
      "it announced at the time it signed the transaction and “do not impact the synergies of the deal,\n",
      "which will enhance competition and expand choice for consumers”\n",
      "\n",
      "\n",
      "Original Text: ['https://edition.cnn.com/2024/09/17/business/uaw-strike-stellantis/index.html']\n",
      "Generated Summary:\n",
      "United Auto Workers union President Shawn Fain says the group plans to hold strike authorization\n",
      "votes against Stellantis. He says the automaker was failing to live up to guarantees it made during\n",
      "a labor deal struck last year. Stellants issued a statement on Tuesday denying it had confirmed a\n",
      "decision to move production of the Dodge Durango to Canada. It said it still had plenty of time\n",
      "within the four-and-a-half years of the contract to meet the agreed investment targets and vehicle\n",
      "production commitments. “We are 100% within our rights and within our power to take strike action if\n",
      "necessary,” Fain said in a speech Tuesday night.\n",
      "\n",
      "\n",
      "Original Text: ['https://edition.cnn.com/2024/09/17/business/sams-club-jobs-pay-costco/index.html']\n",
      "Generated Summary:\n",
      "Sam’s Club, the membership club arm of Walmart, will increase its minimum wage to $16 starting in\n",
      "November from $15 an hour. It will also raise wages for its nearly 100,000 employees, ranging from\n",
      "3% to 6% depending on their tenure. The company hopes a steady workforce of higher-paid employees\n",
      "with financial incentives to stay with the company will lead to improved service. Its main\n",
      "competitor is Costco, which pays among the highest wages in retail — aminimumof $19.50 an hour, and\n",
      "Sam's Club needs to raise its pay to stay competitive with Costco and other chains that have raised\n",
      "wages. The minimum wage in nearly every location in the United States does not cover the cost of\n",
      "living, according to MIT’sliving wage calculator.\n",
      "\n",
      "\n",
      "Original Text: ['https://edition.cnn.com/2024/09/17/media/murdoch-family-succession-fox-news-future/index.html']\n",
      "Generated Summary:\n",
      "Rupert Murdoch, 93, is seeking to amend the family trust that he established decades ago. Under the\n",
      "current terms, Lachlan, James, Elisabeth and Prudence Murdoch each have equal voting rights after\n",
      "their father dies. The prospect of a James-led takeover is openly discussed — and, by some, feared —\n",
      "inside Fox News. Rupert, long known for his conservative politics, evidently wants to ensure that\n",
      "his media outlets retain a right-wing bent long after his death. The proceedings in Reno, Nevada,\n",
      "began Monday and may take about two weeks, according to a person with knowledge of the matter. The\n",
      "case is so secretive that the court wouldn’t even acknowledge the Murdoch’s involved.\n",
      "\n",
      "\n",
      "Original Text: ['https://edition.cnn.com/2024/09/17/economy/us-retail-sales-august/index.html']\n",
      "Generated Summary:\n",
      "Spending at US retailers rose 0.1% in August from the prior month, the Commerce Department says.\n",
      "That’s a much slower pace than July’S upwardly revised 1.1%, but well above the 0.2% decline\n",
      "economists projected. The debate over whether the Fed will roll out a quarter-point rate cut, or a\n",
      "larger, half-point cut, has intensified. The health of the US economy, especially the job market, is\n",
      "top of mind for the Fed and Wall Street, which is betting the central bank will start cutting\n",
      "aggressively. The numbers do little to influence the size of the expected rate cut.\n",
      "\n",
      "\n",
      "Original Text: ['https://edition.cnn.com/2024/09/17/economy/interest-rate-cut-economic-impact/index.html']\n",
      "Generated Summary:\n",
      "The Fed is widely expected to announce Wednesdayit is finally cutting its benchmark lending rate.\n",
      "Most Americans will have to squint pretty hard to notice much of a difference from one cut, or even\n",
      "multiple cuts, for at least a year, if not more. When rates are low, borrowing money is cheaper\n",
      "thanwhen rates are high. Lower interest rates make it more attractive for consumers to spend money,\n",
      "as opposed to saving it, since the rate they can get for setting aside money in savings accounts\n",
      "goes down.. While businesses could respond to the uptick in demand by raising prices, that doesn't\n",
      "happen overnight after the Fed lowers rates, said Thomas Drechsel, an economics professor at the\n",
      "University of Maryland.\n",
      "\n",
      "\n",
      "Original Text: ['https://edition.cnn.com/2024/09/18/business/japan-nippon-steel-us-takeover-extended-hnk-', 'intl/index.html']\n",
      "Generated Summary:\n",
      "Nippon Steel is expected to re-file its application for a national security review by American\n",
      "regulators of its $15 billion takeover bid of US Steel. The surprise development allows the\n",
      "steelmakers to reset the clock, potentially keeping the controversial deal alive. President Joe\n",
      "Biden, Republican presidential nominee Donald Trump and Democratic nominee Vice President Kamala\n",
      "Harris have all voiced opposition to the deal. Several business groups sent a letter last week to\n",
      "Treasury Secretary Janet Yellen, warning of the risks of politicizing a process that was created to\n",
      "study national security risks on their merits. US Steel put itself up for sale in 2023 after\n",
      "receiving an unsolicited $7 billion takeover offer from Ohio-based Cleveland Cliffs.\n",
      "\n",
      "\n",
      "Original Text: ['https://edition.cnn.com/2024/09/17/tech/instagram-teens-protected-accounts/index.html']\n",
      "Generated Summary:\n",
      "Instagram will automatically apply the new “teen accounts’ settings to all users under the age of\n",
      "18. After the update, 16- and 17-year-old users will be able to manually change the app back to\n",
      "their preferred settings. Instagram plans to apply the changes for all teen accounts in select\n",
      "countries, including the United States, starting next week. The shift limits the types of\n",
      "“sensitive” content teens can see on their Explore page and in Reels, such as posts promoting\n",
      "cosmetic procedures. The app will also add new features to its parental supervision tool, allowing\n",
      "parents to see what accounts their teen has recently messaged.\n",
      "\n",
      "\n",
      "Original Text: ['https://www.bbc.com/news/articles/c33vvlj76m7o']\n",
      "Generated Summary:\n",
      "The Plucky Squire is a new video game set inside - and outside - a storybook. It follows title\n",
      "character Jot, and his quest to save the Land of Mojo from evil wizard Humgrump. The game is one of\n",
      "this year's most anticipated independently developed games. It marks the end of a four-year quest\n",
      "for one of its lead designers, James Turner. James studied computer graphics at university and got a\n",
      "job at a London game studio. He was eventually noticed by Game Freak - the makers of the mainline\n",
      "Pokémon titles - and worked his way up to art director on Pokémon Sword and Shield. James says he's\n",
      "\"always had a passion for doing, building things from scratch\"\n",
      "\n",
      "\n",
      "Original Text: ['https://www.bbc.com/news/articles/cn8l3x9e8ezo']\n",
      "Generated Summary:\n",
      "Oceangate’s Titan sub went down to visit the Titanic in June 2023. The vessel suffered a\n",
      "catastrophic failure as it neared the sea floor. The US Coast Guard is holding a public hearing on\n",
      "16 September to examine why the disaster happened. There are five key questions that still need to\n",
      "be answered. The submersible was made out of carbon fibre, an unconventional material for a deep-sea\n",
      "vessel. The viewport window was only rated to a depth of 1,300m (4,300ft) by its manufacturer, but\n",
      "Titan was diving almost three times deeper. The hull was also an unusual shape - cylindrical, rather\n",
      "than spherical. The junctions between different materials also gave cause for concern.\n",
      "\n",
      "\n",
      "Original Text: ['https://www.bbc.com/news/technology-53476117']\n",
      "Generated Summary:\n",
      "Lawmakers from both major US political parties supported a law that bans TikTok. They fear the\n",
      "Chinese government could force ByteDance to hand over data about TikTok's 170 million US users.\n",
      "Previous attempts to block the app in the US on national security grounds have failed. TikTok has\n",
      "previously called the legislation an \"unconstitutional ban\" and affront to the US right to free\n",
      "speech. It is already banned in India, which was one of the app's largest markets before it was\n",
      "outlawed in June. The app is also blocked in Iran, Nepal, Afghanistan and Somalia. The UK government\n",
      "has advised staff to delete TikTok from corporate phones because of security fears.\n",
      "\n",
      "\n",
      "Original Text: ['https://www.bbc.com/sport']\n",
      "Generated Summary:\n",
      "'He is still phenomenal' - Kane's record-breaking night as Bayern 'dream' The records Harry Kane\n",
      "broke in scoring four as Bayern Munich thrash Dinamo Zagreb. 'Unbelievable we lose to Forest if we\n",
      "can play like this' Endrick sets record in Real Madrid Champions League win. 'We won the World Cup\n",
      "and it changed nothing' Preston stun Fulham with historic shootout triumph. 'They could dominate' -\n",
      "how Real Madrid built a new team of Galacticos. Are Saliba and Gabriel best centre-back partnership\n",
      "in Europe? Will new Champions League format succeed or fail? Who was to blame for Arsenal winner\n",
      "against Spurs?\n",
      "\n",
      "\n",
      "Original Text: ['https://www.bbc.com/news/articles/cvgl08wg4kqo']\n",
      "Generated Summary:\n",
      "Former operations director David Lochridge testified to US Coast Guard investigators. He said he had\n",
      "warned of potential safety problems before he was fired in 2018. Five people on board the Titan sub\n",
      "died when the experimental deep-sea craft imploded in June 2023 as it began a planned descent to the\n",
      "Titanic wreck. The public hearings began on Monday as part of a two-week inquiry into the disaster.\n",
      "The investigation has been going on for 15 months and officials detailed communications between the\n",
      "Titan and its mother ship, the Polar Prince. Officials also offered a handful of specific examples\n",
      "of submersible failures including its batteries dying and leaving passengers stuck inside for 27\n",
      "hours.\n",
      "\n",
      "\n",
      "Original Text: ['https://www.bbc.com/video']\n",
      "Generated Summary:\n",
      "Nasa has released new'sonifications' of the Universe on the 25th anniversary of Chandra, its X-ray\n",
      "Observatory. An 11-mile hike in the Bako National Park in Malaysia transports travellers through\n",
      "seven distinct habitats. A wild orangutan has astonished researchers by using a medicinal plant to\n",
      "treat his cheek wound. We explore the range of health benefits linked to cold water swimming. At 51,\n",
      "Andy Macdonald shows you're never too old to compete at the Olympics. But how can you keep fit as\n",
      "you age? In a clash of wit and technology, we bring together two unlikely adversaries: a comedian\n",
      "and an AI chatbot. Find out how the rich culture of Normandy inspired one of the world's greatest\n",
      "artists.\n",
      "\n",
      "\n",
      "Original Text: ['https://www.bbc.com/news/articles/cx25ljxpygeo']\n",
      "Generated Summary:\n",
      "Of the 2.8 million private cars registered in Norway, 754,303 are now all-electric. That compares\n",
      "with 753,905 that run on petrol, according to new figures. Sales of electric vehicles (EVs) have\n",
      "been boosted by tax breaks and other incentives, funded in large part from the money Norway makes\n",
      "out of oil and gas. The Nordic country of 5.5 million people is aiming to become the first nation to\n",
      "end the sale of new petrol and diesel cars - by 2025. At present, nine out of 10 new cars sold in\n",
      "Norway are electric vehicles. Diesel models remain most numerous at just under one million, but\n",
      "their sales are falling rapidly.\n",
      "\n",
      "\n",
      "Original Text: ['https://www.bbc.com/']\n",
      "Generated Summary:\n",
      "Nine people have been killed and 2,800 wounded by the blasts, the Lebanese health ministry says.\n",
      "Donald Trump said a woman spotted his would-be assassin while driving, leading to his arrest. The\n",
      "convicted fraudster who conned her way into New York's elite is facing deportation back to Germany.\n",
      "Philip Morris International bought Vectura Group just three years ago in a deal worth more than\n",
      "£1bn. The video app TikTok could be banned in the US unless it is sold by its Chinese parent company\n",
      "ByteDance. In tropical storms, location matters when it comes to what damage to expect – especially\n",
      "if you're on the \"dirty\" side of a hurricane.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "generated_summaries = [generate_summary(model, tokenizer, text) for text in test['txt']]\n",
    "test['summaried'] = generated_summaries\n",
    "\n",
    "for i in range(len(generated_summaries)):\n",
    "    print(f\"Original Text: {textwrap.wrap(test['url'][i], width=100)}\")\n",
    "    print(\"Generated Summary:\")\n",
    "    summary = test['summaried'][i]\n",
    "    paragraphs = summary.split('\\n')\n",
    "    for paragraph in paragraphs:\n",
    "        lines = textwrap.wrap(paragraph, width=100)\n",
    "        for line in lines:\n",
    "            print(line)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with another model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd5edd3351aa48c2acc852b23e834fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "# Set CUDA_LAUNCH_BLOCKING for debugging\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Define the model name\n",
    "MODEL_NAME = r\"D:\\Online_Learning\\Practical_DL\\models\\gemma_2_2b_it\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_gemma = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16,)\n",
    "tokenizer_gemma = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Move model to the appropriate device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape: torch.Size([1, 1266])\n",
      "Input tensor dtype: torch.int64\n",
      "Response: Instagram is implementing new \"teen account\" settings to protect young users from online dangers. These settings will automatically make millions of teen accounts private, restrict content, and require parental approval for changes. The changes are a response to growing concerns about the platform's impact on teens and criticism of Meta for not doing enough to protect them.\n"
     ]
    }
   ],
   "source": [
    "# Function to generate a response from the model\n",
    "def generate_response(model, tokenizer, prompt, max_length=2048):\n",
    "    # Tokenize the input prompt\n",
    "    prompt = \"Summarize this text, show only result: \" + prompt\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(device)\n",
    "    print(f\"Input tensor shape: {inputs['input_ids'].shape}\")\n",
    "    print(f\"Input tensor dtype: {inputs['input_ids'].dtype}\")\n",
    "\n",
    "\n",
    "    response_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        num_beams=2,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    # Decode the generated response\n",
    "    response = tokenizer.decode(response_ids[0])\n",
    "\n",
    "    summary_start = response.find(\"**Summary:**\")\n",
    "    if summary_start != -1:\n",
    "        summary_start += len(\"**Summary:**\")\n",
    "        summary_end = response.find(\"<eos>\", summary_start)\n",
    "        if summary_end != -1:\n",
    "            summary = response[summary_start:summary_end].strip()\n",
    "        else:\n",
    "            summary = response[summary_start:].strip()\n",
    "    else:\n",
    "        summary = \"Summary not found in the response.\"\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Example question to ask the model\n",
    "text = texts[-1]\n",
    "# Generate response\n",
    "response = generate_response(model_gemma, tokenizer_gemma, text)\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8844300d283493a96264aaf2d0a14c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.cuda.amp import autocast\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Define the model names\n",
    "MODEL_NAME_BART = r\"D:\\Online_Learning\\Practical_DL\\models\\bart_large_cnn\"\n",
    "MODEL_NAME_GEMMA = r\"D:\\Online_Learning\\Practical_DL\\models\\gemma_2_2b_it\"\n",
    "\n",
    "# Load the BART model and tokenizer\n",
    "model_bart = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME_BART)\n",
    "tokenizer_bart = AutoTokenizer.from_pretrained(MODEL_NAME_BART)\n",
    "model_bart.eval()\n",
    "\n",
    "# Load the GEMMA model and tokenizer\n",
    "model_gemma = AutoModelForCausalLM.from_pretrained(MODEL_NAME_GEMMA, torch_dtype=torch.float16)\n",
    "tokenizer_gemma = AutoTokenizer.from_pretrained(MODEL_NAME_GEMMA)\n",
    "model_gemma.eval()\n",
    "\n",
    "# Move models to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_bart.to(device)\n",
    "model_gemma.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Function to generate summary using BART\n",
    "def generate_summary_bart(model, tokenizer, text, max_length=1024):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        num_beams=2,\n",
    "        early_stopping=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "    # Decode the generated summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Function to generate summary using GEMMA\n",
    "def generate_summary_gemma(model, tokenizer, prompt, max_length=2048):\n",
    "    # Tokenize the input prompt\n",
    "    prompt = \"Summarize this text, show only result: \" + prompt\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    with autocast():\n",
    "        response_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_beams=2,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    # Decode the generated response\n",
    "    response = tokenizer.decode(response_ids[0])\n",
    "\n",
    "    match = re.search(r'\\*\\*Summary:\\*\\*|## Summary:', response)\n",
    "    if match:\n",
    "        summary_start = match.end()\n",
    "        summary_end = response.find(\"<eos>\", summary_start)\n",
    "        if summary_end != -1:\n",
    "            summary = response[summary_start:summary_end].strip()\n",
    "        else:\n",
    "            summary = response[summary_start:].strip()\n",
    "    else:\n",
    "        summary = \"Summary not found in the response.\"\n",
    "\n",
    "    return summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:603: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference Summaries Finsihsed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# test = pd.read_csv('scraped_texts.csv')\n",
    "# test.head()\n",
    "\n",
    "# Load the first 10 texts from the CSV file\n",
    "df = pd.read_csv('scraped_texts.csv')\n",
    "df = df.loc[~df['url'].str.contains('thesaigontimes')]\n",
    "texts = df['txt'][:10].tolist()\n",
    "# Generate reference summaries using BART\n",
    "reference_summaries = [generate_summary_bart(model_bart, tokenizer_bart, text) for text in texts]\n",
    "print(\"Reference Summaries Finsihsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_check = df.head(10).copy()\n",
    "# df_check['reference_summaries'] = reference_summaries\n",
    "# df_check.to_csv('reference_summaries.csv')\n",
    "import pandas as pd\n",
    "df_check = pd.read_csv('reference_summaries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\gemma2\\modeling_gemma2.py:582: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m reference_summary \u001b[38;5;241m=\u001b[39m  df_check[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreference_summaries\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[i]\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Generate summary using GEMMA\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m summary_gemma \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_summary_gemma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_gemma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_gemma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m summaries\u001b[38;5;241m.\u001b[39mappend(summary_gemma)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Evaluate summary\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 55\u001b[0m, in \u001b[0;36mgenerate_summary_gemma\u001b[1;34m(model, tokenizer, prompt, max_length)\u001b[0m\n\u001b[0;32m     52\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[1;32m---> 55\u001b[0m     response_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Decode the generated response\u001b[39;00m\n\u001b[0;32m     63\u001b[0m response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(response_ids[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\generation\\utils.py:1953\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1945\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1946\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1947\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   1948\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1949\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1950\u001b[0m     )\n\u001b[0;32m   1952\u001b[0m     \u001b[38;5;66;03m# 14. run beam sample\u001b[39;00m\n\u001b[1;32m-> 1953\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1954\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1960\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1961\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1962\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1964\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[0;32m   1965\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   1966\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   1967\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1968\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1974\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   1975\u001b[0m     )\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\generation\\utils.py:2914\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, logits_warper, **model_kwargs)\u001b[0m\n\u001b[0;32m   2911\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m stack_model_outputs(outputs_per_sub_batch)\n\u001b[0;32m   2913\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Unchanged original behavior\u001b[39;00m\n\u001b[1;32m-> 2914\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2915\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2917\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2918\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2919\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2922\u001b[0m     cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\gemma2\\modeling_gemma2.py:1068\u001b[0m, in \u001b[0;36mGemma2ForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1065\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1068\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1073\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1081\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1082\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\gemma2\\modeling_gemma2.py:908\u001b[0m, in \u001b[0;36mGemma2Model.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    897\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    898\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    899\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    905\u001b[0m         cache_position,\n\u001b[0;32m    906\u001b[0m     )\n\u001b[0;32m    907\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 908\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    913\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    914\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    918\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\gemma2\\modeling_gemma2.py:665\u001b[0m, in \u001b[0;36mGemma2DecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[0;32m    663\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_feedforward_layernorm(hidden_states)\n\u001b[0;32m    664\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m--> 665\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_feedforward_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    666\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    668\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\gemma2\\modeling_gemma2.py:83\u001b[0m, in \u001b[0;36mGemma2RMSNorm.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 83\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# Llama does x.to(float16) * w whilst Gemma2 is (x * w).to(float16)\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m# See https://github.com/huggingface/transformers/pull/29402\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     output \u001b[38;5;241m=\u001b[39m output \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mfloat())\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\gemma2\\modeling_gemma2.py:80\u001b[0m, in \u001b[0;36mGemma2RMSNorm._norm\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_norm\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Function to evaluate summaries using ROUGE\n",
    "def evaluate_summaries(reference, generated):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, generated)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# Initialize lists to store scores\n",
    "rouge1_gemma_scores = []\n",
    "rouge2_gemma_scores = []\n",
    "rougeL_gemma_scores = []\n",
    "\n",
    "summaries = []\n",
    "\n",
    "# Generate and evaluate summaries for the first 10 texts\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    text = df_check['txt'].values[i]\n",
    "    reference_summary =  df_check['reference_summaries'].values[i]\n",
    "\n",
    "    # Generate summary using GEMMA\n",
    "    summary_gemma = generate_summary_gemma(model_gemma, tokenizer_gemma, text)\n",
    "    summaries.append(summary_gemma)\n",
    "\n",
    "    # Evaluate summary\n",
    "    scores_gemma = evaluate_summaries(reference_summary, summary_gemma)\n",
    "\n",
    "    # Store scores\n",
    "    rouge1_gemma_scores.append(scores_gemma['rouge1'].fmeasure)\n",
    "    rouge2_gemma_scores.append(scores_gemma['rouge2'].fmeasure)\n",
    "    rougeL_gemma_scores.append(scores_gemma['rougeL'].fmeasure)\n",
    "\n",
    "# Calculate average ROUGE scores\n",
    "avg_rouge1_gemma = sum(rouge1_gemma_scores) / len(rouge1_gemma_scores)\n",
    "avg_rouge2_gemma = sum(rouge2_gemma_scores) / len(rouge2_gemma_scores)\n",
    "avg_rougeL_gemma = sum(rougeL_gemma_scores) / len(rougeL_gemma_scores)\n",
    "\n",
    "# Print average ROUGE scores\n",
    "print(\"Average ROUGE scores for GEMMA compared to BART reference summaries:\")\n",
    "print(f\"ROUGE-1: {avg_rouge1_gemma}\")\n",
    "print(f\"ROUGE-2: {avg_rouge2_gemma}\")\n",
    "print(f\"ROUGE-L: {avg_rougeL_gemma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess input text\n",
    "def preprocess_text(text):\n",
    "    # Remove unnecessary information\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # Remove content in brackets\n",
    "    return text\n",
    "\n",
    "# Function to post-process generated summaries\n",
    "def postprocess_summary(summary):\n",
    "    # Remove redundant information\n",
    "    summary = re.sub(r'\\s+', ' ', summary)  # Remove extra whitespace\n",
    "    summary = re.sub(r'\\[.*?\\]', '', summary)  # Remove content in brackets\n",
    "    return summary\n",
    "\n",
    "# Function to generate summary using GEMMA\n",
    "def generate_summary_gemma(model, tokenizer, prompt, max_length=1024, min_length=100, num_beams=2):\n",
    "    # Tokenize the input prompt\n",
    "    prompt = \"Summarize this text, show only result: \" + prompt\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    # # Ensure inputs are in FP32 if using mixed precision\n",
    "    # if inputs['input_ids'].dtype == torch.float16:\n",
    "    #     inputs['input_ids'] = inputs['input_ids'].to(torch.float16)\n",
    "    # if inputs['attention_mask'].dtype == torch.float16:\n",
    "    #     inputs['attention_mask'] = inputs['attention_mask'].to(torch.float16)\n",
    "\n",
    "    response_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        min_length=min_length,\n",
    "        num_beams=num_beams,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # Decode the generated response\n",
    "    response = tokenizer.decode(response_ids[0])\n",
    "\n",
    "    # Extract the summary text using regular expressions\n",
    "    match = re.search(r'\\*\\*Summary:\\*\\*|## Summary:', response)\n",
    "    if match:\n",
    "        summary_start = match.end()\n",
    "        summary_end = response.find(\"<eos>\", summary_start)\n",
    "        if summary_end != -1:\n",
    "            summary = response[summary_start:summary_end].strip()\n",
    "        else:\n",
    "            summary = response[summary_start:].strip()\n",
    "    else:\n",
    "        summary = \"Summary not found in the response.\"\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: Here's a summary:\n",
      "\n",
      "Iran retaliated against Israel with a ballistic missile attack after the killing of two high-profile targets: Hassan Nasrallah, leader of Hezbollah, and Abbas Nilforoushan, an Iranian Revolutionary Guards deputy commander. The attacks were in response to Israel's deployment of ground forces into south Lebanon as part of its ongoing war against Hamas. Iran threatened further retaliation if Israel responded, labeling them \"crushing\". The conflict is escalating between Israel and Iran's allies, with tensions running high after a string of recent deadly strikes on Iranian targets in Lebanon and Gaza.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Import necessary modules\n",
    "import ollama\n",
    "\n",
    "# Step 3: Load the LLaMA model\n",
    "model_name = \"llama3.2:3b\"\n",
    "client = ollama.Client()\n",
    "client.use_gpu = True\n",
    "\n",
    "# Step 4: Define the summarization function\n",
    "def summarize_text(text, model_name):\n",
    "    # Prepare the input for the model\n",
    "    input_text = f\"summarize: {text}\"\n",
    "    \n",
    "    # Generate the summary\n",
    "    summary = client.generate(prompt=input_text, model=model_name, stream=False)\n",
    "    \n",
    "    return summary['response']\n",
    "    # Ensure the client uses GPU if available\n",
    "    \n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    text = \"\"\"\n",
    "    Iran on Tuesday launched a ballistic missile attack on Israel in retaliation for its recent killing of Hezbollah leader Hassan Nasrallah and an Iranian commander in Lebanon.\n",
    "\n",
    "    The attack came on the heels of Israel’s deployment of ground forces into south Lebanon, escalating its offensive on Hezbollah, a militant group backed by Iran.\n",
    "\n",
    "    “In response to the martyrdom of Martyr Haniyeh, Seyyed Hassan Nasrallah, and Martyr Nilforoushan, we have targeted the heart of the occupied territories,” Iran’s Revolutionary Guard said in a statement after missiles began appearing in the skies over Israel.\n",
    "\n",
    "    “Should the Zionist regime respond to Iran’s operation, it will face crushing attack,” said the group, Iran’s paramilitary organization.\n",
    "\n",
    "    Abbas Nilforoushan was an Iranian Revolutionary Guards deputy commander, who was killed with Nasrallah in a bombing by Israel last Friday in Beirut.\n",
    "\n",
    "    Ismail Haniyeh was the political commander of the Hamas terror group, who was killed in July by an Israeli strike on Tehran, the capital of Iran. Israel has been engaged in a brutal war on Hamas since the group launched the Oct. 7 terror attack on Israel from Gaza.\n",
    "    \"\"\"\n",
    "    summary = summarize_text(text, model_name)\n",
    "    print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pinecone\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(r\"D:\\Online_Learning\\Practical_DL\\langchain_udemy\\ice_breaker\\.env\")\n",
    "\n",
    "# Retrieve the API key and index name from environment variables\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "INDEX_NAME = os.getenv(\"INDEX_NAME\")\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:3b\", use_gpu=True, max_tokens=1024, temperature=0.7, top_p=0.9)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize Pinecone vector store\n",
    "docsearch = PineconeVectorStore(index_name=INDEX_NAME, embedding=embeddings)\n",
    "\n",
    "# Initialize conversation memory with summary\n",
    "# Load environment variables\n",
    "load_dotenv(r\"D:\\Online_Learning\\Practical_DL\\langchain_udemy\\ice_breaker\\.env\")\n",
    "\n",
    "# Retrieve the API key and index name from environment variables\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "INDEX_NAME = os.getenv(\"INDEX_NAME\")\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:3b\", use_gpu=True, max_tokens=1024, temperature=0.7, top_p=0.9)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize Pinecone vector store\n",
    "docsearch = PineconeVectorStore(index_name=INDEX_NAME, embedding=embeddings)\n",
    "\n",
    "# Initialize conversation memory with summary\n",
    "memory = ConversationSummaryMemory(llm=llm, memory_key=\"chat_history\", return_messages=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hello! How can I assist you today?\n",
      "Chatbot: One-hot encoding is a method used to convert categorical data into numerical data, which can be fed into machine learning algorithms. It works by creating a binary vector for each category in the data, where one element in the vector is set to 1 and all others are set to 0.\n",
      "\n",
      "For example, if you have a dataset with text data and you want to convert it into numerical features using one-hot encoding, you would create a separate feature for each word in the vocabulary. If a document contains the word \"hello\", its corresponding feature vector would be [1, 0, ..., 0] where the first element is 1 (indicating that the word \"hello\" exists in the document) and all other elements are 0.\n",
      "\n",
      "However, as mentioned in the text you provided, one-hot encoding has some limitations, such as introducing multicollinearity which can be an issue for certain models.\n",
      "Chatbot: A syntactic parser is a program that decomposes sentences into a parse tree, which consists of hierarchical constituents, or syntactic categories. The goal of a syntactic parser is to break down a sentence into its underlying syntactic structure, revealing the relationships between words and phrases.\n",
      "\n",
      "In simpler terms, a syntactic parser takes a sentence and tries to figure out how the individual words are related to each other in terms of grammar and syntax. It does this by analyzing the grammatical rules that define the language and applying them to the sentence to identify its constituent parts and their relationships.\n",
      "\n",
      "A syntactic parser can provide valuable insights into the meaning and structure of a sentence, enabling tasks such as:\n",
      "\n",
      "* Identifying parts of speech (e.g., nouns, verbs, adjectives)\n",
      "* Recognizing phrase structures (e.g., noun phrases, verb phrases)\n",
      "* Detecting grammatical errors\n",
      "* Analyzing syntax for language understanding and generation\n",
      "\n",
      "Constituency parsing is a type of syntactic parsing that focuses on breaking down sentences into nested phrase structures. This approach provides an opportunity to apply tree-traversal algorithms that can facilitate computation on text.\n",
      "\n",
      "In the context of natural language processing, syntactic parsers are often used in combination with other techniques, such as grammar-based feature extraction and dependency parsing, to improve model performance and build more accurate language models.\n",
      "Chatbot: You're asking about syntactic parsing! That's a great topic.\n",
      "\n",
      "To answer your question directly: The main types of syntactic parsers are rule-based parsers, statistical parsers, and deep learning parsers. Each type has its own strengths and weaknesses, but they all aim to break down sentences into their constituent parts based on grammatical rules and syntax.\n",
      "\n",
      "If you'd like more information or clarification on any aspect of syntactic parsing, feel free to ask!\n",
      "Chatbot: A context-free grammar is a set of rules for combining syntactic components to form sensical strings. It defines how different parts of speech can be related to each other, and is used by parsers to systematically search out meaningful syntactic structures from text. In essence, it's a way to specify the structure of language using a set of abstract rules that don't rely on specific contexts or examples.\n",
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Create a conversational chain\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=docsearch.as_retriever(),\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# Chatbot loop\n",
    "print(\"Chatbot: Hello! How can I assist you today?\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in ['exit', 'quit', 'bye']:\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    response = qa({\"question\": user_input})\n",
    "    wrapped_response = textwrap.fill(response['answer'], width=20)\n",
    "    print(\"Chatbot:\", wrapped_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test With Gemma 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "903b3dcf2dd6465e9379b8120089cbe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from dotenv import load_dotenv\n",
    "import pinecone\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(r\"D:\\Online_Learning\\Practical_DL\\langchain_udemy\\ice_breaker\\.env\")\n",
    "\n",
    "# Retrieve the API key and index name from environment variables\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "INDEX_NAME = os.getenv(\"INDEX_NAME\")\n",
    "MODEL_NAME_GEMMA = r\"D:\\Online_Learning\\Practical_DL\\models\\gemma_2_2b_it\"\n",
    "\n",
    "# Load the GEMMA model and tokenizer\n",
    "model_gemma = AutoModelForCausalLM.from_pretrained(MODEL_NAME_GEMMA, torch_dtype=torch.float16)\n",
    "tokenizer_gemma = AutoTokenizer.from_pretrained(MODEL_NAME_GEMMA)\n",
    "model_gemma.eval()\n",
    "\n",
    "# Move models to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_gemma.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize Pinecone vector store\n",
    "docsearch = PineconeVectorStore(index_name=INDEX_NAME, embedding=embeddings)\n",
    "\n",
    "# Initialize conversation memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hello! How can I assist you today?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "d:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatbot: Goodbye!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mqa\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatbot:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:180\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    179\u001b[0m     emit_warning()\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\langchain\\chains\\base.py:383\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    376\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    381\u001b[0m }\n\u001b[1;32m--> 383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\langchain\\chains\\base.py:166\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    167\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\langchain\\chains\\base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    161\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    162\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\langchain\\chains\\conversational_retrieval\\base.py:167\u001b[0m, in \u001b[0;36mBaseConversationalRetrievalChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    165\u001b[0m         new_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m new_question\n\u001b[0;32m    166\u001b[0m     new_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m chat_history_str\n\u001b[1;32m--> 167\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_documents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_inputs\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m     output[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m answer\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_source_documents:\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:180\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    179\u001b[0m     emit_warning()\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\langchain\\chains\\base.py:605\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    601\u001b[0m         _output_key\n\u001b[0;32m    602\u001b[0m     ]\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m--> 605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[0;32m    606\u001b[0m         _output_key\n\u001b[0;32m    607\u001b[0m     ]\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    612\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but none were provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m     )\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:180\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    179\u001b[0m     emit_warning()\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\langchain\\chains\\base.py:383\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    376\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    381\u001b[0m }\n\u001b[1;32m--> 383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\langchain\\chains\\base.py:166\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    167\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\langchain\\chains\\base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    161\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    162\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\langchain\\chains\\combine_documents\\base.py:137\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[0;32m    136\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[1;32m--> 137\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mother_keys\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\langchain\\chains\\combine_documents\\stuff.py:244\u001b[0m, in \u001b[0;36mStuffDocumentsChain.combine_docs\u001b[1;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    242\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_inputs(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m# Call predict on the LLM.\u001b[39;00m\n\u001b[1;32m--> 244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m, {}\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\langchain\\chains\\llm.py:316\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[1;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    302\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \n\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:180\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    179\u001b[0m     emit_warning()\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\langchain\\chains\\base.py:383\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    376\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    381\u001b[0m }\n\u001b[1;32m--> 383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\langchain\\chains\\base.py:166\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    167\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\langchain\\chains\\base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    161\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    162\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\langchain\\chains\\llm.py:126\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    123\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    124\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    125\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 126\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\langchain\\chains\\llm.py:138\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    136\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[0;32m    146\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[0;32m    147\u001b[0m     )\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\langchain_core\\language_models\\llms.py:750\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    744\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    748\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    749\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\langchain_core\\language_models\\llms.py:944\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    930\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    931\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    932\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    942\u001b[0m         )\n\u001b[0;32m    943\u001b[0m     ]\n\u001b[1;32m--> 944\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\langchain_core\\language_models\\llms.py:787\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    786\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 787\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    788\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\langchain_core\\language_models\\llms.py:774\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    766\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    771\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 774\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    777\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[0;32m    778\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    781\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    782\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    783\u001b[0m         )\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\langchain_huggingface\\llms\\huggingface_pipeline.py:269\u001b[0m, in \u001b[0;36mHuggingFacePipeline._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    266\u001b[0m batch_prompts \u001b[38;5;241m=\u001b[39m prompts[i : i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size]\n\u001b[0;32m    268\u001b[0m \u001b[38;5;66;03m# Process batch of prompts\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpipeline_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;66;03m# Process each response in the batch\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(responses):\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:262\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[1;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\pipelines\\base.py:1235\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[0;32m   1232\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   1233\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[0;32m   1234\u001b[0m     )\n\u001b[1;32m-> 1235\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\pipelines\\base.py:1161\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1159\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1160\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1161\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1162\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:349\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    346\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[0;32m    348\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[1;32m--> 349\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    350\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\generation\\utils.py:1914\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1906\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1907\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1908\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   1909\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1910\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1911\u001b[0m     )\n\u001b[0;32m   1913\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 1914\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1915\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1922\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   1926\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[0;32m   1927\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1928\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   1929\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[0;32m   1930\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1931\u001b[0m     )\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\generation\\utils.py:2646\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[0;32m   2643\u001b[0m unfinished_sequences \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(batch_size, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2644\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_initial_cache_position(input_ids, model_kwargs)\n\u001b[1;32m-> 2646\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_has_unfinished_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis_peer_finished\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   2647\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[0;32m   2648\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2650\u001b[0m     \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\generation\\utils.py:2085\u001b[0m, in \u001b[0;36mGenerationMixin._has_unfinished_sequences\u001b[1;34m(self, this_peer_finished, synced_gpus, device)\u001b[0m\n\u001b[0;32m   2083\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m this_peer_finished_flag\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m   2084\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 2085\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   2087\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from transformers import pipeline\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "\n",
    "# Initialize the text generation pipeline\n",
    "generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_gemma,\n",
    "    tokenizer=tokenizer_gemma,\n",
    "    device=0,  # Automatically select the device\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "# Wrap the pipeline with HuggingFacePipeline\n",
    "chat = HuggingFacePipeline(pipeline=generation_pipeline)\n",
    "\n",
    "# Create a conversational chain\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=chat,\n",
    "    retriever=docsearch.as_retriever(),\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# Chatbot loop\n",
    "print(\"Chatbot: Hello! How can I assist you today?\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in ['exit', 'quit', 'bye']:\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "\n",
    "    response = qa({\"question\": user_input})\n",
    "    print(\"Chatbot:\", response['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
