{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import albumentations as A # type: ignore\n",
    "# from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pycocotools --quiet\n",
    "# !git clone https://github.com/pytorch/vision.git\n",
    "# # !git checkout v0.3.0\n",
    "\n",
    "# !cp vision/references/detection/utils.py ./\n",
    "# !cp vision/references/detection/transforms.py ./\n",
    "# !cp vision/references/detection/coco_eval.py ./\n",
    "# !cp vision/references/detection/engine.py ./\n",
    "# !cp vision/references/detection/coco_utils.py ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('D:\\Online_Learning\\Practical_DL\\vision')\n",
    "\n",
    "# # Basic python and ML Libraries\n",
    "# import os\n",
    "# import random\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# # for ignoring warnings\n",
    "\n",
    "\n",
    "# # We will be reading images using OpenCV\n",
    "# import cv2\n",
    "\n",
    "# # xml library for parsing xml files\n",
    "# from xml.etree import ElementTree as et\n",
    "\n",
    "# # matplotlib for visualization\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "\n",
    "# # torchvision libraries\n",
    "# import torch\n",
    "# import torchvision\n",
    "# from torchvision import transforms as torchtrans\n",
    "# from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# # these are the helper libraries imported.\n",
    "# from vision.references.detection.engine import train_one_epoch, evaluate\n",
    "# import vision.references.detection.utils\n",
    "# import vision.references.detection.transforms as T\n",
    "\n",
    "# # for image augmentations\n",
    "# import albumentations as A\n",
    "# from albumentations.pytorch.transforms import ToTensorV2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Gemma 2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf2ecca150e4c7da4b4ebbccd5a0ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "# 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713e395d3ba9472bb899557952df5c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write me a poem about Machine Learning.\n",
      "\n",
      "Machines, they weave a web,\n",
      "Of algorithms\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(r\"D:\\Online_Learning\\Practical_DL\\gemma_2_2b_it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(r\"D:\\Online_Learning\\Practical_DL\\gemma_2_2b_it\",\n",
    "                                             quantization_config=quantization_config)\n",
    "\n",
    "# model.to(device)\n",
    "\n",
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write a funtion to calculate exponential in Python.\n",
      "\n",
      "```python\n",
      "def exponential(base, exponent):\n",
      "  \"\"\"\n",
      "  Calculates the exponential of a given base and exponent.\n",
      "\n",
      "  Args:\n",
      "    base: The base of the exponential.\n",
      "    exponent: The exponent of the base.\n",
      "\n",
      "  Returns:\n",
      "    The exponential of the base and exponent.\n",
      "  \"\"\"\n",
      "\n",
      "  return math.exp(base * exponent)\n",
      "```\n",
      "\n",
      "**Example Usage:**\n",
      "\n",
      "```python\n",
      "# Calculate the exponential of 2 with an exponent of 5\n",
      "result = exponential(2, 5)\n",
      "\n",
      "# Print the result\n",
      "print(result)\n",
      "```\n",
      "\n",
      "**Output:**\n",
      "\n",
      "```\n",
      "32\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* The `exponential()` function takes two arguments: `base` and `exponent`.\n",
      "* It uses the `math.exp()` function to calculate the exponential of `base` raised to the power of `exponent`.\n",
      "* The `math.exp()` function takes two arguments, `base` and `exponent`, and returns the exponential of `base` raised to the power of `exponent`.\n",
      "* The `math.exp()` function is a built-in Python function that is used to calculate the exponential of a given number.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Write a funtion to calculate exponential in Python\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=250)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Large Bart CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(r\"D:\\Online_Learning\\Practical_DL\\bart_large_cnn\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(r\"D:\\Online_Learning\\Practical_DL\\bart_large_cnn\")\n",
    "\n",
    "ARTICLE = \"\"\" President Joe Biden's campaign is testing head-to-head matchups of Vice President Kamala Harris against former President Donald Trump, a source familiar with the strategy told ABC News.\n",
    "\n",
    "It's a strategic shift that comes with increasing scrutiny on whether Biden should end his 2024 reelection campaign, as a growing number Democrats are calling on him to step aside.\n",
    "\n",
    "The source cast the move as a response to the fact that Trump has begun to attack Harris in his public statements and speeches.\n",
    "\n",
    "MORE: Biden's solo, unscripted news conference a pivotal moment in debate rebound effort\n",
    "\"Donald Trump shifted his stump speech. We'd be dumb not to adjust,\" the source told ABC News. \"We obviously pay close attention to what he is saying.\"\n",
    "\n",
    "The New York Times was first to report on the campaign's actions.\n",
    "\n",
    "A new ABC News/Washington Post/Ipsos poll found Biden continues to run evenly with Trump: Americans were divided 46-47% between Biden and Trump if the election were today.\n",
    "\n",
    "But two-thirds of Americans, including a majority of Biden's supporters, said he should step down as his party's presumptive nominee. On the issue of age, 85% of respondents said Biden he is too old for a second term, up a few points from April.\n",
    "\n",
    "Were Harris to replace Biden as the Democratic nominee, the poll found Harris leading Trump 49-46% among all adults and 49-47% among registered voters.\n",
    "\n",
    "MORE: Biden and Trump tied despite debate, as 67% call for president to drop out: POLL\n",
    "Harris recently defended Biden as she's hit the campaign trail.\n",
    "\n",
    "\"Now, we always knew this election would be tough,\" she said on Tuesday at as the campaign launched an outreach effort to Asian American voters. \"And the past few days have been a reminder that running for president of the United States is never easy.\"\n",
    "\n",
    "\"But the one thing we know about our President Joe Biden, is that he is a fighter, and he is the first to say, 'when you get knocked down you get back up,'\" Harris said.\n",
    "\n",
    "ABC News' Alexandra Hutzler contributed to this report.\n",
    "\"\"\"\n",
    "# Tokenize input text\n",
    "inputs = tokenizer([ARTICLE], max_length=1024, return_tensors='pt', truncation=True)\n",
    "\n",
    "# Generate summary\n",
    "summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=100, early_stopping=True)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biden and Trump are neck-and-neck in the race for the Democratic presidential nomination. A new CNN/ORC poll shows the race is evenly split between the two candidates. The poll also found that 67% of Americans believe that Biden should step down as the Democratic nominee.\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import math\n",
    "\n",
    "# Data preparation\n",
    "class MultiNewsDataset(Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.dataset = load_dataset(\"multi_news\", split=split)\n",
    "        self.tokenizer = get_tokenizer(\"basic_english\")\n",
    "        self.vocab = self.build_vocabulary()\n",
    "        \n",
    "    def build_vocabulary(self):\n",
    "        def yield_tokens(data_iter):\n",
    "            for item in data_iter:\n",
    "                yield self.tokenizer(item['document'])\n",
    "                yield self.tokenizer(item['summary'])\n",
    "        \n",
    "        vocab = build_vocab_from_iterator(yield_tokens(self.dataset), specials=[\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"])\n",
    "        vocab.set_default_index(vocab[\"<unk>\"])\n",
    "        return vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        document = torch.tensor([self.vocab[token] for token in self.tokenizer(item['document'])])\n",
    "        summary = torch.tensor([self.vocab[token] for token in self.tokenizer(item['summary'])])\n",
    "        return document, summary\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "# Transformer-based Summarizer\n",
    "class TransformerSummarizer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        \n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        \n",
    "        output = self.transformer(src, tgt, src_mask, tgt_mask)\n",
    "        return self.fc_out(output)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "# Hyperparameters\n",
    "VOCAB_SIZE = 50000  # We'll limit the vocabulary size for simplicity\n",
    "D_MODEL = 512\n",
    "NHEAD = 8\n",
    "NUM_ENCODER_LAYERS = 6\n",
    "NUM_DECODER_LAYERS = 6\n",
    "DIM_FEEDFORWARD = 2048\n",
    "DROPOUT = 0.1\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "# Initialize dataset, model, and optimizer\n",
    "train_dataset = MultiNewsDataset(\"train\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "model = TransformerSummarizer(VOCAB_SIZE, D_MODEL, NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, DIM_FEEDFORWARD, DROPOUT)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab[\"<pad>\"])\n",
    "\n",
    "# Training loop\n",
    "def train(model, loader, optimizer, criterion, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (src, tgt) in enumerate(loader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            src_mask = model.generate_square_subsequent_mask(src.size(1)).to(src.device)\n",
    "            tgt_mask = model.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "            \n",
    "            output = model(src, tgt[:, :-1], src_mask, tgt_mask)\n",
    "            loss = criterion(output.reshape(-1, output.shape[-1]), tgt[:, 1:].reshape(-1))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Epoch: {epoch+1}, Batch: {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Run training\n",
    "train(model, train_loader, optimizer, criterion, EPOCHS)\n",
    "\n",
    "# Generate summary function\n",
    "def generate_summary(model, text, dataset, max_length=100):\n",
    "    model.eval()\n",
    "    tokenizer = dataset.tokenizer\n",
    "    vocab = dataset.vocab\n",
    "    \n",
    "    tokens = tokenizer(text)\n",
    "    src = torch.tensor([vocab[token] for token in tokens]).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        memory = model.transformer.encoder(model.pos_encoder(model.embedding(src) * math.sqrt(model.d_model)))\n",
    "        \n",
    "        ys = torch.ones(1, 1).fill_(vocab[\"<sos>\"]).type(torch.long)\n",
    "        for i in range(max_length-1):\n",
    "            tgt_mask = model.generate_square_subsequent_mask(ys.size(1)).to(src.device)\n",
    "            out = model.transformer.decoder(model.pos_encoder(model.embedding(ys) * math.sqrt(model.d_model)), \n",
    "                                            memory, tgt_mask)\n",
    "            prob = model.fc_out(out[:, -1])\n",
    "            _, next_word = torch.max(prob, dim=1)\n",
    "            next_word = next_word.item()\n",
    "            \n",
    "            ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "            \n",
    "            if next_word == vocab[\"<eos>\"]:\n",
    "                break\n",
    "    \n",
    "    ys = ys.view(-1).tolist()\n",
    "    summary = vocab.lookup_tokens(ys)\n",
    "    summary = ' '.join([word for word in summary if word not in [\"<sos>\", \"<eos>\", \"<pad>\", \"<unk>\"]])\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have trained the model and have a test dataset\n",
    "test_dataset = MultiNewsDataset(\"test\")\n",
    "torch.save(model.state_dict(), \"transformer_summarizer.pth\")  # Save the trained model\n",
    "model.load_state_dict(torch.load(\"transformer_summarizer.pth\"))  # Load the trained model\n",
    "\n",
    "sample_text = \"Your long input text goes here...\"\n",
    "generated_summary = generate_summary(model, sample_text, test_dataset)\n",
    "print(\"Generated Summary:\", generated_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
