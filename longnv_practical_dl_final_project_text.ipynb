{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU device count: {torch.cuda.device_count()}\")\n",
    "print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "print(f\"Device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets transformers peft nltk rouge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.1+cu118\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import BartTokenizer\n",
    "\n",
    "\n",
    "def sample_dataset(dataset, fraction=0.3, seed=42):\n",
    "    \"\"\"\n",
    "    Sample a fraction of the dataset.\n",
    "    \"\"\"\n",
    "    sampled_dataset = dataset.shuffle(seed=seed)\n",
    "    num_samples = int(len(dataset) * fraction)\n",
    "    return sampled_dataset.select(range(num_samples))\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=MAX_TARGET_LENGTH, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Create the custom dataset class and data loaders\n",
    "class MultiNewsDataset(Dataset):\n",
    "    def __init__(self, tokenized_datasets, split):\n",
    "        self.dataset = tokenized_datasets[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        return {\n",
    "            'input_ids': torch.tensor(item['input_ids']),\n",
    "            'attention_mask': torch.tensor(item['attention_mask']),\n",
    "            'labels': torch.tensor(item['labels'])\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = pad_sequence([item['input_ids'] for item in batch], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = pad_sequence([item['attention_mask'] for item in batch], batch_first=True, padding_value=0)\n",
    "    labels = pad_sequence([item['labels'] for item in batch], batch_first=True, padding_value=-100)  # Assuming -100 is ignore index for labels\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "\n",
    "# 1. Load the dataset\n",
    "dataset = load_dataset(\"multi_news\")\n",
    "\n",
    "# 2. Set up the tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "# 3. Preprocess the dataset\n",
    "MAX_INPUT_LENGTH = 1024\n",
    "MAX_TARGET_LENGTH = 100\n",
    "\n",
    "# Sample 10% of each split\n",
    "train_dataset = sample_dataset(dataset['train'])\n",
    "validation_dataset = sample_dataset(dataset['validation'])\n",
    "test_dataset = sample_dataset(dataset['test'])\n",
    "\n",
    "# Create a new dataset dictionary with the sampled splits\n",
    "dataset = {\n",
    "    'train': train_dataset,\n",
    "    'validation': validation_dataset,\n",
    "    'test': test_dataset\n",
    "}\n",
    "\n",
    "\n",
    "tokenized_datasets = {}\n",
    "for split in dataset.keys():\n",
    "    tokenized_datasets[split] = dataset[split].map(preprocess_function, batched=True)\n",
    "\n",
    "train_dataset = MultiNewsDataset(tokenized_datasets, 'train')\n",
    "val_dataset = MultiNewsDataset(tokenized_datasets, 'validation')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, get_linear_schedule_with_warmup, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base model\n",
    "model_path = r\"model\\bart_large_cnn\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_summarizer(model_path, device):\n",
    "    # Load pre-trained model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "def summarize_news(model, tokenizer, news_article):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer([news_article], max_length=2048, return_tensors='pt', truncation=True)\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=100, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "model, tokenizer = load_summarizer(model_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "# Add LoRA adaptor\n",
    "model_lora = get_peft_model(model, lora_config)\n",
    "model_lora.print_trainable_parameters()\n",
    "\n",
    "# Move model to device\n",
    "model_lora.to(device)\n",
    "\n",
    "# Set up optimizer and scheduler\n",
    "optimizer = optim.AdamW(model_lora.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "num_epochs = 7\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=1,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Set CUDA_LAUNCH_BLOCKING for synchronous CUDA errors\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "def train_lora(model, train_dataloader, val_dataloader, optimizer, scheduler, num_epochs, device):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader, desc=\"Validation\"):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        print(f\"Average validation loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Run the training\n",
    "model_lora = train_lora(model_lora, train_dataloader, val_dataloader, optimizer, scheduler, num_epochs, device)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model_lora.save_pretrained(\"./model/bart_large_cnn_lora_finetuned_multinews_v2\")\n",
    "tokenizer.save_pretrained(\"./model/bart_large_cnn_lora_finetuned_multinews_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge with base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Load the base model\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "# Load the LoRA config\n",
    "lora_config = PeftConfig.from_pretrained(\"./model/bart_large_cnn_lora_finetuned_multinews_v2\")\n",
    "\n",
    "# Load the LoRA model\n",
    "lora_model = PeftModel.from_pretrained(base_model, \"./model/bart_large_cnn_lora_finetuned_multinews_v2\")\n",
    "\n",
    "# Merge the LoRA weights with the base model\n",
    "merged_model = lora_model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "merged_model.save_pretrained(\"./model/bart_large_cnn_lora_merged_202408\")\n",
    "tokenizer.save_pretrained(\"./model/bart_large_cnn_lora_merged_202408\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Path to your saved merged model\n",
    "merged_model_path = \"./model/bart_large_cnn_lora_merged_202408\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(merged_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(merged_model_path)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to generate summary\n",
    "def generate_summary(model, tokenizer, text, max_length=400, min_length=130):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "        max_length=max_length,\n",
    "        min_length=min_length,\n",
    "        # num_beams=4,\n",
    "        # length_penalty=2.0,\n",
    "        # early_stopping=True,\n",
    "        # no_repeat_ngram_size=3,  # Reduce repetition\n",
    "        # do_sample=True,  # Enable sampling\n",
    "        # top_k=50,  # Limit vocabulary for sampling\n",
    "        # top_p=0.95,  # Nucleus sampling\n",
    "    )\n",
    "\n",
    "    # Decode the generated summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50264, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartEncoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartDecoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Base Model\n",
    "model_path = r\"D:\\Online_Learning\\Practical_DL\\bart_large_cnn\"\n",
    "\n",
    "model_original = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "tokenizer_original = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "model_original.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_summarize = '''\n",
    "The widespread anti-immigrant riots in the United Kingdom of the past week, and the false viral claims that fueled them, may be the clearest, most direct example yet of the way unchecked misinformation on social media can produce violence and harm in the real world.\n",
    "\n",
    "Even after authorities identified a UK national as the suspect behind a series of deadly stabbings targeting children, false claims about the attacker‚Äôs name and origins continued to stoke anti-immigrant fervor and propel far-right demonstrations.\n",
    "\n",
    "The fake claims have circulated widely, particularly on X, the platform formerly known as Twitter, extremism researchers said. And police have openly blamed that misinformation for the violence that has wracked the country in recent days, with rioters throwing bricks at mosques, setting cars on fire and chanting anti-Islamic slogans while clashing with officers in riot gear.\n",
    "\n",
    "The events of the past week are hardly the only example of the link between online misinformation and politically motivated violence: From the Rohingya genocide to the attack on the US Capitol on January 6, 2021, false and misleading claims have consistently been at the center of high-profile incidences of political unrest and violence.\n",
    "\n",
    "It is a pattern that keeps repeating despite years of calls by governments and civil society groups for social media platforms to rein in inflammatory, hateful posts, as well as pledges by companies themselves to do more.\n",
    "\n",
    "A recent retreat from content moderation by some major platforms, however, suggests that the problem of violence fueled by misinformation may well get worse before it gets better.\n",
    "\n",
    "For nearly a decade, governments and civil rights groups have increasingly argued that online platforms have created enormous societal costs.\n",
    "\n",
    "Critics of social media have repeatedly accused the industry of putting corporate profits before users‚Äô mental health, or opening the door to foreign meddling, without doing enough to shield the world from those risks.\n",
    "\n",
    "An economist might call these negative externalities ‚Äì like pollution, they are byproducts of a profit-seeking business that, left unaddressed, everyone else must either learn to live with or mitigate, usually at great collective expense. The consequences tend to play out over long timeframes and with large-scale, systemic effects.\n",
    "\n",
    "Police hold back rioters near a burning police vehicle after disorder broke out on July 30, 2024, in Southport, England.\n",
    "Related article\n",
    "Elon Musk says ‚Äòcivil war is inevitable‚Äô as UK rocked by far-right riots. He‚Äôs part of the problem\n",
    "This week, it is hard to avoid wondering whether politically motivated violence based on nothing more than bad-faith, evidence-free speculation has become a permanent fixture among social media‚Äôs various externalities, and if we are being asked to make peace with it as a condition of living in a digitally connected world.\n",
    "\n",
    "Many social media companies have invested heavily in content moderation over the years. But the industry‚Äôs recent track record hints at a bet ‚Äì or perhaps a hope ‚Äì that just maybe, the public will tolerate a bit more pollution.\n",
    "\n",
    "There are some signs of pushback. In the European Union, officials are looking to hold social media companies accountable for spreading misinformation under the new Digital Services Act. In the UK, the Online Safety Act could take effect as soon as this year, requiring, among other things, social media platforms to remove illegal content.\n",
    "\n",
    "And even tougher rules may be on the way as a result of the riots. ‚ÄúWe‚Äôre going to have to look more broadly at social media after this disorder,‚Äù UK Prime Minister Keir Starmer said in a video distributed to media Friday.\n",
    "\n",
    "But punishments for online wrongdoing are already being handed out to individual perpetrators. On Friday, Jordan Parlour from Leeds, England, was sentenced to 20 months in jail after being convicted of publishing written material intended to stir racial hatred. The 28-year-old had posted the material on Facebook.\n",
    "\n",
    "The United States has lagged on platform regulation, partly due to congressional dysfunction and partly because of legal and constitutional differences that grant online platforms more freedom to manage their own websites.\n",
    "\n",
    "Still, lawmakers made some moves last month when the US Senate passed the Kids Online Safety Act, which aims to combat mental health harms for teens linked to social media.\n",
    "\n",
    "It may be tempting to dismiss social media‚Äôs role in the UK riots as merely a reflection of latent political trends or the result of activism that would have happened on other platforms anyway.\n",
    "\n",
    "But that distracts from the calculation that some platforms appear to have made: At least some of the time, some amount of misinformation-fueled violence is a reasonable cost for society to pay.\n",
    "\n",
    "Olesya Dmitracova and Kara Fox contributed reporting.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:603: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary:\n",
      "False and misleading claims have consistently been at the center of high-profile incidences of political unrest and violence. It is a pattern that keeps repeating despite years of calls by governments and civil society groups for social media platforms to rein in inflammatory, hateful posts. A recent retreat from content moderation by some major platforms suggests that the problem of violence fueled by misinformation may well get worse before it gets better. ‚ÄúWe‚Äôre going to have to look more broadly at social media after this disorder,‚Äù UK Prime Minister Keir Starmer said in a video distributed to media Friday. But punishments for online wrongdoing are already being handed out to individual perpetrators. On Friday, Jordan Parlour was sentenced to 20 months in jail after being convicted of publishing written material intended to stir racial hatred.\n"
     ]
    }
   ],
   "source": [
    "summary = generate_summary(model_original, tokenizer_original, text_to_summarize)\n",
    "print(\"Generated Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary:\n",
      "‚Äì The UK riots of the past week may be the clearest, most direct example yet of the way unchecked misinformation on social media can produce violence and harm in the real world, the Guardian reports. Even after authorities identified a UK national as the suspect behind a series of deadly stabbings targeting children, false claims about the attacker's name and origins continued to stoke anti-immigrant fervor and propel far-right demonstrations. The fake claims have circulated widely, particularly on X, the platform formerly known as Twitter, extremism researchers say. And police have openly blamed that misinformation for the violence that has wracked the country in recent days, with rioters\n"
     ]
    }
   ],
   "source": [
    "## Merge model\n",
    "summary = generate_summary(model, tokenizer_original, text_to_summarize)\n",
    "print(\"Generated Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "test_dataset = sample_dataset(dataset['test'])\n",
    "test_dataset_tokenized = MultiNewsDataset(tokenized_datasets, 'test')\n",
    "\n",
    "generated_summaries = [generate_summary(model, tokenizer_original, doc) for doc in test_dataset['document']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \n",
    "generated_summaries_original = [generate_summary(model_original, tokenizer_original, doc) for doc in test_dataset['document']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_32284\\1842060126.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric('rouge',trust_remote_code=True)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with ROUGE\n",
    "rouge = load_metric('rouge',trust_remote_code=True)\n",
    "\n",
    "references = test_dataset['summary']\n",
    "result_1 = rouge.compute(predictions=generated_summaries, references=references)\n",
    "result_2 = rouge.compute(predictions=generated_summaries_original, references=references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Crawled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_text = pd.read_csv('D:\\Online_Learning\\Practical_DL\\practical_dl_final_project\\scraped_texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_result = []\n",
    "summary_result_cnn = []\n",
    "for item in df_text[df_text['url'].str.contains('cnn')]['txt']:\n",
    "    summary_result.append(generate_summary(model, tokenizer_original, item))\n",
    "    summary_result_cnn.append(generate_summary(model_original, tokenizer_original, item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cnn = df_text[df_text['url'].str.contains('cnn')]['txt'].copy()\n",
    "df_cnn['summary_lora'] = summary_result\n",
    "df_cnn['summary_cnn'] = summary_result_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Äì A major ad industry group is shutting down, days after Elon Musk-owned X filed a lawsuit claiming the group illegally conspired to boycott advertising on his platform. The group, Global Alliance for Responsible Media, also known as GARM, is a voluntary ad-industry initiative run by the World Federation of Advertisers that aims to help brands avoid having their advertisements appear alongside illegal or harmful content. ‚ÄúGARM is a small, not-for-profit initiative, and recent allegations that unfortunately misconstrue its purpose and activities have caused a distraction and significantly drained its resources and finances,‚Äù the group said in a statement Friday.\n",
      "The group, Global Alliance for Responsible Media, also known as GARM, is a voluntary ad-industry initiative run by the World Federation of Advertisers. The end of GARM marks a temporary victory for Elon Musk and X CEO Linda Yaccarino, even though a judge hasn‚Äôt made a ruling yet. The lawsuit could drive away even more advertisers from X, Nandini Jammi and Claire Atkin, founders of watchdog group Check My Ads Institutewrote in an op-ed Thursday. X alsosued the progressive watchdog groupMedia Matters over itsanalysishighlighting antisemitic and pro-Nazi content on X.\n"
     ]
    }
   ],
   "source": [
    "print(summary_result[2])\n",
    "print(summary_result_cnn[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‚Äì If reelected, Donald Trump said Thursday, he\\'d try to exert direct power over monetary policy. \"I feel the president should have at least a say in there. I feel that strongly,‚Äù Trump said toward the end of hispress conference. ‚ÄúI made a lot of money. I was very successful. And I think I have a better instinct than, in many cases, people that would be on the Federal Reserve ‚Äî or the chairman.‚Äù The former president said that Fed Chair Jerome Powell, whom Trump appointed to the position in 2017, has got the timing of rate moves wrong throughout his tenure.',\n",
       " \"‚Äì In a letter sent Thursday to CrowdStrike's attorneys written by Delta‚Äôs high-powered lawyer, David Boies, the airline lashed out at the cybersecurity company, which has apologized for introducing a bug thatled to a global tech outage. CrowdStrike said it took responsibility for the initial outage, but it has said Delta was responsible for thousands of cancellations that piled up over the course of a week ‚Äì long after its competitors came back online. ‚ÄúAn apology alone in these circumstances is vastly inadequate, and when accompanied by misstatements and attempts to shift the blame to Delta of doubtful sincerity,‚Äù said Boies.\",\n",
       " '‚Äì A major ad industry group is shutting down, days after Elon Musk-owned X filed a lawsuit claiming the group illegally conspired to boycott advertising on his platform. The group, Global Alliance for Responsible Media, also known as GARM, is a voluntary ad-industry initiative run by the World Federation of Advertisers that aims to help brands avoid having their advertisements appear alongside illegal or harmful content. ‚ÄúGARM is a small, not-for-profit initiative, and recent allegations that unfortunately misconstrue its purpose and activities have caused a distraction and significantly drained its resources and finances,‚Äù the group said in a statement Friday.',\n",
       " '‚Äì ChatGPT, an artificial intelligence chatbot that sounds like a human, is getting chattier and chattering more and more with its new \"advanced voice mode,\" according to a new report from AI company OpenAI. The company is concerned that users are forming \"shared bonds\" with the chatbot because of its new human-sounding voice mode, according to the Wall Street Journal. \"ChatGPT‚Äôs advanced voice mode sounds remarkably lifelike. It responds in real time, can adjust to being interrupted, makes the kinds of noises that humans make during conversations like laughing or ‚Äúhmms,‚Äù',\n",
       " \"‚Äì Paramount Global, the storied media conglomerate, announced Thursday it will lay off 15% of its US staff and write down $6 billion in value of its cable television networks as it prepares to merge with Skydance Media. The layoffs, which will affect around 2,000 staffers in the coming weeks, are part of Paramount's bid to trim $500 million in annual costs companywide ahead of its merger with technology scion David Ellison‚Äôs SkyDance. The announcement is the latest painful sign of the dramatic changes impacting the traditional television business as consumers rapidly shift away from the cable bundle in favor of streaming services. On Wednesday, Warner Bros.\",\n",
       " \"‚Äì Retailers are getting into the spooky season earlier than ever before, and consumers can't get enough of it. The dog days of summer have traditionally been prime time for retailers to stock shelves with folders, lunch boxes and backpacks, but suddenly those items are sharing real estate with skeletons, witches and ghosts. Coming right on the heels of last year‚ÄôsSummer Christmas, welcome to Summerween. Popularized by the Disney TV show ‚ÄúGravity Falls,‚Äù TikTok users are showcasing their Summerween parties with watermelon-carved jack-o‚Äô-lanterns, coffin-shaped ice cream sandwiches and skeletons propped up on pool floats.\",\n",
       " \"‚Äì When Wall Street traders decide they‚Äôve maxed out the value they can get from a particular sector of the market ‚Äî tech, say ‚Äî they often cash out and move their profits into another area. It‚Äôs called a sector rotation, and it happens all the time as part of the natural course of a business cycle. Whenconsumersdecide they've maxed-out the value from the things they typically buy, business leaders tend to view it with alarm ‚Äî a sign that folks don‚Äôt have the money to spend and therefore a recession must be on the horizon. But what if those consumers are just behaving like traders, and finding value\",\n",
       " '‚Äì A group of senators sent a letter to the CEOs of 11 tech companies on Friday urging them to do more to address the rising threat of non-consensual explicit images online. The letter criticizes nearly a dozen tech firms for their lack of participation in two programs that make it easier for people to request the removal of explicit images and videos from the internet, CNN reports. It urges them to join the National Center for Missing and Exploited Children‚Äôs ‚ÄúTake It Down‚Äô program, which helps people remove nude or sexually explicit images or videos of children from online platforms, as well as theRevenge Porn Helpline‚Äô',\n",
       " '‚Äì The widespread anti-immigrant riots in the United Kingdom of the past week may be the clearest, most direct example yet of the way unchecked misinformation on social media can produce violence and harm in the real world. Even after authorities identified a UK national as the suspect behind a series of deadly stabbings targeting children, false claims about the attacker‚Äôs name and originscontinued to stoke anti- immigrant fervorand propel far-right demonstrations. The fake claims have circulated widely, particularly on X, the platform formerly known as Twitter,extremism researchers said. Police have openly blamed that misinformation for the violence that has wracked the country in recent days, with rioters',\n",
       " '‚Äì The Japanese Nikkei 225 index tanked more than 12% on Monday, marking its worst performance since 1987. The S&P 500 sank more than 3% and shed $1.3 trillion in value, notching its worst day since the 2022 bear market. The Dow lost 1,000 points that same day, and the Nasdaq Composite ventured further into correction territory. One trigger for theselloff was the unraveling of theJapanese yen carry trade. That‚Äôs when investors borrow yen to invest money in other assets like stocks and bonds with higher-yielding returns. That has been a popular trade in recent years']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Donald Trump said Thursday he'd try to exert direct power over monetary policy if reelected. The former president said that Fed Chair Jerome Powell has got the timing of rate moves wrong throughout his tenure. Trump has publicly feuded with Powell for years, frequently posting on social media that he disagreed with the Fed‚Äôs decision to raise rates in Powell's pre-Covid rate-hiking campaign. The Fed is designed to be an independent governing body, free from political influence, so that it cannot be bullied into making emotional decisions that could upset the delicate balance of job creation and low inflation. It takes time for rate hikes or cuts to take effect in the economy, so timing a policy decision right is a tricky game.\",\n",
       " \"CrowdStrike and Microsoft have claimed Delta‚Äôs outage lasted substantially longer than its rivals‚Äô service downtime. CrowdStrike said Delta was responsible for thousands of cancellations that piled up over the course of a week. Delta canceled 7,000 flights between July 19 and July 24 as a result of the outage. Delta customers this week have fileda class-action lawsuit, alleging that Delta refused to refundsto them with mass cancellations. The US Department of Transportation is investigating those claims and is investigating CrowdStrike and Delta's claim that Delta ignored its help to restore its systems. The battle between Delta and the tech companies kicked off late last month when Delta CEO Ed Bastian saidin an interview on CNBC that CrowdStrike was willfully negligent in its post-crash conduct.\",\n",
       " 'The group, Global Alliance for Responsible Media, also known as GARM, is a voluntary ad-industry initiative run by the World Federation of Advertisers. The end of GARM marks a temporary victory for Elon Musk and X CEO Linda Yaccarino, even though a judge hasn‚Äôt made a ruling yet. The lawsuit could drive away even more advertisers from X, Nandini Jammi and Claire Atkin, founders of watchdog group Check My Ads Institutewrote in an op-ed Thursday. X alsosued the progressive watchdog groupMedia Matters over itsanalysishighlighting antisemitic and pro-Nazi content on X.',\n",
       " \"ChatGPT‚Äôs advanced voice mode sounds remarkably lifelike. It responds in real time, can adjust to being interrupted, makes the kinds of noises that humans make during conversations like laughing or ‚Äúhmms‚Äù OpenAI says it observed users talking to ChatGPT's voice mode in language ‚Äúexpressing shared bonds‚Äù with the tool. Eventually, ‚Äúusers might form social relationships with the AI, reducing their need for human interaction ‚Äî potentially benefiting lonely individuals but possibly affecting healthy relationships,‚Äù the company said in a report. It adds that hearing information from a bot that sounds like a human could lead users to trust the tool more than they should, given AI's propensity to get things wrong.\",\n",
       " 'Paramount Global, the storied media conglomerate, announced Thursday it will lay off 15% of its US staff and write down $6 billion in value of its cable television networks. The layoffs, which will affect around 2,000 staffers in the coming weeks, are part of Paramount‚Äôs bid to trim $500 million in annual costs companywide. The announcement is the latest painful sign of the dramatic changes impacting the traditional television business as consumers rapidly shift away from the cable bundle in favor of streaming services. On Wednesday, Warner Bros. Discovery, the parent company of CNN, TNT, HGTV and other cable networks, posted a$9.1 billion write downon its television business.',\n",
       " 'Many big-box retailers are getting into spooky season earlier than ever before. Popularized by the Disney TV show ‚ÄúGravity Falls,‚Äô TikTok users are showcasing their Summerween parties with watermelon-carved jack-o‚Äô-lanterns. Starbucks released its beloved (or hated, depending on the point of view)pumpkin spice latteon Thursday, the earliest it has ever started selling the seasonal beverage. It underscores how even though consumers may want to cut back on spending, ‚Äúin the actual moment, such as when Halloween products are marketing early, they may actually splurge instead,‚Äù Kelsey Robinson, a senior partner at McKinsey said.',\n",
       " \"US economy is still growing at an annualized rate close to 3% in the most recent quarter. While any pullback in spending can be a worrying sign for the US economy, it's important to zoom out before you freak out. The travel industry is also seeing demand soften, but it‚Äôs softening relative to the huge boost it got in the months after Covid restrictions were lifted. ‚ÄúYou need a PhD plan to Disney World anymore ‚Äî they‚Äôve made it so complicated,‚Äù Pete Werner, who runs Dreams Unlimited Travel, told me last year. The high-income consumer is feeling a little bit of stress internationally, he said.\",\n",
       " 'A letter criticizes nearly a dozen tech firms for their lack of participation in two programs that make it easier for people to request the removal of non-consensual explicit images and videos from the internet. This year alone, women around the world were targeted by AI-generated pornographic images, ranging from popstar Taylor Swift tohigh school girls. Most of the companies named in the letter have policies against the creation or sharing of such images, and in some cases offer their own ways for users to report or remove such content. But the benefit of joining the group is that users need to submit only one removal request that is directed to all the participating platforms, rather than contacting each individual company one-by-one.',\n",
       " 'False and misleading claims have consistently been at the center of high-profile incidences of political unrest and violence. It is a pattern that keeps repeating despite years of calls by governments and civil society groups for social media platforms to rein in inflammatory, hateful posts. A recent retreatfrom content moderation by some major platforms suggests that the problem of violence fueled by misinformation may well get worse before it gets better. It may be tempting to dismiss social media‚Äôs role in the UK riots as merely a reflection of latent political trends or the result of activism that would have happened on other platforms. But that distracts from the calculation that some platforms appear to have made: At least some of the time, some amount of misinformation-fueled violence is a reasonable cost for society to pay.',\n",
       " \"The Japanese Nikkei 225 index tanked more than 12% on Monday, marking its worst performance since 1987. The S&P 500 sank more than 3% and shed $1.3 trillion in value, notching its worst day since the 2022 bear market. The VIX, known as Wall Street‚Äôs fear gauge, shot up to a four-year high. Despite the brutal week, stocks are still on pace to notch strong returns for the year: The Dow is up 11.5% for 2024, and the Nasdaq has climbed 11.7% for the same period.. Some investors say there could be more volatility to come, particularly since it's unclear how much more the yen carry trade could unwind.\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_result_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
