# Project Description
This project demonstrates fine-tuning large language models using Low-Rank Adaptation (LoRA) for text summarization tasks. LoRA reduces memory requirements while maintaining model performance by adding trainable rank decomposition matrices.

# Key Features
- PyTorch-based implementation:
    - Model: facebook/bart_large_cnn
    - Data for fine-tuning: multi_news from HuggingFace
- CUDA-enabled training
- Low-memory footprint using LoRA:
- 
  ![image](https://github.com/user-attachments/assets/60a69988-e4dc-4c05-9997-62192a0cf5fd)
- Text summarization optimization
