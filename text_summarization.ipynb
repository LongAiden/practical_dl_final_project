{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import albumentations as A # type: ignore\n",
    "# from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pycocotools --quiet\n",
    "# !git clone https://github.com/pytorch/vision.git\n",
    "# # !git checkout v0.3.0\n",
    "\n",
    "# !cp vision/references/detection/utils.py ./\n",
    "# !cp vision/references/detection/transforms.py ./\n",
    "# !cp vision/references/detection/coco_eval.py ./\n",
    "# !cp vision/references/detection/engine.py ./\n",
    "# !cp vision/references/detection/coco_utils.py ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('D:\\Online_Learning\\Practical_DL\\vision')\n",
    "\n",
    "# # Basic python and ML Libraries\n",
    "# import os\n",
    "# import random\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# # for ignoring warnings\n",
    "\n",
    "\n",
    "# # We will be reading images using OpenCV\n",
    "# import cv2\n",
    "\n",
    "# # xml library for parsing xml files\n",
    "# from xml.etree import ElementTree as et\n",
    "\n",
    "# # matplotlib for visualization\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "\n",
    "# # torchvision libraries\n",
    "# import torch\n",
    "# import torchvision\n",
    "# from torchvision import transforms as torchtrans\n",
    "# from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# # these are the helper libraries imported.\n",
    "# from vision.references.detection.engine import train_one_epoch, evaluate\n",
    "# import vision.references.detection.utils\n",
    "# import vision.references.detection.transforms as T\n",
    "\n",
    "# # for image augmentations\n",
    "# import albumentations as A\n",
    "# from albumentations.pytorch.transforms import ToTensorV2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Gemma 2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94aa799d9235499790653d4623ac3ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "# 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(r\"D:\\Online_Learning\\Practical_DL\\gemma_2_2b_it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(r\"D:\\Online_Learning\\Practical_DL\\gemma_2_2b_it\",\n",
    "                                             quantization_config=quantization_config)\n",
    "\n",
    "# model.to(device)\n",
    "\n",
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write a funtion to calculate exponential in Python.\n",
      "\n",
      "```python\n",
      "def exponential(base, exponent):\n",
      "  \"\"\"\n",
      "  Calculates the exponential of a given base and exponent.\n",
      "\n",
      "  Args:\n",
      "    base: The base of the exponential.\n",
      "    exponent: The exponent of the base.\n",
      "\n",
      "  Returns:\n",
      "    The exponential of the base and exponent.\n",
      "  \"\"\"\n",
      "\n",
      "  return math.exp(base * exponent)\n",
      "```\n",
      "\n",
      "**Example Usage:**\n",
      "\n",
      "```python\n",
      "# Calculate the exponential of 2 with an exponent of 5\n",
      "result = exponential(2, 5)\n",
      "\n",
      "# Print the result\n",
      "print(result)\n",
      "```\n",
      "\n",
      "**Output:**\n",
      "\n",
      "```\n",
      "32\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* The `exponential()` function takes two arguments: `base` and `exponent`.\n",
      "* It uses the `math.exp()` function to calculate the exponential of `base` raised to the power of `exponent`.\n",
      "* The `math.exp()` function takes two arguments, `base` and `exponent`, and returns the exponential of `base` raised to the power of `exponent`.\n",
      "* The `math.exp()` function is a built-in Python function that is used to calculate the exponential of a given number.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Write a funtion to calculate exponential in Python\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=250)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biden and Trump are neck-and-neck in the race for the Democratic presidential nomination. A new CNN/ORC poll shows the race is evenly split between the two candidates. The poll also found that 67% of Americans believe that Biden should step down as the Democratic nominee.\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try with Customize Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.1+cu118\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n",
      "GPU device count: 1\n",
      "Current CUDA device: 0\n",
      "Device name: NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU device count: {torch.cuda.device_count()}\")\n",
    "print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "print(f\"Device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate summary function\n",
    "# def generate_summary(model, text, dataset, max_length=100):\n",
    "#     model.eval()\n",
    "#     tokenizer = dataset.tokenizer\n",
    "#     vocab = dataset.vocab\n",
    "    \n",
    "#     tokens = tokenizer(text)\n",
    "#     src = torch.tensor([vocab[token] for token in tokens]).unsqueeze(0)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         memory = model.transformer.encoder(model.pos_encoder(model.embedding(src) * math.sqrt(model.d_model)))\n",
    "        \n",
    "#         ys = torch.ones(1, 1).fill_(vocab[\"<sos>\"]).type(torch.long)\n",
    "#         for i in range(max_length-1):\n",
    "#             tgt_mask = model.generate_square_subsequent_mask(ys.size(1)).to(src.device)\n",
    "#             out = model.transformer.decoder(model.pos_encoder(model.embedding(ys) * math.sqrt(model.d_model)), \n",
    "#                                             memory, tgt_mask)\n",
    "#             prob = model.fc_out(out[:, -1])\n",
    "#             _, next_word = torch.max(prob, dim=1)\n",
    "#             next_word = next_word.item()\n",
    "            \n",
    "#             ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "            \n",
    "#             if next_word == vocab[\"<eos>\"]:\n",
    "#                 break\n",
    "    \n",
    "#     ys = ys.view(-1).tolist()\n",
    "#     summary = vocab.lookup_tokens(ys)\n",
    "#     summary = ' '.join([word for word in summary if word not in [\"<sos>\", \"<eos>\", \"<pad>\", \"<unk>\"]])\n",
    "    \n",
    "#     return summary\n",
    "\n",
    "# # Example usage:\n",
    "# # Assuming you have trained the model and have a test dataset\n",
    "# test_dataset = MultiNewsDataset(\"test\")\n",
    "# torch.save(model.state_dict(), \"transformer_summarizer.pth\")  # Save the trained model\n",
    "# model.load_state_dict(torch.load(\"transformer_summarizer.pth\"))  # Load the trained model\n",
    "\n",
    "# sample_text = \"Your long input text goes here...\"\n",
    "# generated_summary = generate_summary(model, sample_text, test_dataset)\n",
    "# print(\"Generated Summary:\", generated_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: transformers in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (4.42.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (2024.2.0)\n",
      "Requirement already satisfied: xxhash in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: filelock in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: requests>=2.32.2 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (0.23.4)\n",
      "Requirement already satisfied: pandas in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: packaging in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohttp in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: pyarrow-hotfix in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: multiprocess in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: colorama in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08de198d226a4623afd42539c7f182e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13491 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea92602f21004a76958c762e659f7a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1686 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8357bfd8d8254f8f966975bc39d1a214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1686 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import BartTokenizer\n",
    "\n",
    "\n",
    "def sample_dataset(dataset, fraction=0.3, seed=42):\n",
    "    \"\"\"\n",
    "    Sample a fraction of the dataset.\n",
    "    \"\"\"\n",
    "    sampled_dataset = dataset.shuffle(seed=seed)\n",
    "    num_samples = int(len(dataset) * fraction)\n",
    "    return sampled_dataset.select(range(num_samples))\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=MAX_TARGET_LENGTH, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Create the custom dataset class and data loaders\n",
    "class MultiNewsDataset(Dataset):\n",
    "    def __init__(self, tokenized_datasets, split):\n",
    "        self.dataset = tokenized_datasets[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        return {\n",
    "            'input_ids': torch.tensor(item['input_ids']),\n",
    "            'attention_mask': torch.tensor(item['attention_mask']),\n",
    "            'labels': torch.tensor(item['labels'])\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = pad_sequence([item['input_ids'] for item in batch], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = pad_sequence([item['attention_mask'] for item in batch], batch_first=True, padding_value=0)\n",
    "    labels = pad_sequence([item['labels'] for item in batch], batch_first=True, padding_value=-100)  # Assuming -100 is ignore index for labels\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "\n",
    "# 1. Load the dataset\n",
    "dataset = load_dataset(\"multi_news\")\n",
    "\n",
    "# 2. Set up the tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "# 3. Preprocess the dataset\n",
    "MAX_INPUT_LENGTH = 2048\n",
    "MAX_TARGET_LENGTH = 142\n",
    "\n",
    "# Sample 10% of each split\n",
    "train_dataset = sample_dataset(dataset['train'])\n",
    "validation_dataset = sample_dataset(dataset['validation'])\n",
    "test_dataset = sample_dataset(dataset['test'])\n",
    "\n",
    "# Create a new dataset dictionary with the sampled splits\n",
    "dataset = {\n",
    "    'train': train_dataset,\n",
    "    'validation': validation_dataset,\n",
    "    'test': test_dataset\n",
    "}\n",
    "\n",
    "\n",
    "tokenized_datasets = {}\n",
    "for split in dataset.keys():\n",
    "    tokenized_datasets[split] = dataset[split].map(preprocess_function, batched=True)\n",
    "\n",
    "train_dataset = MultiNewsDataset(tokenized_datasets, 'train')\n",
    "val_dataset = MultiNewsDataset(tokenized_datasets, 'validation')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_input_ids(input_ids, vocabulary_size):\n",
    "    if input_ids.max() >= vocabulary_size or input_ids.min() < 0:\n",
    "        raise ValueError(\"input_ids contain out-of-range values\")\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
    "        \n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, 1e-8)\n",
    "        \n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output\n",
    "\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        div_term = div_term + 1e-8  # Add small epsilon to prevent division by zero\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2).to(src.device)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3).to(tgt.device)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool().to(tgt.device)\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src))).to(src.device)\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt))).to(tgt.device)\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, optimizer, criterion,\n",
    "          scheduler, device, num_epochs):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for i, batch in enumerate(tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\")):\n",
    "            src = batch['input_ids'].to(device)\n",
    "            tgt = batch['labels'].to(device)\n",
    "\n",
    "            # Use this function in your training loop\n",
    "            check_input_ids(src, len(tokenizer))\n",
    "            check_input_ids(tgt, len(tokenizer))\n",
    "\n",
    "            output = model(src, tgt[:, :-1])\n",
    "\n",
    "            # debug_forward_pass(model, src, tgt)\n",
    "\n",
    "            loss = criterion(output.contiguous().view(-1, output.size(-1)), tgt[:, 1:].contiguous().view(-1))\n",
    "            loss.backward()\n",
    "\n",
    "            # Check model\n",
    "            for name, param in model.named_parameters():\n",
    "              if param.grad is not None:\n",
    "                  if torch.isnan(param.grad).any():\n",
    "                      print(f\"NaN gradient detected in {name}\")\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_dataloader)\n",
    "        val_loss = evaluate(model, val_dataloader, criterion, device)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            src = batch['input_ids'].to(device)\n",
    "            tgt = batch['labels'].to(device)\n",
    "            \n",
    "            output = model(src, tgt[:, :-1])\n",
    "            loss = criterion(output.contiguous().view(-1, output.size(-1)), tgt[:, 1:].contiguous().view(-1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "    \n",
    "d_model = 512\n",
    "num_heads = 4\n",
    "num_layers = 5\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "src_vocab_size = len(tokenizer)\n",
    "tgt_vocab_size = len(tokenizer)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/15: 100%|██████████| 2249/2249 [07:14<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Train Loss: 6.4543, Val Loss: 5.7575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/15: 100%|██████████| 2249/2249 [07:12<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15, Train Loss: 5.4304, Val Loss: 5.1953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/15: 100%|██████████| 2249/2249 [07:12<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15, Train Loss: 4.9396, Val Loss: 4.8410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/15: 100%|██████████| 2249/2249 [07:15<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15, Train Loss: 4.5466, Val Loss: 4.4939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/15: 100%|██████████| 2249/2249 [35:49<00:00,  1.05it/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15, Train Loss: 4.1929, Val Loss: 4.2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/15: 100%|██████████| 2249/2249 [07:12<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15, Train Loss: 3.8699, Val Loss: 3.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/15: 100%|██████████| 2249/2249 [07:15<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15, Train Loss: 3.5758, Val Loss: 3.7319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/15: 100%|██████████| 2249/2249 [07:13<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15, Train Loss: 3.3147, Val Loss: 3.5192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/15: 100%|██████████| 2249/2249 [27:43<00:00,  1.35it/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15, Train Loss: 3.0883, Val Loss: 3.3713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/15: 100%|██████████| 2249/2249 [07:19<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15, Train Loss: 2.8890, Val Loss: 3.2696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11/15: 100%|██████████| 2249/2249 [07:14<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15, Train Loss: 2.7102, Val Loss: 3.1098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12/15: 100%|██████████| 2249/2249 [07:14<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15, Train Loss: 2.5557, Val Loss: 3.0421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13/15: 100%|██████████| 2249/2249 [07:15<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15, Train Loss: 2.4167, Val Loss: 2.9581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14/15: 100%|██████████| 2249/2249 [07:15<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15, Train Loss: 2.2897, Val Loss: 2.8920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15/15: 100%|██████████| 2249/2249 [07:14<00:00,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15, Train Loss: 2.1776, Val Loss: 2.7987\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "trained_model = train(model, train_dataloader, val_dataloader,\n",
    "                      optimizer, criterion, scheduler, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage:\n",
    "# Assuming you have trained the model and have a test dataset\n",
    "torch.save(trained_model.state_dict(), \"transformer_summarizer.pth\")  # Save the trained model\n",
    "trained_model.load_state_dict(torch.load(\"transformer_summarizer.pth\"))  # Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARTICLE = \"\"\" President Joe Biden's campaign is testing head-to-head matchups of Vice President Kamala Harris against former President Donald Trump, a source familiar with the strategy told ABC News.\n",
    "\n",
    "# It's a strategic shift that comes with increasing scrutiny on whether Biden should end his 2024 reelection campaign, as a growing number Democrats are calling on him to step aside.\n",
    "\n",
    "# The source cast the move as a response to the fact that Trump has begun to attack Harris in his public statements and speeches.\n",
    "\n",
    "# MORE: Biden's solo, unscripted news conference a pivotal moment in debate rebound effort\n",
    "# \"Donald Trump shifted his stump speech. We'd be dumb not to adjust,\" the source told ABC News. \"We obviously pay close attention to what he is saying.\"\n",
    "\n",
    "# The New York Times was first to report on the campaign's actions.\n",
    "\n",
    "# A new ABC News/Washington Post/Ipsos poll found Biden continues to run evenly with Trump: Americans were divided 46-47% between Biden and Trump if the election were today.\n",
    "\n",
    "# But two-thirds of Americans, including a majority of Biden's supporters, said he should step down as his party's presumptive nominee. On the issue of age, 85% of respondents said Biden he is too old for a second term, up a few points from April.\n",
    "\n",
    "# Were Harris to replace Biden as the Democratic nominee, the poll found Harris leading Trump 49-46% among all adults and 49-47% among registered voters.\n",
    "\n",
    "# MORE: Biden and Trump tied despite debate, as 67% call for president to drop out: POLL\n",
    "# Harris recently defended Biden as she's hit the campaign trail.\n",
    "\n",
    "# \"Now, we always knew this election would be tough,\" she said on Tuesday at as the campaign launched an outreach effort to Asian American voters. \"And the past few days have been a reminder that running for president of the United States is never easy.\"\n",
    "\n",
    "# \"But the one thing we know about our President Joe Biden, is that he is a fighter, and he is the first to say, 'when you get knocked down you get back up,'\" Harris said.\n",
    "\n",
    "# ABC News' Alexandra Hutzler contributed to this report.\n",
    "# \"\"\"\n",
    "# # Tokenize input text\n",
    "# inputs = tokenizer([ARTICLE], max_length=1024, return_tensors='pt', truncation=True)\n",
    "\n",
    "# # Generate summary\n",
    "# summary_ids = trained_model.generate(inputs['input_ids'], num_beams=4, max_length=100, early_stopping=True)\n",
    "# summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: On Friday, people wearing black queued in the capital, Hanoi, to see the leader before he was buried at Mai Dich Cemetery, the final resting place for many senior leaders in Vietnam.\n",
      "Images on social media showed his hearse being paraded through the streets of the capital before the burial\n",
      "\n",
      "Generated: 'sus them into into a both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both among a a a a an an office reportedly a a claims life– while killingst killing so things found her people— more Florida found us found them people are the- people\n"
     ]
    }
   ],
   "source": [
    "sample_text = '''On Friday, people wearing black queued in the capital, Hanoi, to see the leader before he was buried at Mai Dich Cemetery, the final resting place for many senior leaders in Vietnam.\n",
    "Images on social media showed his hearse being paraded through the streets of the capital before the burial'''\n",
    "\n",
    "def generate_text(model, src, max_length, device, tokenizer):\n",
    "    model.eval()\n",
    "    src = src.to(device)\n",
    "    \n",
    "    # Generate initial target sequence with start token\n",
    "    tgt = torch.ones(1, 1).fill_(tokenizer.bos_token_id).type(torch.long).to(device)\n",
    "    \n",
    "    for i in range(max_length - 1):\n",
    "        # Generate output using current source and target\n",
    "        output = model(src, tgt)\n",
    "        \n",
    "        # Get the next token prediction\n",
    "        next_token = output[:, -1, :].argmax(dim=-1).unsqueeze(1)\n",
    "        \n",
    "        # Append the next token to the target sequence\n",
    "        tgt = torch.cat([tgt, next_token], dim=1)\n",
    "        \n",
    "        # Stop if end of sequence token is generated\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    # Decode the generated sequence\n",
    "    generated_text = tokenizer.decode(tgt[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "# src_text = \"Translate this to French: Hello, how are you?\"\n",
    "src_tokens = tokenizer(sample_text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "generated_text = generate_text(trained_model, src_tokens, max_length=100, device=device, tokenizer=tokenizer)\n",
    "print(f\"Input: {sample_text}\")\n",
    "print()\n",
    "print(f\"Generated: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test With Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(r\"D:\\Online_Learning\\Practical_DL\\bart_large_cnn\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(r\"D:\\Online_Learning\\Practical_DL\\bart_large_cnn\")\n",
    "\n",
    "ARTICLE = \"\"\" President Joe Biden's campaign is testing head-to-head matchups of Vice President Kamala Harris against former President Donald Trump, a source familiar with the strategy told ABC News.\n",
    "\n",
    "It's a strategic shift that comes with increasing scrutiny on whether Biden should end his 2024 reelection campaign, as a growing number Democrats are calling on him to step aside.\n",
    "\n",
    "The source cast the move as a response to the fact that Trump has begun to attack Harris in his public statements and speeches.\n",
    "\n",
    "MORE: Biden's solo, unscripted news conference a pivotal moment in debate rebound effort\n",
    "\"Donald Trump shifted his stump speech. We'd be dumb not to adjust,\" the source told ABC News. \"We obviously pay close attention to what he is saying.\"\n",
    "\n",
    "The New York Times was first to report on the campaign's actions.\n",
    "\n",
    "A new ABC News/Washington Post/Ipsos poll found Biden continues to run evenly with Trump: Americans were divided 46-47% between Biden and Trump if the election were today.\n",
    "\n",
    "But two-thirds of Americans, including a majority of Biden's supporters, said he should step down as his party's presumptive nominee. On the issue of age, 85% of respondents said Biden he is too old for a second term, up a few points from April.\n",
    "\n",
    "Were Harris to replace Biden as the Democratic nominee, the poll found Harris leading Trump 49-46% among all adults and 49-47% among registered voters.\n",
    "\n",
    "MORE: Biden and Trump tied despite debate, as 67% call for president to drop out: POLL\n",
    "Harris recently defended Biden as she's hit the campaign trail.\n",
    "\n",
    "\"Now, we always knew this election would be tough,\" she said on Tuesday at as the campaign launched an outreach effort to Asian American voters. \"And the past few days have been a reminder that running for president of the United States is never easy.\"\n",
    "\n",
    "\"But the one thing we know about our President Joe Biden, is that he is a fighter, and he is the first to say, 'when you get knocked down you get back up,'\" Harris said.\n",
    "\n",
    "ABC News' Alexandra Hutzler contributed to this report.\n",
    "\"\"\"\n",
    "# Tokenize input text\n",
    "inputs = tokenizer([ARTICLE], max_length=1024, return_tensors='pt', truncation=True)\n",
    "\n",
    "# Generate summary\n",
    "summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=100, early_stopping=True)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.1+cu118\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n",
      "GPU: NVIDIA GeForce RTX 3060 Ti\n",
      "Total GPU memory: 8.59 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Set CUDA_LAUNCH_BLOCKING for synchronous CUDA errors\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "def get_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    return 0\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total GPU memory: {get_gpu_memory():.2f} GB\")\n",
    "\n",
    "model_path = r\"D:\\Online_Learning\\Practical_DL\\bart_large_cnn\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_summarizer(model_path, device):\n",
    "    # Load pre-trained model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "def summarize_news(model, tokenizer, news_article):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer([news_article], max_length=2048, return_tensors='pt', truncation=True)\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=100, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "model, tokenizer = load_summarizer(model_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: China’s industrial profits grew at a faster clip in June, official data showed on Saturday. A 3.6% year-on-year rise in profits last month followed a 0.7% gain in May. First-half earnings were up 3.5%, accelerating from a 3.4% increase in January-May period.\n"
     ]
    }
   ],
   "source": [
    "news_article = \"\"\"\n",
    "China’s industrial profits grew at a faster clip in June, official data showed on Saturday, even as businesses were grappling with a downshift in consumers’ sentiment amid a shaky economic recovery.\n",
    "\n",
    "A 3.6% year-on-year rise in profits last month followed a 0.7% gain in May, while first-half earnings were up 3.5%, accelerating from a 3.4% increase in the January-May period, National Bureau of Statistics (NBS) data showed.\n",
    "\n",
    "The robust data contrasted with a slowing economy, which missed forecasts in the second quarter as the consumer sector was downbeat amid job market woes and a protracted housing downturn.\n",
    "\n",
    "Roughly half of more than 10 mainland-listed alcoholic beverage firms that had released forecasts for H1 earnings expected a loss-making first half.\n",
    "\n",
    "Yet in spite of rising trade tensions with the West, optical transceiver firms Zhongji Innolight and Suzhou TFC Optical Communication  forecast multi-fold rises in first-half earnings, as the two suppliers for U.S. chip giant Nvidia\n",
    " turn out to be big winners from a global artificial intelligence build out.\n",
    "\n",
    "China is trying to provide heavier monetary stimulus to prop up its fragile economy, surprising markets for a second time on Thursday by conducting an unscheduled lending operation at steeply lower rates. Only days earlier the authorities cut several benchmark lending rates in the wake of a top leadership meeting, which had mapped out other major reforms.\n",
    "\n",
    "The country’s state planner and finance ministry announced plans on Thursday to arrange about 300 billion yuan of funds from ultra-long special treasury bonds to step up a nationwide equipment upgrade and consumer goods trade-in campaign.\n",
    "\n",
    "State-owned firms reported profits up 0.3% in the first half, foreign firms recorded an 11% gain, while private-sector companies booked a 6.8% rise, according to a breakdown of the NBS data.\n",
    "\n",
    "Industrial profit numbers cover firms with annual revenues of at least 20 million yuan ($2.75 million) from their main operations.\n",
    "\"\"\"\n",
    "summary = summarize_news(model, tokenizer, news_article)\n",
    "print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import time\n",
    "\n",
    "def compare_models(text, models):\n",
    "    results = {}\n",
    "    for model_name in models:\n",
    "        summarizer = pipeline(\"summarization\", model=model_name)\n",
    "        start_time = time.time()\n",
    "        summary = summarizer(text, max_length=130, min_length=30, do_sample=False)[0]['summary_text']\n",
    "        end_time = time.time()\n",
    "        results[model_name] = {\n",
    "            \"summary\": summary,\n",
    "            \"time\": end_time - start_time\n",
    "        }\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "news_article = \"\"\"\n",
    "The United Nations has warned that the ongoing global climate crisis is causing unprecedented changes \n",
    "to our planet. Rising sea levels, extreme weather events, and biodiversity loss are just a few of the \n",
    "challenges we face. Scientists stress the urgent need for collective action to reduce greenhouse gas \n",
    "emissions and transition to renewable energy sources.\n",
    "\"\"\"\n",
    "\n",
    "models_to_compare = [\"facebook/bart-large-cnn\", \"google/pegasus-cnn_dailymail\"]\n",
    "results = compare_models(news_article, models_to_compare)\n",
    "\n",
    "for model, result in results.items():\n",
    "    print(f\"\\nModel: {model}\")\n",
    "    print(f\"Summary: {result['summary']}\")\n",
    "    print(f\"Time taken: {result['time']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "def evaluate_summary(generated_summary, reference_summary):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(generated_summary, reference_summary)\n",
    "    return scores[0]  # We get only one score as we're comparing one summary\n",
    "\n",
    "# Example usage\n",
    "generated_summary = \"The UN warns of unprecedented changes due to the global climate crisis, including rising sea levels and biodiversity loss. Scientists urge action to reduce emissions and shift to renewable energy.\"\n",
    "reference_summary = \"UN warns of climate crisis causing unprecedented planetary changes. Rising sea levels, extreme weather, and biodiversity loss are key challenges. Scientists call for urgent action to reduce emissions and adopt renewable energy.\"\n",
    "\n",
    "scores = evaluate_summary(generated_summary, reference_summary)\n",
    "\n",
    "print(\"ROUGE Scores:\")\n",
    "for metric, score in scores.items():\n",
    "    print(f\"{metric}: {score['f']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.1+cu118\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import BartTokenizer\n",
    "\n",
    "\n",
    "def sample_dataset(dataset, fraction=0.3, seed=42):\n",
    "    \"\"\"\n",
    "    Sample a fraction of the dataset.\n",
    "    \"\"\"\n",
    "    sampled_dataset = dataset.shuffle(seed=seed)\n",
    "    num_samples = int(len(dataset) * fraction)\n",
    "    return sampled_dataset.select(range(num_samples))\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=MAX_TARGET_LENGTH, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Create the custom dataset class and data loaders\n",
    "class MultiNewsDataset(Dataset):\n",
    "    def __init__(self, tokenized_datasets, split):\n",
    "        self.dataset = tokenized_datasets[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        return {\n",
    "            'input_ids': torch.tensor(item['input_ids']),\n",
    "            'attention_mask': torch.tensor(item['attention_mask']),\n",
    "            'labels': torch.tensor(item['labels'])\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = pad_sequence([item['input_ids'] for item in batch], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = pad_sequence([item['attention_mask'] for item in batch], batch_first=True, padding_value=0)\n",
    "    labels = pad_sequence([item['labels'] for item in batch], batch_first=True, padding_value=-100)  # Assuming -100 is ignore index for labels\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "\n",
    "# 1. Load the dataset\n",
    "dataset = load_dataset(\"multi_news\")\n",
    "\n",
    "# 2. Set up the tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "# 3. Preprocess the dataset\n",
    "MAX_INPUT_LENGTH = 1024\n",
    "MAX_TARGET_LENGTH = 100\n",
    "\n",
    "# Sample 10% of each split\n",
    "train_dataset = sample_dataset(dataset['train'])\n",
    "validation_dataset = sample_dataset(dataset['validation'])\n",
    "test_dataset = sample_dataset(dataset['test'])\n",
    "\n",
    "# Create a new dataset dictionary with the sampled splits\n",
    "dataset = {\n",
    "    'train': train_dataset,\n",
    "    'validation': validation_dataset,\n",
    "    'test': test_dataset\n",
    "}\n",
    "\n",
    "\n",
    "tokenized_datasets = {}\n",
    "for split in dataset.keys():\n",
    "    tokenized_datasets[split] = dataset[split].map(preprocess_function, batched=True)\n",
    "\n",
    "train_dataset = MultiNewsDataset(tokenized_datasets, 'train')\n",
    "val_dataset = MultiNewsDataset(tokenized_datasets, 'validation')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, get_linear_schedule_with_warmup, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base model\n",
    "model_path = r\"D:\\Online_Learning\\Practical_DL\\bart_large_cnn\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_summarizer(model_path, device):\n",
    "    # Load pre-trained model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "def summarize_news(model, tokenizer, news_article):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer([news_article], max_length=2048, return_tensors='pt', truncation=True)\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=100, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "model, tokenizer = load_summarizer(model_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,179,648 || all params: 407,470,080 || trainable%: 0.2895\n"
     ]
    }
   ],
   "source": [
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "# Add LoRA adaptor\n",
    "model_lora = get_peft_model(model, lora_config)\n",
    "model_lora.print_trainable_parameters()\n",
    "\n",
    "# Move model to device\n",
    "model_lora.to(device)\n",
    "\n",
    "# Set up optimizer and scheduler\n",
    "optimizer = optim.AdamW(model_lora.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "num_epochs = 7\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=1,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7:   0%|          | 0/3373 [00:00<?, ?it/s]d:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:603: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Epoch 1/7: 100%|██████████| 3373/3373 [39:14<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 2.3824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 422/422 [02:03<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 2.2356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7: 100%|██████████| 3373/3373 [43:57<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 2.2618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 422/422 [02:18<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 2.2055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7: 100%|██████████| 3373/3373 [45:49<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 2.2362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 422/422 [02:23<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 2.1856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7: 100%|██████████| 3373/3373 [44:14<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 2.2217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 422/422 [02:12<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 2.1832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/7: 100%|██████████| 3373/3373 [42:02<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 2.2133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 422/422 [02:16<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 2.1780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/7: 100%|██████████| 3373/3373 [41:26<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 2.2075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 422/422 [02:04<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 2.1737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/7: 100%|██████████| 3373/3373 [39:40<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 2.2042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 422/422 [02:04<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 2.1742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model/bart_large_cnn_lora_finetuned_multinews_v2\\\\tokenizer_config.json',\n",
       " './model/bart_large_cnn_lora_finetuned_multinews_v2\\\\special_tokens_map.json',\n",
       " './model/bart_large_cnn_lora_finetuned_multinews_v2\\\\vocab.json',\n",
       " './model/bart_large_cnn_lora_finetuned_multinews_v2\\\\merges.txt',\n",
       " './model/bart_large_cnn_lora_finetuned_multinews_v2\\\\added_tokens.json',\n",
       " './model/bart_large_cnn_lora_finetuned_multinews_v2\\\\tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Set CUDA_LAUNCH_BLOCKING for synchronous CUDA errors\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "def train_lora(model, train_dataloader, val_dataloader, optimizer, scheduler, num_epochs, device):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader, desc=\"Validation\"):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        print(f\"Average validation loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Run the training\n",
    "model_lora = train_lora(model_lora, train_dataloader, val_dataloader, optimizer, scheduler, num_epochs, device)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model_lora.save_pretrained(\"./model/bart_large_cnn_lora_finetuned_multinews_v2\")\n",
    "tokenizer.save_pretrained(\"./model/bart_large_cnn_lora_finetuned_multinews_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge with base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('D:\\\\Online_Learning\\\\Practical_DL\\\\practical_dl_final_project\\\\model\\\\bart_large_cnn_lora_merged_202408\\\\tokenizer_config.json',\n",
       " 'D:\\\\Online_Learning\\\\Practical_DL\\\\practical_dl_final_project\\\\model\\\\bart_large_cnn_lora_merged_202408\\\\special_tokens_map.json',\n",
       " 'D:\\\\Online_Learning\\\\Practical_DL\\\\practical_dl_final_project\\\\model\\\\bart_large_cnn_lora_merged_202408\\\\vocab.json',\n",
       " 'D:\\\\Online_Learning\\\\Practical_DL\\\\practical_dl_final_project\\\\model\\\\bart_large_cnn_lora_merged_202408\\\\merges.txt',\n",
       " 'D:\\\\Online_Learning\\\\Practical_DL\\\\practical_dl_final_project\\\\model\\\\bart_large_cnn_lora_merged_202408\\\\added_tokens.json',\n",
       " 'D:\\\\Online_Learning\\\\Practical_DL\\\\practical_dl_final_project\\\\model\\\\bart_large_cnn_lora_merged_202408\\\\tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Load the base model\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "# Load the LoRA config\n",
    "lora_config = PeftConfig.from_pretrained(r\"D:\\Online_Learning\\Practical_DL\\practical_dl_final_project\\model\\bart_large_cnn_lora_finetuned_multinews_v2\")\n",
    "\n",
    "# Load the LoRA model\n",
    "lora_model = PeftModel.from_pretrained(base_model, r\"D:\\Online_Learning\\Practical_DL\\practical_dl_final_project\\model\\bart_large_cnn_lora_finetuned_multinews_v2\")\n",
    "\n",
    "# Merge the LoRA weights with the base model\n",
    "merged_model = lora_model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "merged_model.save_pretrained(r\"D:\\Online_Learning\\Practical_DL\\practical_dl_final_project\\model\\bart_large_cnn_lora_merged_202408\")\n",
    "tokenizer.save_pretrained(r\"D:\\Online_Learning\\Practical_DL\\practical_dl_final_project\\model\\bart_large_cnn_lora_merged_202408\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Path to your saved merged model\n",
    "merged_model_path = r\"./model/bart_large_cnn_lora_merged_202408\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(merged_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(merged_model_path)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to generate summary\n",
    "def generate_summary(model, tokenizer, text, max_length=500, min_length=120):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "        max_length=max_length,\n",
    "        min_length=min_length,\n",
    "        # num_beams=4,\n",
    "        # length_penalty=2.0,\n",
    "        # early_stopping=True,\n",
    "        # no_repeat_ngram_size=3,  # Reduce repetition\n",
    "        # do_sample=True,  # Enable sampling\n",
    "        # top_k=50,  # Limit vocabulary for sampling\n",
    "        # top_p=0.95,  # Nucleus sampling\n",
    "    )\n",
    "\n",
    "    # Decode the generated summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50264, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartEncoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartDecoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Base Model\n",
    "model_path = r\"D:\\Online_Learning\\Practical_DL\\bart_large_cnn\"\n",
    "\n",
    "# text_to_summarize = \"\"\"\n",
    "# The United Nations Climate Change Conference, more commonly referred to as COP26, was held in Glasgow, Scotland, United Kingdom, from 31 October to 13 November 2021. The conference was the 26th United Nations Climate Change conference and brought parties together to accelerate action towards the goals of the Paris Agreement and the UN Framework Convention on Climate Change. The conference was originally scheduled to be held in 2020, but was postponed for a year due to the COVID-19 pandemic. Some of the key outcomes of COP26 included new pledges to cut methane emissions, agreements on reducing coal usage, and commitments to end deforestation. The conference also saw increased financial pledges from developed countries to support climate action in developing nations. However, many critics argued that the agreements and commitments made at COP26 were not ambitious enough to adequately address the urgency of the climate crisis.\n",
    "# \"\"\"\n",
    "# text_to_summarize = '''\n",
    "# PHNOM PENH, Cambodia — Vietnamese President To Lam was confirmed Saturday as the new chief of the Communist Party after his predecessor died July 19.\n",
    "# Lam will be the general secretary of the Communist Party of Vietnam, the country’s most powerful political role, state media said. It was unclear if Lam will stay in his role as president.\n",
    "# The previous general secretary, Nguyen Phu Trong, dominated Vietnamese politics since he became party chief in 2011. He was elected to a third term as general secretary in 2021. He was an ideologue who viewed corruption as the gravest threat facing the party.\n",
    "# In his first speech as the Communist Party chief, Lam said that him taking the reigns was because of “an urgent need to ensure the leadership of the party.”\n",
    "# Lam said he would maintain the legacies of his predecessor, notably the anti-corruption campaign that has rocked the country’s political and business elites and a pragmatic approach to foreign policy known as bamboo diplomacy — a phrase coined by Trong referring to the plant’s flexibility, bending but not breaking in the shifting headwinds of global geopolitics.\n",
    "# Lam spent over four decades in the Ministry of Public Security before becoming the minister in 2016. As Vietnam’s top security official, Lam led Trong’s sweeping anti-graft campaign until May, when he became president following the resignation of his predecessor, who stepped down after being caught by the campaign.\n",
    "# Big changes in Vietnam’s strategic approach are unlikely, said Nguyen Khac Giang, a visiting fellow in the Vietnam Studies Program at Singapore’s ISEAS–Yusof Ishak Institute, but Lam’s relative newness to governing meant that it remains to be seen how he will lead.\n",
    "# Given the current composition of the upper echelons of Vietnamese politics, Giang said it was possible that Lam’s promotion could mean an end to the internal fighting that has rocked the party for several years.\n",
    "# “To Lam is the new unchallenged power who will dominate Vietnamese politics in the years, if not a decade, ahead,” he said.\n",
    "# Giang said the party will vote for the general secretary again in 2026, and Lam’s performance will be a factor.\n",
    "# '''\n",
    "\n",
    "text_to_summarize = '''\n",
    "Vietnamese President To Lam was confirmed Saturday as the new chief of the Communist Party after his predecessor died July 19.\n",
    "Lam will be the general secretary of the Communist Party of Vietnam, the country's most powerful political role, state media said. It was unclear if Lam will stay in his role as president.\n",
    "The previous general secretary, Nguyen Phu Trong, dominated Vietnamese politics since he became party chief in 2011. He was elected to a third term as general secretary in 2021. He was an ideologue who viewed corruption as the gravest threat facing the party.\n",
    "In his first speech as the Communist Party chief, Lam said that him taking the reigns was because of an urgent need to ensure the leadership of the party.\n",
    "Lam said he would maintain the legacies of his predecessor, notably the anti-corruption campaign that has rocked the country's political and business elites and a pragmatic approach to foreign policy known as bamboo diplomacy a phrase coined by Trong referring to the plant's flexibility, bending but not breaking in the shifting headwinds of global geopolitics.\n",
    "Lam spent over four decades in the Ministry of Public Security before becoming the minister in 2016. As Vietnam's top security official, Lam led Trong's sweeping anti-graft campaign until May, when he became president following the resignation of his predecessor, who stepped down after being caught by the campaign.\n",
    "'''\n",
    "\n",
    "model_original = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "tokenizer_original = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "model_original.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary:\n",
      "Lam will be the general secretary of the Communist Party of Vietnam, the country's most powerful political role. The previous general secretary, Nguyen Phu Trong, dominated Vietnamese politics since he became party chief in 2011. Lam led Trong's sweeping anti-graft campaign until May, when he became president following the resignation of his predecessor. It was unclear if Lam will stay in his role as president. He said that him taking the reigns was because of an urgent need to ensure the leadership of the party, state media said. He was elected to a third term as general secretary in 2021.\n"
     ]
    }
   ],
   "source": [
    "summary = generate_summary(model_original, tokenizer_original, text_to_summarize)\n",
    "print(\"Generated Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary:\n",
      "– Vietnam's President To Lam was confirmed Saturday as the new chief of the Communist Party after his predecessor died July 19. Lam will be the general secretary of Vietnam's most powerful political role, state media reports. The previous general secretary, Nguyen Phu Trong, dominated Vietnamese politics since he became party chief in 2011. He was an ideologue who viewed corruption as the gravest threat facing the party. Lam led Trong's sweeping anti-graft campaign until May, when he became president following the resignation of his predecessor, who stepped down after being caught by the campaign.\n"
     ]
    }
   ],
   "source": [
    "## Merge model\n",
    "summary = generate_summary(model, tokenizer_original, text_to_summarize)\n",
    "print(\"Generated Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "test_dataset = sample_dataset(dataset['test'])\n",
    "test_dataset_tokenized = MultiNewsDataset(tokenized_datasets, 'test')\n",
    "\n",
    "generated_summaries = [generate_summary(model, tokenizer_original, doc) for doc in test_dataset['document']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk rouge_score\n",
    "generated_summaries_original = [generate_summary(model_original, tokenizer_original, doc) for doc in test_dataset['document']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with ROUGE\n",
    "rouge = load_metric('rouge',trust_remote_code=True)\n",
    "\n",
    "references = test_dataset['summary']\n",
    "result_1 = rouge.compute(predictions=generated_summaries, references=references)\n",
    "result_2 = rouge.compute(predictions=generated_summaries_original, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=0.5479843894209953, recall=0.2676857829584659, fmeasure=0.3533563653441356), mid=Score(precision=0.55812313289974, recall=0.27486886146612516, fmeasure=0.36068234767653307), high=Score(precision=0.569104235738958, recall=0.2825283682970618, fmeasure=0.36790756661671364)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=0.1907355865739771, recall=0.09407736570854235, fmeasure=0.12366408052157919), mid=Score(precision=0.2032077097863752, recall=0.10122414256559198, fmeasure=0.13249641953702926), high=Score(precision=0.21564400942089407, recall=0.10835930117449066, fmeasure=0.1410796769284588)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=0.29764509559304503, recall=0.146092330005403, fmeasure=0.1922335065330105), mid=Score(precision=0.3074876098539968, recall=0.15247839809123387, fmeasure=0.19953579764612642), high=Score(precision=0.3172363332638653, recall=0.15890678208485798, fmeasure=0.20674121161676992)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=0.2970457021088716, recall=0.1463778467014445, fmeasure=0.19236351058140178), mid=Score(precision=0.30734257905136086, recall=0.1524967457332131, fmeasure=0.19958659320908007), high=Score(precision=0.3176014052261906, recall=0.1592276382148091, fmeasure=0.20738459779506124))}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=0.5158774107593943, recall=0.24771419195225883, fmeasure=0.32867035109277626), mid=Score(precision=0.5260309054587642, recall=0.2546000438460435, fmeasure=0.335619561508647), high=Score(precision=0.5372020575055785, recall=0.26112587057596137, fmeasure=0.3424589465885243)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=0.1584536120894146, recall=0.07572054103956338, fmeasure=0.10036693179696962), mid=Score(precision=0.16768617281386455, recall=0.08106595431193776, fmeasure=0.10698533705792487), high=Score(precision=0.17700655186715836, recall=0.08652310063926245, fmeasure=0.11368836339666258)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=0.26868490125685857, recall=0.12888077429550615, fmeasure=0.17079926841003878), mid=Score(precision=0.27607415268796076, recall=0.13385095358941979, fmeasure=0.17621709129583452), high=Score(precision=0.2839650932407216, recall=0.13877185724628593, fmeasure=0.1814324789841698)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=0.26794739587838295, recall=0.1291638529050483, fmeasure=0.17094049876901468), mid=Score(precision=0.27599289446503966, recall=0.1338253441554077, fmeasure=0.1762345623568392), high=Score(precision=0.28349147117611995, recall=0.13900820433540817, fmeasure=0.1816830298123972))}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Crawled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_text = pd.read_csv('D:\\Online_Learning\\Practical_DL\\practical_dl_final_project\\scraped_texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_result = []\n",
    "summary_result_cnn = []\n",
    "for item in df_text[df_text['url'].str.contains('cnn')]['txt']:\n",
    "    summary_result.append(generate_summary(model, tokenizer_original, item))\n",
    "    summary_result_cnn.append(generate_summary(model_original, tokenizer_original, item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cnn = df_text[df_text['url'].str.contains('cnn')]['txt'].copy()\n",
    "df_cnn['summary_lora'] = summary_result\n",
    "df_cnn['summary_cnn'] = summary_result_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "– A major ad industry group is shutting down, days after Elon Musk-owned X filed a lawsuit claiming the group illegally conspired to boycott advertising on his platform. The group, Global Alliance for Responsible Media, also known as GARM, is a voluntary ad-industry initiative run by the World Federation of Advertisers that aims to help brands avoid having their advertisements appear alongside illegal or harmful content. “GARM is a small, not-for-profit initiative, and recent allegations that unfortunately misconstrue its purpose and activities have caused a distraction and significantly drained its resources and finances,” the group said in a statement Friday.\n",
      "The group, Global Alliance for Responsible Media, also known as GARM, is a voluntary ad-industry initiative run by the World Federation of Advertisers. The end of GARM marks a temporary victory for Elon Musk and X CEO Linda Yaccarino, even though a judge hasn’t made a ruling yet. The lawsuit could drive away even more advertisers from X, Nandini Jammi and Claire Atkin, founders of watchdog group Check My Ads Institutewrote in an op-ed Thursday. X alsosued the progressive watchdog groupMedia Matters over itsanalysishighlighting antisemitic and pro-Nazi content on X.\n"
     ]
    }
   ],
   "source": [
    "print(summary_result[2])\n",
    "print(summary_result_cnn[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['– If reelected, Donald Trump said Thursday, he\\'d try to exert direct power over monetary policy. \"I feel the president should have at least a say in there. I feel that strongly,” Trump said toward the end of hispress conference. “I made a lot of money. I was very successful. And I think I have a better instinct than, in many cases, people that would be on the Federal Reserve — or the chairman.” The former president said that Fed Chair Jerome Powell, whom Trump appointed to the position in 2017, has got the timing of rate moves wrong throughout his tenure.',\n",
       " \"– In a letter sent Thursday to CrowdStrike's attorneys written by Delta’s high-powered lawyer, David Boies, the airline lashed out at the cybersecurity company, which has apologized for introducing a bug thatled to a global tech outage. CrowdStrike said it took responsibility for the initial outage, but it has said Delta was responsible for thousands of cancellations that piled up over the course of a week – long after its competitors came back online. “An apology alone in these circumstances is vastly inadequate, and when accompanied by misstatements and attempts to shift the blame to Delta\",\n",
       " '– A major ad industry group is shutting down, days after Elon Musk-owned X filed a lawsuit claiming the group illegally conspired to boycott advertising on his platform. The group, Global Alliance for Responsible Media, also known as GARM, is a voluntary ad-industry initiative run by the World Federation of Advertisers that aims to help brands avoid having their advertisements appear alongside illegal or harmful content. “GARM is a small, not-for-profit initiative, and recent allegations that unfortunately misconstrue its purpose and activities have caused a distraction and significantly drained its resources and finances,” the group said in a statement Friday.',\n",
       " '– ChatGPT, an artificial intelligence chatbot that sounds like a human, is getting chattier and chattering more and more with its new \"advanced voice mode,\" according to a new report from AI company OpenAI. The company is concerned that users are forming \"shared bonds\" with the chatbot because of its new human-sounding voice mode, according to the Wall Street Journal. \"ChatGPT\\'s advanced voice mode sounds remarkably lifelike. It responds in real time, can adjust to being interrupted, makes the kinds of noises that humans make during conversations like laughing or',\n",
       " \"– Paramount Global, the storied media conglomerate, announced Thursday it will lay off 15% of its US staff and write down $6 billion in value of its cable television networks as it prepares to merge with Skydance Media. The layoffs, which will affect around 2,000 staffers in the coming weeks, are part of Paramount's bid to trim $500 million in annual costs companywide ahead of its merger with technology scion David Ellison’s SkyDance. The announcement is the latest painful sign of the dramatic changes impacting the traditional television business as consumers rapidly shift away from the cable bundle in favor of streaming\",\n",
       " \"– Retailers are getting into the spooky season earlier than ever before, and consumers can't get enough of it. The dog days of summer have traditionally been prime time for retailers to stock shelves with folders, lunch boxes and backpacks, but suddenly those items are sharing real estate with skeletons, witches and ghosts. Coming right on the heels of last year’sSummer Christmas, welcome to Summerween. Popularized by the Disney TV show “Gravity Falls,” TikTok users are showcasing their Summerween parties with watermelon-carved jack-o’-lanterns, coffin-shaped ice cream sandwiches and skeletons propped up on pool floats.\",\n",
       " \"– When Wall Street traders decide they’ve maxed out the value they can get from a particular sector of the market — tech, say — they often cash out and move their profits into another area. It’s called a sector rotation, and it happens all the time as part of the natural course of a business cycle. Whenconsumersdecide they've maxed-out the value from the things they typically buy, business leaders tend to view it with alarm — a sign that folks don’t have the money to spend and therefore a recession must be on the horizon. But what if those consumers are just behaving like traders, and finding value\",\n",
       " '– A group of senators sent a letter to the CEOs of 11 tech companies on Friday urging them to do more to address the rising threat of non-consensual explicit images online. The letter criticizes nearly a dozen tech firms for their lack of participation in two programs that make it easier for people to request the removal of explicit images and videos from the internet, CNN reports. It urges them to join the National Center for Missing and Exploited Children’s “Take It Down’ program, which helps people remove nude or sexually explicit images or videos of children from online platforms, and theRevenge Porn Helpline’',\n",
       " '– The widespread anti-immigrant riots in the United Kingdom of the past week may be the clearest, most direct example yet of the way unchecked misinformation on social media can produce violence and harm in the real world. Even after authorities identified a UK national as the suspect behind a series of deadly stabbings targeting children, false claims about the attacker’s name and originscontinued to stoke anti- immigrant fervorand propel far-right demonstrations. The fake claims have circulated widely, particularly on X, the platform formerly known as Twitter,extremism researchers said. And police have openly blamed that misinformation for the violence that has wracked the country in recent days.',\n",
       " '– The Japanese Nikkei 225 index tanked more than 12% on Monday, marking its worst performance since 1987. The S&P 500 sank more than 3% and shed $1.3 trillion in value, notching its worst day since the 2022 bear market. The Dow lost 1,000 points that same day, and the Nasdaq Composite ventured further into correction territory. One trigger for theselloff was the unraveling of theJapanese yen carry trade. That’s when investors borrow yen to invest money in other assets like stocks and bonds with higher-yielding returns.']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summary_result_cnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msummary_result_cnn\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'summary_result_cnn' is not defined"
     ]
    }
   ],
   "source": [
    "summary_result_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Online_Learning\\\\Practical_DL\\\\practical_dl_final_project'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push to hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c44cc37cf5f64fd1b295d9672a2d1113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "# 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\n",
    "\n",
    "\n",
    "merged_model_path = r\"./model/bart_large_cnn_lora_merged_202408\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(merged_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(merged_model_path)\n",
    "\n",
    "model.push_to_hub(\"longnv2024/practical_dl_summarization\")\n",
    "tokenizer.push_to_hub(\"longnv2024/practical_dl_summarization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
