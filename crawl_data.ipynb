{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl data from Goodreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodread_df = pd.read_csv(r'D:\\Online_Learning\\Practical_DL\\final_project\\books.csv', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "def fetch_data_for_isbn(isbn):\n",
    "    return get_reviews_from_isbn(url, isbn)\n",
    "\n",
    "\n",
    "def get_reviews_from_isbn(url, isbn):\n",
    "    full_url = url+f\"{isbn}\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    # Retry mechanism\n",
    "    for attempt in range(3):  # Retry up to 3 times\n",
    "        try:\n",
    "            response = requests.get(full_url, headers=headers, allow_redirects=True, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                break\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching data for ISBN {isbn}: {e}\")\n",
    "            time.sleep(2 ** attempt)  # Exponential backoff\n",
    "        else:\n",
    "            print(f\"Failed to fetch data after {attempt + 1} attempts. Status Code: {response.status_code}\")\n",
    "            return pd.DataFrame()  # Return empty DataFrame if unsuccessful\n",
    "\n",
    "    # Parsing logic as before\n",
    "    reviews = soup.find_all('article', class_='ReviewCard')\n",
    "    all_reviews = []\n",
    "\n",
    "    for review in reviews:\n",
    "        review_data = parse_review(review, isbn)  # Modularize parsing into a function\n",
    "        all_reviews.append(review_data)\n",
    "\n",
    "    return pd.DataFrame(all_reviews)\n",
    "\n",
    "\n",
    "def parse_review(review, isbn):\n",
    "    try:\n",
    "        reviewer_name = review.find('div', {'data-testid': 'name'}).get_text(strip=True)\n",
    "        rating_section = review.find('div', class_='ShelfStatus')\n",
    "        rating = rating_section.find('span', {'role': 'img'}).get('aria-label', '').split()[1]\n",
    "        comment_section = review.find('div', {'data-testid': 'contentContainer'})\n",
    "        comment = comment_section.get_text(strip=True) if comment_section else \"No comment provided\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing review for ISBN {isbn}: {e}\")\n",
    "        return {'isbn': isbn, 'reviewer': '', 'rating': '', 'comment': ''}\n",
    "    \n",
    "    return {\n",
    "        'isbn': isbn,\n",
    "        'reviewer': reviewer_name,\n",
    "        'rating': rating,\n",
    "        'comment': comment\n",
    "    }\n",
    "\n",
    "\n",
    "def fetch_reviews_parallel(isbn_list):\n",
    "    reviews_df = []\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:  # Reduced number of workers to lessen the load\n",
    "        future_to_isbn = {executor.submit(fetch_data_for_isbn, isbn): isbn for isbn in isbn_list}\n",
    "        for future in as_completed(future_to_isbn):\n",
    "            isbn = future_to_isbn[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "                reviews_df.append(data)\n",
    "                print(f\"Data fetched for ISBN {isbn} with shape {data.shape}\")\n",
    "                time.sleep(1)  # Throttle requests\n",
    "            except Exception as exc:\n",
    "                print(f\"{isbn} generated an exception: {exc}\")\n",
    "    return reviews_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"https://www.goodreads.com/book/isbn/\"\n",
    "isbn_list = goodread_df['isbn'].unique()\n",
    "\n",
    "reviews_dataframes = fetch_reviews_parallel(isbn_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat(reviews_dataframes)\n",
    "test = test[test['reviewer']!=\"\"]\n",
    "test.to_csv('reviews_goodread.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(273442, 4)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(r'D:\\Online_Learning\\Practical_DL\\final_project\\reviews_goodread.csv')\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>rating</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0439358078</td>\n",
       "      <td>Jayson</td>\n",
       "      <td>5</td>\n",
       "      <td>(A) 86%| ExtraordinaryNotes:An angsty apprehen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0439358078</td>\n",
       "      <td>Navessa</td>\n",
       "      <td>5</td>\n",
       "      <td>Seriously, don't read this review if you haven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0439358078</td>\n",
       "      <td>Diane ϟ [ Lestrange ]</td>\n",
       "      <td>5</td>\n",
       "      <td>Interview with JK Rowling...Stephen Fry:Can we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0439358078</td>\n",
       "      <td>Jayson</td>\n",
       "      <td>5</td>\n",
       "      <td>(A) 86%| ExtraordinaryNotes:It's a transitiona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0439358078</td>\n",
       "      <td>Hannah Azerang</td>\n",
       "      <td>5</td>\n",
       "      <td>I had to re read it. I was in such a nostalgic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         isbn               reviewer  rating  \\\n",
       "0  0439358078                 Jayson       5   \n",
       "1  0439358078                Navessa       5   \n",
       "2  0439358078  Diane ϟ [ Lestrange ]       5   \n",
       "3  0439358078                 Jayson       5   \n",
       "4  0439358078         Hannah Azerang       5   \n",
       "\n",
       "                                             comment  \n",
       "0  (A) 86%| ExtraordinaryNotes:An angsty apprehen...  \n",
       "1  Seriously, don't read this review if you haven...  \n",
       "2  Interview with JK Rowling...Stephen Fry:Can we...  \n",
       "3  (A) 86%| ExtraordinaryNotes:It's a transitiona...  \n",
       "4  I had to re read it. I was in such a nostalgic...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl news from ABC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop & Shop is closing 32 underperforming grocery stores across the US northeast as part of the company’s efforts to improve its financial performance.\n",
      "Shoppers are also being squeezed by higher prices at the grocery store, with food prices experiencing modest upticks last month, according to thenewest inflation report.\n",
      "“Stop & Shop has evaluated its overall store portfolio and made the difficult decision to close underperforming stores to create a healthy base for the future growth of our brand,” said the chain’s president Gordon Reid in a release.\n",
      "Ahold Delhaize, the chain’s Dutch parent company, revealed the locations Friday adding that they will close in early November. The closures come amid a broader shake up in the US grocery industry including the rise of low-cost grocery brand Aldi, which plans to openopen 800 locations.In addition, Kroger and Albertson’s announced plans for a$25 billion mergerearlier this year. The Federal Trade Commission has sued to block it.\n",
      "Ahold Delhaize announced in May at its investor day that store reductions were planned, but didn’t specify locations. The affected stores span all five states it has locations in, including 10 in New Jersey, eight in Massachusetts, seven in New York, five in Connecticut plus two locations in Rhode Island.\n",
      "Stop & Shop said it “remains committed to serving its communities through other store locations, online shopping and home delivery services.” Employees impacted by the closures will be offered other opportunities in the company.\n",
      "Following the closures, the 110-year-old chain said it will “continue to have a strong presence across its five-state footprint with more than 350 stores.” Stop & Shop has remodeled about half of its locations since 2018, with those refreshed stores “outperforming” the ones that haven’t been updated.\n",
      "In addition to operating Stop & Shop, Ahold Delhaize owns Food Lion and Giant grocery stores in the US.\n",
      "Here’s where the locations are closing:\n",
      "100 Division St., Ansonia\n",
      "72 Newtown Road, Danbury\n",
      "855 Bridgeport Ave., Milford\n",
      "1937 West Main St., Stamford\n",
      "211 High St., Torrington\n",
      "932 North Montello St., Brockton\n",
      "341 Plymouth St., Halifax\n",
      "165 Needham St., Newton\n",
      "24 Mattakeesett St., Pembroke\n",
      "36 New State Highway, Raynham\n",
      "539-571 Boston Turnpike, Shrewsbury\n",
      "415 Cooley St., Springfield\n",
      "545 Lincoln St. Worcester\n",
      "625 Paterson Ave., Carlstadt\n",
      "1083 Inman Ave., Edison\n",
      "1049 US Highway 1 South, Edison\n",
      "1221 State Route 27, Franklin Township\n",
      "4861 US Highway 9, Howell\n",
      "2275 West County Line Rd., Jackson\n",
      "1278 US Highway 22, Phillipsburg\n",
      "581 Stelton Rd., Piscataway\n",
      "505 Richmond Ave, Point Pleasant Beach\n",
      "130 Skyline Dr., Ringwood\n",
      "2965 Cropsey Ave., Brooklyn\n",
      "294 Middle Country Road, Coram\n",
      "2525 Hempstead Turnpike, East Meadow\n",
      "130 Wheatley Plaza, Greenvale\n",
      "132 Fulton Ave., Hempstead\n",
      "240 East Sanford Blvd., Mt. Vernon\n",
      "7 Samsondale Plaza, West Haverstraw\n",
      "11 Commerce Way, Johnston\n",
      "176 Pittman St., Providence (Eastside Marketplace)\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Your HTML content\n",
    "html_content = '''<div class=\"article__content\" data-editable=\"content\" itemprop=\"articleBody\" data-reorderable=\"content\">\n",
    "                    <div data-uri=\"cms.cnn.com/_components/source/instances/clyiud13t002igyqi7yuc9krn@published\" class=\"source inline-placeholder\" data-article-gutter=\"true\">\n",
    "    <cite class=\"source__cite\">\n",
    "      <span class=\"source__location\" data-editable=\"location\">New York</span>\n",
    "      <span class=\"source__text\" data-editable=\"source\">CNN</span>\n",
    "        &nbsp;—&nbsp;\n",
    "    </cite>\n",
    "</div>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyiud13t002jgyqi08cudl1e@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            Stop &amp; Shop is closing 32 underperforming grocery stores across the US northeast as part of the company’s efforts to improve its financial performance.\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyiw4coi00303b6kcyy5k40t@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            Shoppers are also being squeezed by&nbsp;higher prices at the grocery store, with food prices experiencing modest upticks last month, according to the <a href=\"https://www.cnn.com/2024/07/11/economy/us-cpi-consumer-inflation-june/index.html\">newest inflation report.</a>\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyiumq0y00033b6k03plsc3o@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            “Stop &amp; Shop has evaluated its overall store portfolio and made the difficult decision to close underperforming stores to create a healthy base for the future growth of our brand,” said the chain’s president Gordon Reid in a release.\n",
    "    </p><div data-uri=\"cms.cnn.com/_components/ad-slot-dynamic/instances/sharethrough@published\" class=\"ad-slot-dynamic ad-slot-dynamic--1\" data-placement=\"{&quot;mobile&quot;:{&quot;position&quot;:5},&quot;desktop&quot;:{&quot;position&quot;:3}}\" data-unselectable=\"true\" style=\"display: none !important;\">\n",
    "        <div data-uri=\"cms.cnn.com/_components/ad-slot/instances/cnn-v1@published\" class=\"ad-slot\" data-path=\"end/ad-slot-dynamic[0]/items\" data-desktop-slot-id=\"ad_nat_btf_01\" data-mobile-slot-id=\"ad_nat_btf_01\" data-unselectable=\"true\" style=\"display: none !important;\"><div id=\"ad_nat_btf_01\" class=\"ad\" style=\"display: none !important;\"></div>\n",
    "        <div class=\"ad-slot__feedback ad-feedback-link-container\">\n",
    "            <div class=\"ad-slot__ad-label\"></div>\n",
    "             \n",
    "  <div data-ad-type=\"DISPLAY\" data-ad-identifier=\"ad_nat_btf_01\" class=\"ad-feedback-link\">\n",
    "    <div class=\"ad-feedback-link__label\">Ad Feedback</div>\n",
    "  </div>\n",
    "            \n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    </div>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyiuhtjk00013b6k661hhban@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            Ahold Delhaize, the chain’s Dutch parent company, revealed the locations Friday adding that they will close in early November. The closures come amid a broader shake up in the US grocery industry including the rise of low-cost grocery brand Aldi, which plans to open <a href=\"https://www.cnn.com/2024/03/07/business/aldi-expansion/index.html\">open 800 locations.</a> In addition, Kroger and Albertson’s announced plans for a <a href=\"https://www.cnn.com/2024/02/27/investing/takeaways-supermarket-merger-ftc/index.html\">$25 billion merger</a> earlier this year. The Federal Trade Commission has sued to block it.\n",
    "    </p>\n",
    "\n",
    "  <div data-uri=\"cms.cnn.com/_components/related-content/instances/clyiwbti100373b6kdnduinfg@published\" class=\"related-content related-content--article\" data-article-gutter=\"true\">\n",
    "      <a class=\"related-content__link\" href=\"/2024/03/07/business/aldi-expansion\">\n",
    "            <div class=\"related-content__image image__related-content\">\n",
    "        <div data-uri=\"cms.cnn.com/_components/image/instances/clthf5fkw002053qgc1gw35hf@published\" class=\"image image__hide-placeholder image--eq-extra-small\" data-image-variation=\"image\" data-name=\"USATSI_21739161.jpg\" data-component-name=\"image\" data-observe-resizes=\"\" data-breakpoints=\"{&quot;image--eq-extra-small&quot;: 115, &quot;image--eq-small&quot;: 300}\" data-original-ratio=\"0.648\" data-original-height=\"1944\" data-original-width=\"3000\" data-url=\"https://media.cnn.com/api/v1/images/stellar/prod/usatsi-21739161.jpg?c=original\" data-editable=\"settings\">\n",
    "      \n",
    "    <div class=\"image__container \" data-image-variation=\"image\" data-breakpoints=\"{&quot;image--eq-extra-small&quot;: 115, &quot;image--eq-small&quot;: 300, &quot;image--show-credits&quot;: 525}\">\n",
    "       <picture class=\"image__picture\"><img src=\"https://media.cnn.com/api/v1/images/stellar/prod/usatsi-21739161.jpg?c=16x9&amp;q=h_144,w_256,c_fill\" alt=\"Aldi in Loxahatchee Groves, Fla.\" class=\"image__dam-img\" onload=\"this.classList.remove('image__dam-img--loading')\" onerror=\"imageLoadError(this)\" height=\"1944\" width=\"3000\" loading=\"lazy\"></picture>\n",
    "    </div>\n",
    "    \n",
    "    \n",
    "      <div class=\"image__metadata\">\n",
    "        <div itemprop=\"caption\" class=\"image__caption attribution\">\n",
    "  \n",
    "  <span data-editable=\"metaCaption\" class=\"inline-placeholder\">Aldi in Loxahatchee Groves, Fla.</span>\n",
    "  \n",
    "</div>\n",
    "        <figcaption class=\"image__credit\">Lannis Waters/USA Today Network</figcaption>\n",
    "      </div>\n",
    "    \n",
    "</div>\n",
    "\n",
    "    </div>\n",
    "    <p class=\"related-content__headline\">\n",
    "      \n",
    "      <span class=\"related-content__title-text\" data-editable=\"content.title\">Related article</span>\n",
    "      <span class=\"related-content__headline-text\" data-editable=\"content.headline\">Aldi plans to open 800 new locations in the US</span>\n",
    "    </p>\n",
    "      </a>\n",
    "</div>\n",
    "\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyiupl3z00053b6kty38kapv@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            Ahold Delhaize announced in May at its investor day that store reductions were planned, but didn’t specify locations. The affected stores span all five states it has locations in, including 10 in New Jersey, eight in Massachusetts, seven in New York, five in Connecticut plus two locations in Rhode Island.\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyiux5t400073b6kkxw4hkgw@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            Stop &amp; Shop said it “remains committed to serving its communities through other store locations, online shopping and home delivery services.” Employees impacted by the closures will be offered other opportunities in the company.\n",
    "    </p><div data-uri=\"cms.cnn.com/_components/ad-slot-dynamic/instances/outstream@published\" class=\"ad-slot-dynamic ad-slot-dynamic--1\" data-placement=\"{&quot;mobile&quot;:{&quot;position&quot;:7},&quot;desktop&quot;:{&quot;position&quot;:6}}\" data-unselectable=\"true\" style=\"display: none !important;\">\n",
    "        <div class=\"ad-slot-dynamic__close\"></div>\n",
    "        <div data-uri=\"cms.cnn.com/_components/ad-slot/instances/cnn-v1@published\" class=\"ad-slot\" data-path=\"end/ad-slot-dynamic[1]/items\" data-desktop-slot-id=\"ad_out_vid_01\" data-mobile-slot-id=\"ad_out_vid_01\" data-unselectable=\"true\" style=\"display: none !important;\"><div id=\"ad_out_vid_01\" class=\"ad\" style=\"display: none !important;\"></div>\n",
    "        <div class=\"ad-slot__feedback ad-feedback-link-container\">\n",
    "            <div class=\"ad-slot__ad-label\"></div>\n",
    "             \n",
    "  <div data-ad-type=\"DISPLAY\" data-ad-identifier=\"ad_out_vid_01\" class=\"ad-feedback-link\">\n",
    "    <div class=\"ad-feedback-link__label\">Ad Feedback</div>\n",
    "  </div>\n",
    "            \n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    </div>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyiv97hm00093b6kekjcgodd@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            Following the closures, the 110-year-old chain said it will “continue to have a strong presence across its five-state footprint with more than 350 stores.” Stop &amp; Shop has remodeled about half of its locations since 2018, with those refreshed stores “outperforming” the ones that haven’t been updated.\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivmyc8001d3b6kj4h74cf5@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            In addition to operating Stop &amp; Shop, Ahold Delhaize owns Food Lion and Giant grocery stores in the US.\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivex2e000e3b6kacavyu5o@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            Here’s where the locations are closing:\n",
    "    </p>\n",
    "\n",
    "  <h2 class=\"subheader\" data-editable=\"text\" data-uri=\"cms.cnn.com/_components/subheader/instances/clyivfhl2000l3b6k6tmly0lf@published\" data-component-name=\"subheader\" id=\"connecticut\" data-article-gutter=\"true\">\n",
    "        Connecticut\n",
    "</h2>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivf6rf000i3b6kiijmtvfq@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            100 Division St., Ansonia\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivfv9k000n3b6kflwt2ed2@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            72 Newtown Road, Danbury\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivfwfs000p3b6k4jvmzy7n@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            855 Bridgeport Ave., Milford\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivgab5000r3b6kc7zsrexo@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            1937 West Main St., Stamford\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivgk9i000t3b6kqdxrb58b@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            211 High St., Torrington\n",
    "    </p>\n",
    "\n",
    "  <h2 class=\"subheader\" data-editable=\"text\" data-uri=\"cms.cnn.com/_components/subheader/instances/clyivh0jo000x3b6ke0vml5i8@published\" data-component-name=\"subheader\" id=\"massachusetts\" data-article-gutter=\"true\">\n",
    "        Massachusetts\n",
    "</h2>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivgolc000v3b6kt1hr6dbl@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            932 North Montello St., Brockton\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivj7tx000z3b6ko8pk9dx6@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            341 Plymouth St., Halifax\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivjoin00113b6kkt5wu7ng@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            165 Needham St., Newton\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivlabz00133b6k9inbmz66@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            24 Mattakeesett St., Pembroke\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivlk0v00153b6k0vf4lx6k@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            36 New State Highway, Raynham\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivlwa400173b6kem6ooort@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            539-571 Boston Turnpike, Shrewsbury\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivm5tv00193b6k4fdy6ttf@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            415 Cooley St., Springfield\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivmkg8001b3b6k6qfxmhq4@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            545 Lincoln St. Worcester\n",
    "    </p>\n",
    "\n",
    "  <h2 class=\"subheader\" data-editable=\"text\" data-uri=\"cms.cnn.com/_components/subheader/instances/clyivplaa001k3b6krggrmhwb@published\" data-component-name=\"subheader\" id=\"new-jersey\" data-article-gutter=\"true\">\n",
    "        New Jersey\n",
    "</h2>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivpfwp001i3b6k4ne7w4n0@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            625 Paterson Ave., Carlstadt\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivpygn001m3b6ke6qmyjtv@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            1083 Inman Ave., Edison\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivq1li001o3b6kkyt78ze9@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            1049 US Highway 1 South, Edison\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivq8ec001q3b6kxusnln3g@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            1221 State Route 27, Franklin Township\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivqfby001s3b6k9jlno8g9@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            4861 US Highway 9, Howell\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivqnvy001u3b6ke1d3oefo@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            2275 West County Line Rd., Jackson\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivqtbt001w3b6kuvgzp5e3@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            1278 US Highway 22, Phillipsburg\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivr4aa001y3b6k0snlm1om@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            581 Stelton Rd., Piscataway\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivrcvw00203b6kd5dglux2@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            505 Richmond Ave, Point Pleasant Beach\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivrk6z00223b6k7gept1pb@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            130 Skyline Dr., Ringwood\n",
    "    </p>\n",
    "\n",
    "  <h2 class=\"subheader\" data-editable=\"text\" data-uri=\"cms.cnn.com/_components/subheader/instances/clyivsyee002c3b6ksw8tt70u@published\" data-component-name=\"subheader\" id=\"new-york\" data-article-gutter=\"true\">\n",
    "        New York\n",
    "</h2>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivrqms00243b6khrtkbsnk@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            2965 Cropsey Ave., Brooklyn\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivt1sm002e3b6koba38nn4@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            294 Middle Country Road, Coram\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivu26x002g3b6kpy22skkc@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            2525 Hempstead Turnpike, East Meadow\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivu995002i3b6kqwg55j5v@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            130 Wheatley Plaza, Greenvale\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivufx9002k3b6kyovq99pb@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            132 Fulton Ave., Hempstead\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivul6w002m3b6klx6pks15@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            240 East Sanford Blvd., Mt. Vernon\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivuso8002o3b6kaa3zopzg@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            7 Samsondale Plaza, West Haverstraw\n",
    "    </p>\n",
    "\n",
    "  <h2 class=\"subheader\" data-editable=\"text\" data-uri=\"cms.cnn.com/_components/subheader/instances/clyivw8uq002u3b6kbo9wbgas@published\" data-component-name=\"subheader\" id=\"rhode-island\" data-article-gutter=\"true\">\n",
    "        Rhode Island\n",
    "</h2>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivvvxp002q3b6kfnj9cn8p@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            11 Commerce Way, Johnston\n",
    "    </p>\n",
    "\n",
    "    <p class=\"paragraph inline-placeholder vossi-paragraph-primary-core-light\" data-uri=\"cms.cnn.com/_components/paragraph/instances/clyivvzp5002s3b6k7qmned8l@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n",
    "            176 Pittman St., Providence (Eastside Marketplace)\n",
    "    </p>\n",
    "\n",
    "                </div>\n",
    "'''\n",
    "\n",
    "# Use Beautiful Soup to parse the HTML\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find all paragraph tags with the specified class\n",
    "paragraphs = soup.find_all('p', class_='paragraph inline-placeholder vossi-paragraph-primary-core-light')\n",
    "\n",
    "# Extract and print the text from each paragraph\n",
    "for paragraph in paragraphs:\n",
    "    print(paragraph.get_text(strip=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided HTML snippet\n",
    "# html_snippet = '''\n",
    "# <a href=\"/2024/07/11/business/uaw-biden-fain/index.html\" class=\"container__link container__link--type-article container_lead-plus-headlines__link\" data-link-type=\"article\" data-zjs=\"click\" data-zjs-cms_id=\"cms.cnn.com/_pages/cl9iplp6y00002vnyxpeejbzw@published\" data-zjs-canonical_url=\"https://www.cnn.com/business\" data-zjs-zone_id=\"cms.cnn.com/_components/zone/instances/cl9iplp9y00142vny1uj75le3@published\" data-zjs-zone_name=\"undefined\" data-zjs-zone_type=\"zone_layout--wide-left-balanced-2\" data-zjs-zone_position_number=\"1\" data-zjs-zone_total_number=\"9\" data-zjs-container_id=\"cms.cnn.com/_components/container/instances/clh6oomd2001l3b6dj5xigaia@published\" data-zjs-container_name=\"undefined\" data-zjs-container_type=\"container_lead-plus-headlines\" data-zjs-container_position_number=\"2\" data-zjs-container_total_number=\"2\" data-zjs-card_id=\"cms.cnn.com/_components/card/instances/clh6oomd2001l3b6dj5xigaia_fill_3@published\" data-zjs-card_name=\"UAW chief and other board members have concerns about Biden’s ability to beat Trump, source says\" data-zjs-card_type=\"card\" data-zjs-card_position_number=\"3\" data-zjs-card_total_number=\"14\">\n",
    "#     <div class=\"container__text container_lead-plus-headlines__text\">\n",
    "#         <div class=\"container__headline container_lead-plus-headlines__headline\">\n",
    "#             <span class=\"container__headline-text\" data-editable=\"headline\">UAW chief and other board members have concerns about Biden’s ability to beat Trump, source says</span>\n",
    "#         </div>\n",
    "#     </div>\n",
    "# </a>\n",
    "# '''\n",
    "\n",
    "# # Use BeautifulSoup to parse the HTML\n",
    "# soup = BeautifulSoup(html_snippet, 'html.parser')\n",
    "\n",
    "# # Find the <a> tag\n",
    "# a_tag = soup.find('a')\n",
    "\n",
    "# # Extract the href attribute\n",
    "# href = a_tag['href']\n",
    "# print(\"Extracted href:\", href)\n",
    "\n",
    "full_url = \"https://edition.cnn.com/business\"\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
    "}\n",
    "\n",
    "def get_links_from_cnn(full_url, headers, num_limit):\n",
    "    response = requests.get(full_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    links = []\n",
    "    for item in soup.find_all('a', class_='container__link', limit=num_limit):\n",
    "        links.append(item['href'])\n",
    "\n",
    "    return [\"https://edition.cnn.com\" + i for i in links]\n",
    "\n",
    "\n",
    "def get_texts_from_links(links, headers):\n",
    "    txt_lists = []\n",
    "    for url in links:\n",
    "        print(url)\n",
    "        txt_tmp = []\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all paragraph tags with the specified class\n",
    "        paragraphs = soup.find_all('p', class_='paragraph inline-placeholder vossi-paragraph-primary-core-light')\n",
    "\n",
    "        # Extract and print the text from each paragraph\n",
    "        for paragraph in paragraphs:\n",
    "            txt_tmp.append(paragraph.get_text(strip=True))\n",
    "        \n",
    "        txt_tmp = ' '.join(txt_tmp)\n",
    "        txt_lists.append(txt_tmp)\n",
    "\n",
    "    return txt_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='edition.cnn.combusiness', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000285BFF0CF40>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\site-packages\\urllib3\\connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\site-packages\\urllib3\\util\\connection.py:72\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m six\u001b[38;5;241m.\u001b[39mraise_from(\n\u001b[0;32m     69\u001b[0m         LocationParseError(\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m host), \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     )\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     73\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\socket.py:918\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    917\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 918\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    919\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\site-packages\\urllib3\\connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\site-packages\\urllib3\\connectionpool.py:404\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 404\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\site-packages\\urllib3\\connectionpool.py:1058\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1058\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\site-packages\\urllib3\\connection.py:363\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;66;03m# Add certificate verification\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    364\u001b[0m     hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\site-packages\\urllib3\\connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e\n\u001b[0;32m    188\u001b[0m     )\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x00000285BFF0CF40>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\site-packages\\urllib3\\connectionpool.py:799\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    797\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[1;32m--> 799\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\site-packages\\urllib3\\util\\retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[1;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[0;32m    594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='edition.cnn.combusiness', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000285BFF0CF40>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 83\u001b[0m\n\u001b[0;32m     79\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     81\u001b[0m }\n\u001b[0;32m     82\u001b[0m num_limit \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m---> 83\u001b[0m links_cnn \u001b[38;5;241m=\u001b[39m \u001b[43mget_links_from_cnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_limit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m links_sgt \u001b[38;5;241m=\u001b[39m get_links_from_sgtimes(category_sgt, headers, num_limit)\n\u001b[0;32m     85\u001b[0m df_txt \u001b[38;5;241m=\u001b[39m run_parallel_processing(links_cnn\u001b[38;5;241m+\u001b[39mlinks_sgt, headers)\n",
      "Cell \u001b[1;32mIn[7], line 36\u001b[0m, in \u001b[0;36mget_links_from_cnn\u001b[1;34m(category, headers, num_limit)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_links_from_cnn\u001b[39m(category, headers, num_limit):\n\u001b[0;32m     35\u001b[0m     full_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://edition.cnn.com\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m category\n\u001b[1;32m---> 36\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     39\u001b[0m     links \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\site-packages\\requests\\adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='edition.cnn.combusiness', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000285BFF0CF40>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "def get_text_from_link(url, headers):\n",
    "    print(f\"Fetching from: {url}\")\n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract text from paragraphs\n",
    "    paragraphs = soup.find_all('p')\n",
    "    return ' '.join(paragraph.get_text(strip=True) for paragraph in paragraphs)\n",
    "\n",
    "\n",
    "def get_links_from_sgtimes(category_sgt, headers, num_limit):\n",
    "    full_url = \"https://thesaigontimes.vn/\" + category_sgt\n",
    "    \n",
    "    response = requests.get(full_url, headers=headers, timeout=10)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    links = soup.find_all('h3', class_='entry-title td-module-title', limit=num_limit)\n",
    "    href_link = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            href_link.append(link.find('a')['href'])\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            print(\"Not Found\")\n",
    "\n",
    "    return href_link\n",
    "\n",
    "def get_links_from_cnn(category, headers, num_limit):\n",
    "    full_url = \"https://edition.cnn.com\" + category\n",
    "    response = requests.get(full_url, headers=headers, timeout=10)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    links = []\n",
    "    for item in soup.find_all('a', class_='container__link', limit=num_limit):\n",
    "        links.append(item['href'])\n",
    "        time.sleep(1)\n",
    "\n",
    "    return [full_url + i for i in links]\n",
    "\n",
    "\n",
    "def get_texts_from_links_parallel(links, headers):\n",
    "    # Use ThreadPoolExecutor to run get_text_from_link in parallel\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        future_to_url = {executor.submit(get_text_from_link, url, headers): url for url in links}\n",
    "        results = []\n",
    "        for future in as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "                print(f\"Completed fetching from: {url}\")\n",
    "                time.sleep(1)\n",
    "            except Exception as exc:\n",
    "                print(f\"{url} generated an exception: {exc}\")\n",
    "        return results\n",
    "\n",
    "\n",
    "def run_parallel_processing(links, headers):\n",
    "    # Get link and texts\n",
    "    texts = get_texts_from_links_parallel(links, headers)  # This processes links in parallel\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    df = pd.DataFrame.from_dict(dict(zip(links,texts)), orient='index').reset_index()\n",
    "    df.columns = ['url','txt']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# TEST WITH CNN\n",
    "category = 'business' # ['business','economy','investing','tech']\n",
    "category_sgt = 'tai-chinh-ngan-hang' # ['tai-chinh-ngan-hang','kinh-doanh']\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
    "}\n",
    "num_limit = 10\n",
    "links_cnn = get_links_from_cnn(category, headers, num_limit)\n",
    "links_sgt = get_links_from_sgtimes(category_sgt, headers, num_limit)\n",
    "df_txt = run_parallel_processing(links_cnn+links_sgt, headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='www.cnn.comtech', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001AF2DBCA7F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\site-packages\\urllib3\\connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\site-packages\\urllib3\\util\\connection.py:72\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m six\u001b[38;5;241m.\u001b[39mraise_from(\n\u001b[0;32m     69\u001b[0m         LocationParseError(\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m host), \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     )\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     73\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\socket.py:918\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    917\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 918\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    919\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\site-packages\\urllib3\\connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\site-packages\\urllib3\\connectionpool.py:404\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 404\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\site-packages\\urllib3\\connectionpool.py:1058\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1058\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\site-packages\\urllib3\\connection.py:363\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;66;03m# Add certificate verification\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    364\u001b[0m     hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\site-packages\\urllib3\\connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e\n\u001b[0;32m    188\u001b[0m     )\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x000001AF2DBCA7F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\site-packages\\urllib3\\connectionpool.py:799\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    797\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[1;32m--> 799\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\site-packages\\urllib3\\util\\retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[1;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[0;32m    594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='www.cnn.comtech', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001AF2DBCA7F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m }\n\u001b[0;32m      5\u001b[0m full_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.cnn.com\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtech\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 6\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\longnv\\lib\\site-packages\\requests\\adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='www.cnn.comtech', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001AF2DBCA7F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
    "}\n",
    "full_url = \"https://www.cnn.com\" + 'tech'\n",
    "response = requests.get(full_url, headers=headers, timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 13:24:02,028 - INFO - Fetching from: https://thesaigontimes.vn/gia-do-la-my-giam-trong-sang-26-7/\n",
      "2024-07-28 13:24:02,034 - INFO - Fetching from: https://thesaigontimes.vn/mo-loi-huy-dong-von-cho-doanh-nghiep-nho-va-vua/\n",
      "2024-07-28 13:24:02,038 - INFO - Fetching from: https://thesaigontimes.vn/de-doanh-nghiep-khong-bo-ngo-truoc-doi-hoi-nang-hang-thi-truong/\n",
      "2024-07-28 13:24:02,041 - INFO - Fetching from: https://thesaigontimes.vn/co-phieu-hbc-va-hng-bi-huy-niem-yet-bat-buoc/\n",
      "2024-07-28 13:24:02,045 - INFO - Fetching from: https://thesaigontimes.vn/cho-vay-tra-no-ngan-hang-khac-canh-tranh-ngay-cang-quyet-liet/\n",
      "2024-07-28 13:24:02,048 - INFO - Fetching from: https://thesaigontimes.vn/vi-sao-viec-giu-tran-lai-suat-do-la-my-0-lai-quan-trong/\n",
      "2024-07-28 13:24:02,049 - INFO - Fetching from: https://edition.cnn.com/2024/07/27/business/apple-union-labor-agreement/index.html\n",
      "2024-07-28 13:24:02,053 - INFO - Fetching from: https://edition.cnn.com/2024/07/27/tech/tiktok-response-to-us-ban/index.html\n",
      "2024-07-28 13:24:02,055 - INFO - Fetching from: https://edition.cnn.com/2024/07/27/business/deadpool-and-wolverine-box-office-opening-weekend/index.html\n",
      "2024-07-28 13:24:02,056 - INFO - Fetching from: https://edition.cnn.com/2024/07/27/business/zyn-nicotine-philip-morris-tobacco/index.html\n",
      "2024-07-28 13:24:03,206 - INFO - Fetching from: https://edition.cnn.com/2024/07/26/investing/stocks-rate-cuts-inflation-fed/index.html\n",
      "2024-07-28 13:24:03,207 - INFO - Completed fetching from: https://thesaigontimes.vn/co-phieu-hbc-va-hng-bi-huy-niem-yet-bat-buoc/\n",
      "2024-07-28 13:24:03,297 - INFO - Fetching from: https://edition.cnn.com/2024/07/27/success/fed-interest-rate-cuts-debt-savings/index.html\n",
      "2024-07-28 13:24:03,733 - INFO - Fetching from: https://edition.cnn.com/2024/07/27/economy/economy-harris-presidential-race/index.html\n",
      "2024-07-28 13:24:03,749 - INFO - Fetching from: https://edition.cnn.com/2024/07/26/business/toyota-recall-engine-replacement-tundra-trucks-lexus-suv/index.html\n",
      "2024-07-28 13:24:03,780 - INFO - Fetching from: https://edition.cnn.com/abc-news-trump-harris-presidential-debate-back-out/index.html\n",
      "2024-07-28 13:24:03,799 - INFO - Fetching from: https://edition.cnn.com/2024/07/27/business/haircut-women-men-more-expensive/index.html\n",
      "2024-07-28 13:24:04,261 - INFO - Completed fetching from: https://thesaigontimes.vn/de-doanh-nghiep-khong-bo-ngo-truoc-doi-hoi-nang-hang-thi-truong/\n",
      "2024-07-28 13:24:05,832 - INFO - Completed fetching from: https://thesaigontimes.vn/vi-sao-viec-giu-tran-lai-suat-do-la-my-0-lai-quan-trong/\n",
      "2024-07-28 13:24:06,960 - INFO - Completed fetching from: https://thesaigontimes.vn/gia-do-la-my-giam-trong-sang-26-7/\n",
      "2024-07-28 13:24:09,817 - INFO - Completed fetching from: https://thesaigontimes.vn/cho-vay-tra-no-ngan-hang-khac-canh-tranh-ngay-cang-quyet-liet/\n",
      "2024-07-28 13:24:12,274 - INFO - Completed fetching from: https://thesaigontimes.vn/mo-loi-huy-dong-von-cho-doanh-nghiep-nho-va-vua/\n",
      "2024-07-28 13:24:13,909 - INFO - Completed fetching from: https://edition.cnn.com/2024/07/27/business/deadpool-and-wolverine-box-office-opening-weekend/index.html\n",
      "2024-07-28 13:24:16,565 - INFO - Completed fetching from: https://edition.cnn.com/2024/07/27/business/apple-union-labor-agreement/index.html\n",
      "2024-07-28 13:24:18,846 - INFO - Completed fetching from: https://edition.cnn.com/2024/07/27/business/zyn-nicotine-philip-morris-tobacco/index.html\n",
      "2024-07-28 13:24:21,843 - INFO - Completed fetching from: https://edition.cnn.com/2024/07/27/tech/tiktok-response-to-us-ban/index.html\n",
      "2024-07-28 13:24:24,770 - INFO - Completed fetching from: https://edition.cnn.com/2024/07/26/investing/stocks-rate-cuts-inflation-fed/index.html\n",
      "2024-07-28 13:24:26,179 - INFO - Completed fetching from: https://edition.cnn.com/2024/07/27/business/haircut-women-men-more-expensive/index.html\n",
      "2024-07-28 13:24:28,789 - INFO - Completed fetching from: https://edition.cnn.com/2024/07/27/economy/economy-harris-presidential-race/index.html\n",
      "2024-07-28 13:24:30,773 - INFO - Completed fetching from: https://edition.cnn.com/2024/07/27/success/fed-interest-rate-cuts-debt-savings/index.html\n",
      "2024-07-28 13:24:31,787 - INFO - Completed fetching from: https://edition.cnn.com/abc-news-trump-harris-presidential-debate-back-out/index.html\n",
      "2024-07-28 13:24:32,986 - INFO - Completed fetching from: https://edition.cnn.com/2024/07/26/business/toyota-recall-engine-replacement-tundra-trucks-lexus-suv/index.html\n",
      "2024-07-28 13:24:34,894 - INFO - Successfully fetched texts from all links.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Logs:\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from urllib.robotparser import RobotFileParser\n",
    "import random\n",
    "import logging\n",
    "\n",
    "# Define a custom logging handler to capture log records in a list\n",
    "class ListHandler(logging.Handler):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.log_records = []\n",
    "\n",
    "    def emit(self, record):\n",
    "        self.log_records.append(record)\n",
    "\n",
    "# Set up basic logging configuration\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Create an instance of the custom handler and add it to the root logger\n",
    "list_handler = ListHandler()\n",
    "logging.getLogger().addHandler(list_handler)\n",
    "\n",
    "# User-agent rotation\n",
    "user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:85.0) Gecko/20100101 Firefox/85.0',\n",
    "]\n",
    "\n",
    "def check_robots_txt(url):\n",
    "    rp = RobotFileParser()\n",
    "    rp.set_url(url + \"/robots.txt\")\n",
    "    rp.read()\n",
    "    return rp\n",
    "\n",
    "def get_links_from_sgtimes(category_sgt, headers, num_limit, rp):\n",
    "    full_url = \"https://thesaigontimes.vn/\" + category_sgt\n",
    "\n",
    "    if not rp.can_fetch(headers['User-Agent'], full_url):\n",
    "        logging.warning(f\"Fetching from {full_url} is disallowed by robots.txt\")\n",
    "        return []\n",
    "\n",
    "    response = requests.get(full_url, headers=headers, timeout=10)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    links = soup.find_all('h3', class_='entry-title td-module-title', limit=num_limit)\n",
    "    href_link = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            href_link.append(link.find('a')['href'])\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            logging.error(\"Not Found\")\n",
    "\n",
    "    return list(set(href_link))\n",
    "\n",
    "def get_links_from_cnn(category, headers, num_limit, rp):\n",
    "    full_url = \"https://edition.cnn.com\" + category\n",
    "\n",
    "    if not rp.can_fetch(headers['User-Agent'], full_url):\n",
    "        logging.warning(f\"Fetching from {full_url} is disallowed by robots.txt\")\n",
    "        print(f\"Fetching from {full_url} is disallowed by robots.txt\")\n",
    "        return []\n",
    "\n",
    "    response = requests.get(full_url, headers=headers, timeout=10)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    links = set()\n",
    "    page_number = 1\n",
    "\n",
    "    while len(links) < num_limit:\n",
    "        response = requests.get(full_url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        for item in soup.find_all('a', class_='container__link'):\n",
    "            links.add(\"https://edition.cnn.com\" + item['href'])\n",
    "            if len(links) >= num_limit:\n",
    "                break\n",
    "\n",
    "        page_number += 1\n",
    "        time.sleep(1)  # Throttle requests\n",
    "\n",
    "    return list(links)[:num_limit]\n",
    "\n",
    "def get_text_from_link(url, headers, failed_links):\n",
    "    logging.info(f\"Fetching from: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            logging.error(f\"Failed to fetch data from {url}, status code: {response.status_code}\")\n",
    "            failed_links.append(url)\n",
    "            return \"\"\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        paragraphs = soup.find_all('p')\n",
    "        return ' '.join(paragraph.get_text(strip=True) for paragraph in paragraphs)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Exception occurred while fetching from {url}: {e}\")\n",
    "        failed_links.append(url)\n",
    "        return \"\"\n",
    "\n",
    "def get_texts_from_links_parallel(links, headers):\n",
    "    failed_links = []\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        future_to_url = {executor.submit(get_text_from_link, url, headers, failed_links): url for url in links}\n",
    "        results = []\n",
    "        for future in as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "                logging.info(f\"Completed fetching from: {url}\")\n",
    "                time.sleep(random.uniform(1, 3))  # Random delay to throttle requests\n",
    "            except Exception as exc:\n",
    "                logging.error(f\"{url} generated an exception: {exc}\")\n",
    "                failed_links.append(url)\n",
    "        return results, failed_links\n",
    "\n",
    "def run_parallel_processing(links, headers):\n",
    "    texts, failed_links = get_texts_from_links_parallel(links, headers)\n",
    "    df = pd.DataFrame.from_dict(dict(zip(links, texts)), orient='index').reset_index()\n",
    "    df.columns = ['url', 'txt']\n",
    "    return df, failed_links\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    category = '/business'  # Example category for CNN\n",
    "    category_sgt = 'tai-chinh-ngan-hang'  # Example category for Saigon Times\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': random.choice(user_agents)\n",
    "    }\n",
    "    num_limit = 10\n",
    "\n",
    "    # Check robots.txt\n",
    "    cnn_rp = check_robots_txt(\"https://edition.cnn.com\")\n",
    "    sgt_rp = check_robots_txt(\"https://thesaigontimes.vn\")\n",
    "\n",
    "    links_cnn = get_links_from_cnn(category, headers, num_limit, cnn_rp)\n",
    "    links_sgt = get_links_from_sgtimes(category_sgt, headers, num_limit, sgt_rp)\n",
    "\n",
    "    df_txt, failed_links = run_parallel_processing(links_sgt+links_cnn, headers)\n",
    "    df_txt.to_csv('scraped_texts.csv', index=False)\n",
    "\n",
    "    # Log failed links\n",
    "    if failed_links:\n",
    "        logging.info(f\"Failed to fetch texts from the following links: {failed_links}\")\n",
    "    else:\n",
    "        logging.info(\"Successfully fetched texts from all links.\")\n",
    "\n",
    "    # Retrieve error log content\n",
    "    error_logs = [record.getMessage() for record in list_handler.log_records if record.levelno == logging.ERROR]\n",
    "    print(\"Error Logs:\\n\", \"\\n\".join(error_logs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('scraped_texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Markets Hot Stocks Fear & Greed Index Latest Market News Hot Stocks Apple and the union representing retail workers at its store in Towson, Maryland, agreed to a tentative labor deal late Friday in the first US labor agreement not only for an Apple store but for any US workers of the tech giant. Workers at the Apple store in Towson hadvoted to join the International Association of Machinists unionin June 2022 and have since been seeking their first contract. In May, theyvoted to authorize a strikewithout providing a deadline. The labor deal, which needs to be ratified by a vote of the 85 rank-and-file members at the store before it can take effect, is a significant milestone. Other high-profile union organizing efforts, such as those atStarbucksandAmazon, have yet to produce deals for those workers, even though workers at those companies voted to join unions well before the workers at the Apple store in Maryland. There are not many legal requirements to force a company to reach a labor agreement with a new union once that union has been recognized by the National Labor Relations Board, the government body that oversees labor relations for most US business. But the process can take a long time, as one recent study byBloomberg Lawfound the average time for reaching a first contract is 465 days, or roughly 15 months. In many cases, it can take longer. A separate2023 academic studyfound 43% of new unions were still seeking their first contract two years after winning a representation election. The Machinists union said the Apple store deal includes scheduling improvements for a better work-life balance, which was considered a major issue in the talks. The deal also includes pay increases of 10% over the three-year life of the contract, as well as job protections such as guarantees for severance packages for laid-off workers and limits on contract employees. “By reaching a tentative agreement with Apple, we are giving our members a voice in their futures and a strong first step toward further gains,” the union said a statement. The union added that it will now look to win the right to represent retail workers at other stores. “Together, we can build on this success in store after store and grow the power (the union) has started here in Maryland,” the union said. Apple has about 270 Applestores in the United States, which are all owned by the company. Only one other store, in Oklahoma City, hasvoted to unionize, joining a different union, the Communications Workers of America. That store is not covered by this tentative labor deal. An Apple spokesperson declined to comment to CNN on this week’s tentative agreement, other than to point to an earlier statement in which the company said, “We deeply value our team members and we’re proud to provide them with industry leading compensation and exceptional benefits.” Most stock quote data provided by BATS. US market indices are shown in real time, except for the S&P 500 which is refreshed every two minutes. All times are ET. Factset: FactSet Research Systems Inc. All rights reserved. Chicago Mercantile: Certain market data is the property of Chicago Mercantile Exchange Inc. and its licensors. All rights reserved. Dow Jones: The Dow Jones branded indices are proprietary to and are calculated, distributed and marketed by DJI Opco, a subsidiary of S&P Dow Jones Indices LLC and have been licensed for use to S&P Opco, LLC and CNN. Standard & Poor’s and S&P are registered trademarks of Standard & Poor’s Financial Services LLC and Dow Jones is a registered trademark of Dow Jones Trademark Holdings LLC. All content of the Dow Jones branded indices Copyright S&P Dow Jones Indices LLC and/or its affiliates. Fair value provided by IndexArb.com. Market holidays and trading hours provided by Copp Clark Limited. © 2024 Cable News Network. A Warner Bros. Discovery Company. All Rights Reserved.CNN Sans ™ & © 2016 Cable News Network.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['txt'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
