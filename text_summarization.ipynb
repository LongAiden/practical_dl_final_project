{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import albumentations as A # type: ignore\n",
    "# from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pycocotools --quiet\n",
    "# !git clone https://github.com/pytorch/vision.git\n",
    "# # !git checkout v0.3.0\n",
    "\n",
    "# !cp vision/references/detection/utils.py ./\n",
    "# !cp vision/references/detection/transforms.py ./\n",
    "# !cp vision/references/detection/coco_eval.py ./\n",
    "# !cp vision/references/detection/engine.py ./\n",
    "# !cp vision/references/detection/coco_utils.py ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('D:\\Online_Learning\\Practical_DL\\vision')\n",
    "\n",
    "# # Basic python and ML Libraries\n",
    "# import os\n",
    "# import random\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# # for ignoring warnings\n",
    "\n",
    "\n",
    "# # We will be reading images using OpenCV\n",
    "# import cv2\n",
    "\n",
    "# # xml library for parsing xml files\n",
    "# from xml.etree import ElementTree as et\n",
    "\n",
    "# # matplotlib for visualization\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "\n",
    "# # torchvision libraries\n",
    "# import torch\n",
    "# import torchvision\n",
    "# from torchvision import transforms as torchtrans\n",
    "# from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# # these are the helper libraries imported.\n",
    "# from vision.references.detection.engine import train_one_epoch, evaluate\n",
    "# import vision.references.detection.utils\n",
    "# import vision.references.detection.transforms as T\n",
    "\n",
    "# # for image augmentations\n",
    "# import albumentations as A\n",
    "# from albumentations.pytorch.transforms import ToTensorV2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Gemma 2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63537f9e99b44239b2f6794672f23ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "# 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(r\"D:\\Online_Learning\\Practical_DL\\gemma_2_2b_it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(r\"D:\\Online_Learning\\Practical_DL\\gemma_2_2b_it\",\n",
    "                                             quantization_config=quantization_config)\n",
    "\n",
    "# model.to(device)\n",
    "\n",
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write a funtion to calculate exponential in Python.\n",
      "\n",
      "```python\n",
      "def exponential(base, exponent):\n",
      "  \"\"\"\n",
      "  Calculates the exponential of a given base and exponent.\n",
      "\n",
      "  Args:\n",
      "    base: The base of the exponential.\n",
      "    exponent: The exponent of the base.\n",
      "\n",
      "  Returns:\n",
      "    The exponential of the base and exponent.\n",
      "  \"\"\"\n",
      "\n",
      "  return math.exp(base * exponent)\n",
      "```\n",
      "\n",
      "**Example Usage:**\n",
      "\n",
      "```python\n",
      "# Calculate the exponential of 2 with an exponent of 5\n",
      "result = exponential(2, 5)\n",
      "\n",
      "# Print the result\n",
      "print(result)\n",
      "```\n",
      "\n",
      "**Output:**\n",
      "\n",
      "```\n",
      "32\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* The `exponential()` function takes two arguments: `base` and `exponent`.\n",
      "* It uses the `math.exp()` function to calculate the exponential of `base` raised to the power of `exponent`.\n",
      "* The `math.exp()` function takes two arguments, `base` and `exponent`, and returns the exponential of `base` raised to the power of `exponent`.\n",
      "* The `math.exp()` function is a built-in Python function that is used to calculate the exponential of a given number.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Write a funtion to calculate exponential in Python\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=250)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biden and Trump are neck-and-neck in the race for the Democratic presidential nomination. A new CNN/ORC poll shows the race is evenly split between the two candidates. The poll also found that 67% of Americans believe that Biden should step down as the Democratic nominee.\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try with Customize Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.1+cu118\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n",
      "GPU device count: 1\n",
      "Current CUDA device: 0\n",
      "Device name: NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU device count: {torch.cuda.device_count()}\")\n",
    "print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "print(f\"Device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate summary function\n",
    "# def generate_summary(model, text, dataset, max_length=100):\n",
    "#     model.eval()\n",
    "#     tokenizer = dataset.tokenizer\n",
    "#     vocab = dataset.vocab\n",
    "    \n",
    "#     tokens = tokenizer(text)\n",
    "#     src = torch.tensor([vocab[token] for token in tokens]).unsqueeze(0)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         memory = model.transformer.encoder(model.pos_encoder(model.embedding(src) * math.sqrt(model.d_model)))\n",
    "        \n",
    "#         ys = torch.ones(1, 1).fill_(vocab[\"<sos>\"]).type(torch.long)\n",
    "#         for i in range(max_length-1):\n",
    "#             tgt_mask = model.generate_square_subsequent_mask(ys.size(1)).to(src.device)\n",
    "#             out = model.transformer.decoder(model.pos_encoder(model.embedding(ys) * math.sqrt(model.d_model)), \n",
    "#                                             memory, tgt_mask)\n",
    "#             prob = model.fc_out(out[:, -1])\n",
    "#             _, next_word = torch.max(prob, dim=1)\n",
    "#             next_word = next_word.item()\n",
    "            \n",
    "#             ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "            \n",
    "#             if next_word == vocab[\"<eos>\"]:\n",
    "#                 break\n",
    "    \n",
    "#     ys = ys.view(-1).tolist()\n",
    "#     summary = vocab.lookup_tokens(ys)\n",
    "#     summary = ' '.join([word for word in summary if word not in [\"<sos>\", \"<eos>\", \"<pad>\", \"<unk>\"]])\n",
    "    \n",
    "#     return summary\n",
    "\n",
    "# # Example usage:\n",
    "# # Assuming you have trained the model and have a test dataset\n",
    "# test_dataset = MultiNewsDataset(\"test\")\n",
    "# torch.save(model.state_dict(), \"transformer_summarizer.pth\")  # Save the trained model\n",
    "# model.load_state_dict(torch.load(\"transformer_summarizer.pth\"))  # Load the trained model\n",
    "\n",
    "# sample_text = \"Your long input text goes here...\"\n",
    "# generated_summary = generate_summary(model, sample_text, test_dataset)\n",
    "# print(\"Generated Summary:\", generated_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: transformers in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (4.42.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (2024.2.0)\n",
      "Requirement already satisfied: xxhash in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: filelock in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: requests>=2.32.2 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (0.23.4)\n",
      "Requirement already satisfied: pandas in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: packaging in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohttp in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: pyarrow-hotfix in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: multiprocess in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: colorama in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import BartTokenizer\n",
    "\n",
    "\n",
    "def sample_dataset(dataset, fraction=0.2, seed=42):\n",
    "    \"\"\"\n",
    "    Sample a fraction of the dataset.\n",
    "    \"\"\"\n",
    "    sampled_dataset = dataset.shuffle(seed=seed)\n",
    "    num_samples = int(len(dataset) * fraction)\n",
    "    return sampled_dataset.select(range(num_samples))\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=MAX_TARGET_LENGTH, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Create the custom dataset class and data loaders\n",
    "class MultiNewsDataset(Dataset):\n",
    "    def __init__(self, tokenized_datasets, split):\n",
    "        self.dataset = tokenized_datasets[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        return {\n",
    "            'input_ids': torch.tensor(item['input_ids']),\n",
    "            'attention_mask': torch.tensor(item['attention_mask']),\n",
    "            'labels': torch.tensor(item['labels'])\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = pad_sequence([item['input_ids'] for item in batch], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = pad_sequence([item['attention_mask'] for item in batch], batch_first=True, padding_value=0)\n",
    "    labels = pad_sequence([item['labels'] for item in batch], batch_first=True, padding_value=-100)  # Assuming -100 is ignore index for labels\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "\n",
    "# 1. Load the dataset\n",
    "dataset = load_dataset(\"multi_news\")\n",
    "\n",
    "# 2. Set up the tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "# 3. Preprocess the dataset\n",
    "MAX_INPUT_LENGTH = 1024\n",
    "MAX_TARGET_LENGTH = 142\n",
    "\n",
    "# Sample 10% of each split\n",
    "train_dataset = sample_dataset(dataset['train'])\n",
    "validation_dataset = sample_dataset(dataset['validation'])\n",
    "test_dataset = sample_dataset(dataset['test'])\n",
    "\n",
    "# Create a new dataset dictionary with the sampled splits\n",
    "dataset = {\n",
    "    'train': train_dataset,\n",
    "    'validation': validation_dataset,\n",
    "    'test': test_dataset\n",
    "}\n",
    "\n",
    "\n",
    "tokenized_datasets = {}\n",
    "for split in dataset.keys():\n",
    "    tokenized_datasets[split] = dataset[split].map(preprocess_function, batched=True)\n",
    "\n",
    "train_dataset = MultiNewsDataset(tokenized_datasets, 'train')\n",
    "val_dataset = MultiNewsDataset(tokenized_datasets, 'validation')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_input_ids(input_ids, vocabulary_size):\n",
    "    if input_ids.max() >= vocabulary_size or input_ids.min() < 0:\n",
    "        raise ValueError(\"input_ids contain out-of-range values\")\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
    "        \n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, 1e-8)\n",
    "        \n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output\n",
    "\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        div_term = div_term + 1e-8  # Add small epsilon to prevent division by zero\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2).to(src.device)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3).to(tgt.device)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool().to(tgt.device)\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src))).to(src.device)\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt))).to(tgt.device)\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, optimizer, criterion,\n",
    "          scheduler, device, num_epochs):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for i, batch in enumerate(tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\")):\n",
    "            src = batch['input_ids'].to(device)\n",
    "            tgt = batch['labels'].to(device)\n",
    "\n",
    "            # Use this function in your training loop\n",
    "            check_input_ids(src, len(tokenizer))\n",
    "            check_input_ids(tgt, len(tokenizer))\n",
    "\n",
    "            output = model(src, tgt[:, :-1])\n",
    "\n",
    "            # debug_forward_pass(model, src, tgt)\n",
    "\n",
    "            loss = criterion(output.contiguous().view(-1, output.size(-1)), tgt[:, 1:].contiguous().view(-1))\n",
    "            loss.backward()\n",
    "\n",
    "            # Check model\n",
    "            for name, param in model.named_parameters():\n",
    "              if param.grad is not None:\n",
    "                  if torch.isnan(param.grad).any():\n",
    "                      print(f\"NaN gradient detected in {name}\")\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_dataloader)\n",
    "        val_loss = evaluate(model, val_dataloader, criterion, device)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            src = batch['input_ids'].to(device)\n",
    "            tgt = batch['labels'].to(device)\n",
    "            \n",
    "            output = model(src, tgt[:, :-1])\n",
    "            loss = criterion(output.contiguous().view(-1, output.size(-1)), tgt[:, 1:].contiguous().view(-1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "    \n",
    "d_model = 512\n",
    "num_heads = 4\n",
    "num_layers = 5\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "src_vocab_size = len(tokenizer)\n",
    "tgt_vocab_size = len(tokenizer)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/15: 100%|██████████| 2249/2249 [07:14<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Train Loss: 6.4543, Val Loss: 5.7575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/15: 100%|██████████| 2249/2249 [07:12<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15, Train Loss: 5.4304, Val Loss: 5.1953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/15: 100%|██████████| 2249/2249 [07:12<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15, Train Loss: 4.9396, Val Loss: 4.8410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/15: 100%|██████████| 2249/2249 [07:15<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15, Train Loss: 4.5466, Val Loss: 4.4939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/15: 100%|██████████| 2249/2249 [35:49<00:00,  1.05it/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15, Train Loss: 4.1929, Val Loss: 4.2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/15: 100%|██████████| 2249/2249 [07:12<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15, Train Loss: 3.8699, Val Loss: 3.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/15: 100%|██████████| 2249/2249 [07:15<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15, Train Loss: 3.5758, Val Loss: 3.7319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/15: 100%|██████████| 2249/2249 [07:13<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15, Train Loss: 3.3147, Val Loss: 3.5192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/15: 100%|██████████| 2249/2249 [27:43<00:00,  1.35it/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15, Train Loss: 3.0883, Val Loss: 3.3713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/15: 100%|██████████| 2249/2249 [07:19<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15, Train Loss: 2.8890, Val Loss: 3.2696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11/15: 100%|██████████| 2249/2249 [07:14<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15, Train Loss: 2.7102, Val Loss: 3.1098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12/15: 100%|██████████| 2249/2249 [07:14<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15, Train Loss: 2.5557, Val Loss: 3.0421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13/15: 100%|██████████| 2249/2249 [07:15<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15, Train Loss: 2.4167, Val Loss: 2.9581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14/15: 100%|██████████| 2249/2249 [07:15<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15, Train Loss: 2.2897, Val Loss: 2.8920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15/15: 100%|██████████| 2249/2249 [07:14<00:00,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15, Train Loss: 2.1776, Val Loss: 2.7987\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "trained_model = train(model, train_dataloader, val_dataloader,\n",
    "                      optimizer, criterion, scheduler, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage:\n",
    "# Assuming you have trained the model and have a test dataset\n",
    "torch.save(trained_model.state_dict(), \"transformer_summarizer.pth\")  # Save the trained model\n",
    "trained_model.load_state_dict(torch.load(\"transformer_summarizer.pth\"))  # Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARTICLE = \"\"\" President Joe Biden's campaign is testing head-to-head matchups of Vice President Kamala Harris against former President Donald Trump, a source familiar with the strategy told ABC News.\n",
    "\n",
    "# It's a strategic shift that comes with increasing scrutiny on whether Biden should end his 2024 reelection campaign, as a growing number Democrats are calling on him to step aside.\n",
    "\n",
    "# The source cast the move as a response to the fact that Trump has begun to attack Harris in his public statements and speeches.\n",
    "\n",
    "# MORE: Biden's solo, unscripted news conference a pivotal moment in debate rebound effort\n",
    "# \"Donald Trump shifted his stump speech. We'd be dumb not to adjust,\" the source told ABC News. \"We obviously pay close attention to what he is saying.\"\n",
    "\n",
    "# The New York Times was first to report on the campaign's actions.\n",
    "\n",
    "# A new ABC News/Washington Post/Ipsos poll found Biden continues to run evenly with Trump: Americans were divided 46-47% between Biden and Trump if the election were today.\n",
    "\n",
    "# But two-thirds of Americans, including a majority of Biden's supporters, said he should step down as his party's presumptive nominee. On the issue of age, 85% of respondents said Biden he is too old for a second term, up a few points from April.\n",
    "\n",
    "# Were Harris to replace Biden as the Democratic nominee, the poll found Harris leading Trump 49-46% among all adults and 49-47% among registered voters.\n",
    "\n",
    "# MORE: Biden and Trump tied despite debate, as 67% call for president to drop out: POLL\n",
    "# Harris recently defended Biden as she's hit the campaign trail.\n",
    "\n",
    "# \"Now, we always knew this election would be tough,\" she said on Tuesday at as the campaign launched an outreach effort to Asian American voters. \"And the past few days have been a reminder that running for president of the United States is never easy.\"\n",
    "\n",
    "# \"But the one thing we know about our President Joe Biden, is that he is a fighter, and he is the first to say, 'when you get knocked down you get back up,'\" Harris said.\n",
    "\n",
    "# ABC News' Alexandra Hutzler contributed to this report.\n",
    "# \"\"\"\n",
    "# # Tokenize input text\n",
    "# inputs = tokenizer([ARTICLE], max_length=1024, return_tensors='pt', truncation=True)\n",
    "\n",
    "# # Generate summary\n",
    "# summary_ids = trained_model.generate(inputs['input_ids'], num_beams=4, max_length=100, early_stopping=True)\n",
    "# summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: On Friday, people wearing black queued in the capital, Hanoi, to see the leader before he was buried at Mai Dich Cemetery, the final resting place for many senior leaders in Vietnam.\n",
      "Images on social media showed his hearse being paraded through the streets of the capital before the burial\n",
      "\n",
      "Generated: 'sus them into into a both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both both among a a a a an an office reportedly a a claims life– while killingst killing so things found her people— more Florida found us found them people are the- people\n"
     ]
    }
   ],
   "source": [
    "sample_text = '''On Friday, people wearing black queued in the capital, Hanoi, to see the leader before he was buried at Mai Dich Cemetery, the final resting place for many senior leaders in Vietnam.\n",
    "Images on social media showed his hearse being paraded through the streets of the capital before the burial'''\n",
    "\n",
    "def generate_text(model, src, max_length, device, tokenizer):\n",
    "    model.eval()\n",
    "    src = src.to(device)\n",
    "    \n",
    "    # Generate initial target sequence with start token\n",
    "    tgt = torch.ones(1, 1).fill_(tokenizer.bos_token_id).type(torch.long).to(device)\n",
    "    \n",
    "    for i in range(max_length - 1):\n",
    "        # Generate output using current source and target\n",
    "        output = model(src, tgt)\n",
    "        \n",
    "        # Get the next token prediction\n",
    "        next_token = output[:, -1, :].argmax(dim=-1).unsqueeze(1)\n",
    "        \n",
    "        # Append the next token to the target sequence\n",
    "        tgt = torch.cat([tgt, next_token], dim=1)\n",
    "        \n",
    "        # Stop if end of sequence token is generated\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    # Decode the generated sequence\n",
    "    generated_text = tokenizer.decode(tgt[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "# src_text = \"Translate this to French: Hello, how are you?\"\n",
    "src_tokens = tokenizer(sample_text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "generated_text = generate_text(trained_model, src_tokens, max_length=100, device=device, tokenizer=tokenizer)\n",
    "print(f\"Input: {sample_text}\")\n",
    "print()\n",
    "print(f\"Generated: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test With Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(r\"D:\\Online_Learning\\Practical_DL\\bart_large_cnn\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(r\"D:\\Online_Learning\\Practical_DL\\bart_large_cnn\")\n",
    "\n",
    "# ARTICLE = \"\"\" President Joe Biden's campaign is testing head-to-head matchups of Vice President Kamala Harris against former President Donald Trump, a source familiar with the strategy told ABC News.\n",
    "\n",
    "# It's a strategic shift that comes with increasing scrutiny on whether Biden should end his 2024 reelection campaign, as a growing number Democrats are calling on him to step aside.\n",
    "\n",
    "# The source cast the move as a response to the fact that Trump has begun to attack Harris in his public statements and speeches.\n",
    "\n",
    "# MORE: Biden's solo, unscripted news conference a pivotal moment in debate rebound effort\n",
    "# \"Donald Trump shifted his stump speech. We'd be dumb not to adjust,\" the source told ABC News. \"We obviously pay close attention to what he is saying.\"\n",
    "\n",
    "# The New York Times was first to report on the campaign's actions.\n",
    "\n",
    "# A new ABC News/Washington Post/Ipsos poll found Biden continues to run evenly with Trump: Americans were divided 46-47% between Biden and Trump if the election were today.\n",
    "\n",
    "# But two-thirds of Americans, including a majority of Biden's supporters, said he should step down as his party's presumptive nominee. On the issue of age, 85% of respondents said Biden he is too old for a second term, up a few points from April.\n",
    "\n",
    "# Were Harris to replace Biden as the Democratic nominee, the poll found Harris leading Trump 49-46% among all adults and 49-47% among registered voters.\n",
    "\n",
    "# MORE: Biden and Trump tied despite debate, as 67% call for president to drop out: POLL\n",
    "# Harris recently defended Biden as she's hit the campaign trail.\n",
    "\n",
    "# \"Now, we always knew this election would be tough,\" she said on Tuesday at as the campaign launched an outreach effort to Asian American voters. \"And the past few days have been a reminder that running for president of the United States is never easy.\"\n",
    "\n",
    "# \"But the one thing we know about our President Joe Biden, is that he is a fighter, and he is the first to say, 'when you get knocked down you get back up,'\" Harris said.\n",
    "\n",
    "# ABC News' Alexandra Hutzler contributed to this report.\n",
    "# \"\"\"\n",
    "# # Tokenize input text\n",
    "# inputs = tokenizer([ARTICLE], max_length=1024, return_tensors='pt', truncation=True)\n",
    "\n",
    "# # Generate summary\n",
    "# summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=100, early_stopping=True)\n",
    "# summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 130, but your input_length is only 67. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: People wearing black queued in the capital, Hanoi, to see the leader. He was buried at Mai Dich Cemetery, the final resting place for many senior leaders.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_path = r\"D:\\Online_Learning\\Practical_DL\\bart_large_cnn\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_summarizer(model_path, device):\n",
    "    # Load pre-trained model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "    \n",
    "    # Create summarization pipeline\n",
    "    summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=device)\n",
    "    return summarizer\n",
    "\n",
    "def summarize_news(summarizer, news_article, max_length=130, min_length=30):\n",
    "    summary = summarizer(news_article, max_length=max_length, min_length=min_length, do_sample=False)\n",
    "    return summary[0]['summary_text']\n",
    "\n",
    "# Example usage\n",
    "summarizer = load_summarizer(model_path, device)\n",
    "\n",
    "news_article = \"\"\"\n",
    "On Friday, people wearing black queued in the capital, Hanoi, to see the leader before he was buried at Mai Dich Cemetery, the final resting place for many senior leaders in Vietnam.\n",
    "Images on social media showed his hearse being paraded through the streets of the capital before the burial\n",
    "\"\"\"\n",
    "\n",
    "summary = summarize_news(summarizer, news_article)\n",
    "print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
