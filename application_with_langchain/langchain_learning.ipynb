{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pinecone\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def view_text(texts):\n",
    "    paragraphs = texts.split('\\n')\n",
    "    for paragraph in paragraphs:\n",
    "        lines = textwrap.wrap(paragraph, width=100)\n",
    "        for line in lines:\n",
    "            print(line)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load  embeddings + Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(r\"D:\\Online_Learning\\Practical_DL\\langchain_udemy\\ice_breaker\\.env\")\n",
    "\n",
    "# Retrieve the API key and index name from environment variables\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "INDEX_NAME = os.getenv(\"INDEX_NAME\")\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:3b\", use_gpu=True, max_tokens=1024, temperature=0.7, top_p=0.9)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize Pinecone vector store\n",
    "docsearch = PineconeVectorStore(index_name=INDEX_NAME, embedding=embeddings)\n",
    "\n",
    "# Initialize conversation memory with summary\n",
    "memory = ConversationSummaryMemory(llm=llm, memory_key=\"chat_history\", return_messages=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You respond only in the JSON format.'), HumanMessage(content='Top 5 countries in World by population.')]\n",
      "{\n",
      "    \"countries\": [\n",
      "        {\n",
      "            \"rank\": 1,\n",
      "            \"country\": \"China\",\n",
      "            \"population\": 1439323776\n",
      "        },\n",
      "        {\n",
      "            \"rank\": 2,\n",
      "            \"country\": \"India\",\n",
      "            \"population\": 1380004385\n",
      "        },\n",
      "        {\n",
      "            \"rank\": 3,\n",
      "            \"country\": \"United States\",\n",
      "            \"population\": 331449281\n",
      "        },\n",
      "        {\n",
      "            \"rank\": 4,\n",
      "            \"country\": \"Indonesia\",\n",
      "            \"population\": 273523615\n",
      "        },\n",
      "        {\n",
      "            \"rank\": 5,\n",
      "            \"country\": \"Pakistan\",\n",
      "            \"population\": 216565317\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# Create a chat template with system and human messages\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content='You respond only in the JSON format.'),\n",
    "        HumanMessagePromptTemplate.from_template('Top {n} countries in {area} by population.')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fill in the specific values for n and area\n",
    "messages = chat_template.format_messages(n='5', area='World')\n",
    "print(messages)  # Outputs the formatted chat messages\n",
    "\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_27856\\3164632939.py:11: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  chain1 = LLMChain(llm=llm, prompt=prompt_template1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mimport numpy as np\n",
      "from scipy import stats\n",
      "\n",
      "def linear_regression(x, y):\n",
      "    \"\"\"\n",
      "    This function performs simple linear regression.\n",
      "\n",
      "    Parameters:\n",
      "    x (list or array): Independent variable.\n",
      "    y (list or array): Dependent variable.\n",
      "\n",
      "    Returns:\n",
      "    slope (float): The slope of the best fit line.\n",
      "    intercept (float): The intercept of the best fit line.\n",
      "    \"\"\"\n",
      "\n",
      "    # Convert lists to numpy arrays for easier calculations\n",
      "    x = np.array(x)\n",
      "    y = np.array(y)\n",
      "\n",
      "    # Calculate the mean of x and y\n",
      "    mean_x = np.mean(x)\n",
      "    mean_y = np.mean(y)\n",
      "\n",
      "    # Calculate the deviations from the mean for x and y\n",
      "    dev_x = x - mean_x\n",
      "    dev_y = y - mean_y\n",
      "\n",
      "    # Calculate the slope using the formula: m = Σ[(xi-x̄)(yi-ȳ)] / Σ(xi-x̄)^2\n",
      "    numerator = np.sum(dev_x * dev_y)\n",
      "    denominator = np.sum(dev_x ** 2)\n",
      "    slope = numerator / denominator if denominator != 0 else 0\n",
      "\n",
      "    # Calculate the intercept using the formula: b = ȳ - m*x̄\n",
      "    intercept = mean_y - slope * mean_x\n",
      "\n",
      "    return slope, intercept\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m**Linear Regression Function in Python**\n",
      "=====================================\n",
      "\n",
      "The provided function `linear_regression` is a simple implementation of linear regression using the least squares method. It takes two lists or arrays `x` and `y` as input, calculates the best fit line for the data, and returns the slope and intercept of the line.\n",
      "\n",
      "**Functionality Overview**\n",
      "-------------------------\n",
      "\n",
      "This function performs the following steps:\n",
      "\n",
      "1.  **Data Preparation**: Converts the input lists to NumPy arrays for efficient numerical computations.\n",
      "2.  **Mean Calculation**: Calculates the mean of both `x` and `y` values using the `np.mean()` function.\n",
      "3.  **Deviations from Mean**: Calculates the deviations of each data point from its respective mean using subtraction.\n",
      "4.  **Slope Calculation**: Uses the formula for calculating the slope (m) in linear regression: m = Σ[(xi-x̄)(yi-ȳ)] / Σ(xi-x̄)^2, where xi and yi are individual data points, x̄ and ȳ are the means of x and y respectively.\n",
      "5.  **Intercept Calculation**: Calculates the intercept (b) using the formula: b = ȳ - m*x̄.\n",
      "\n",
      "**Step-by-Step Breakdown**\n",
      "-------------------------\n",
      "\n",
      "### Step 1: Data Preparation\n",
      "\n",
      "```python\n",
      "x = np.array(x)\n",
      "y = np.array(y)\n",
      "```\n",
      "\n",
      "Converts the input lists to NumPy arrays for efficient numerical computations.\n",
      "\n",
      "### Step 2: Mean Calculation\n",
      "\n",
      "```python\n",
      "mean_x = np.mean(x)\n",
      "mean_y = np.mean(y)\n",
      "```\n",
      "\n",
      "Calculates the mean of both `x` and `y` values using the `np.mean()` function.\n",
      "\n",
      "### Step 3: Deviations from Mean\n",
      "\n",
      "```python\n",
      "dev_x = x - mean_x\n",
      "dev_y = y - mean_y\n",
      "```\n",
      "\n",
      "Calculates the deviations of each data point from its respective mean using subtraction.\n",
      "\n",
      "### Step 4: Slope Calculation\n",
      "\n",
      "```python\n",
      "numerator = np.sum(dev_x * dev_y)\n",
      "denominator = np.sum(dev_x ** 2)\n",
      "slope = numerator / denominator if denominator != 0 else 0\n",
      "```\n",
      "\n",
      "Uses the formula for calculating the slope (m) in linear regression. If the denominator is zero, it returns a slope of zero to avoid division by zero errors.\n",
      "\n",
      "### Step 5: Intercept Calculation\n",
      "\n",
      "```python\n",
      "intercept = mean_y - slope * mean_x\n",
      "```\n",
      "\n",
      "Calculates the intercept (b) using the formula: b = ȳ - m*x̄.\n",
      "\n",
      "**Return Values**\n",
      "-----------------\n",
      "\n",
      "The function returns a tuple containing three values:\n",
      "\n",
      "*   `slope`: The slope of the best fit line.\n",
      "*   `intercept`: The intercept of the best fit line.\n",
      "*   No additional information is returned by this function.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "\n",
    "# Define the first prompt template\n",
    "prompt_template1 = PromptTemplate.from_template(\n",
    "    template='You are an experienced scientist and Python programmer. Write a function that implements the concept of {concept}.'\n",
    ")\n",
    "# Create an LLMChain using the first model and the prompt template\n",
    "chain1 = LLMChain(llm=llm, prompt=prompt_template1)\n",
    "\n",
    "# Define the second prompt template\n",
    "prompt_template2 = PromptTemplate.from_template(\n",
    "    template='Given the Python function {function}, describe it as detailed as possible.'\n",
    ")\n",
    "# Create another LLMChain using the second model and the prompt template\n",
    "chain2 = LLMChain(llm=llm, prompt=prompt_template2)\n",
    "\n",
    "# Combine both chains into a SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "\n",
    "# Invoke the overall chain with the concept \"linear regression\"\n",
    "output = overall_chain.invoke('linear regression')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m# Syntactic Parser Function\n",
      "=====================================\n",
      "\n",
      "In this example, we will use a simple recursive descent parser to parse a syntax tree for a basic grammar.\n",
      "\n",
      "```python\n",
      "import re\n",
      "\n",
      "class Node:\n",
      "    \"\"\"Represents a node in the syntax tree.\"\"\"\n",
      "    \n",
      "    def __init__(self, value):\n",
      "        self.value = value\n",
      "        \n",
      "    def __str__(self):\n",
      "        return f\"Node({self.value})\"\n",
      "\n",
      "\n",
      "class Parser:\n",
      "    \"\"\"A simple syntactic parser.\"\"\"\n",
      "    \n",
      "    def __init__(self, grammar):\n",
      "        self.grammar = grammar\n",
      "        self.tokenizer = Tokenizer()\n",
      "        \n",
      "    def parse(self, sentence):\n",
      "        \"\"\"Parses a sentence using the provided grammar.\"\"\"\n",
      "        \n",
      "        # Split the sentence into tokens\n",
      "        tokens = self.tokenizer.tokenize(sentence)\n",
      "        \n",
      "        # Start parsing at the first token\n",
      "        return self.parse_token(tokens[0])\n",
      "    \n",
      "    def parse_token(self, token):\n",
      "        \"\"\"Parses a single token.\"\"\"\n",
      "        \n",
      "        # Check if the token matches any of the grammar rules\n",
      "        for rule in self.grammar:\n",
      "            match = re.match(rule['pattern'], token)\n",
      "            if match:\n",
      "                # Recursively parse the remaining tokens\n",
      "                return Node(match.group(1))  # Return the value of the matched group\n",
      "        \n",
      "        # If no match is found, return a default node\n",
      "        return Node(\"Unknown\")\n",
      "\n",
      "\n",
      "class Tokenizer:\n",
      "    \"\"\"A simple tokenizer.\"\"\"\n",
      "    \n",
      "    def tokenize(self, sentence):\n",
      "        \"\"\"Splits a sentence into individual tokens.\"\"\"\n",
      "        \n",
      "        # Split the sentence into words\n",
      "        words = re.findall(r'\\w+', sentence)\n",
      "        \n",
      "        # Return the list of words as tokens\n",
      "        return words\n",
      "\n",
      "\n",
      "# Example usage\n",
      "if __name__ == \"__main__\":\n",
      "    # Define a simple grammar with two rules\n",
      "    grammar = [\n",
      "        {'pattern': r'hello world', 'value': 'greeting'},\n",
      "        {'pattern': r'start|end', 'value': 'terminator'}\n",
      "    ]\n",
      "    \n",
      "    parser = Parser(grammar)\n",
      "    \n",
      "    sentence = \"start hello world end\"\n",
      "    parsed_tree = parser.parse(sentence)\n",
      "    \n",
      "    print(parsed_tree)  # Output: Node(greeting)\n",
      "```\n",
      "\n",
      "This code defines a simple syntactic parser that uses a recursive descent approach to parse a sentence based on a provided grammar. The `Parser` class takes in the grammar and tokenizes the input sentence into individual tokens. It then recursively parses each token using the grammar rules, returning a syntax tree representing the parsed structure.\n",
      "\n",
      "The example usage demonstrates how to define a simple grammar with two rules (one for greetings and one for terminators) and parse an input sentence using the parser.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mThis Python code implements a basic syntactic parser that utilizes a recursive descent approach to analyze a sentence based on a predefined grammar.\n",
      "\n",
      "**Overview of the Code Structure**\n",
      "\n",
      "The provided code is structured into three main classes: `Node`, `Parser`, and `Tokenizer`. Each class plays a crucial role in parsing a sentence according to the specified grammar.\n",
      "\n",
      "1.  **`Node` Class**: This class represents individual nodes in the syntax tree, which is generated by the parser.\n",
      "2.  **`Parser` Class**: The `Parser` class serves as the primary interface for parsing sentences based on the provided grammar. It utilizes the `Tokenizer` to split the input sentence into tokens and then recursively analyzes each token using the grammar rules.\n",
      "3.  **`Tokenizer` Class**: The `Tokenizer` class is responsible for splitting the input sentence into individual tokens.\n",
      "\n",
      "**Key Components of the Parser**\n",
      "\n",
      "1.  **Grammar Definition**: A grammar defines a set of production rules that describe how to generate a syntax tree from an input sentence. In this example, the grammar consists of two rules:\n",
      "    *   The first rule matches strings like \"hello world\" and returns a node with value \"greeting\".\n",
      "    *   The second rule matches strings like \"start\" or \"end\" and returns a node with value \"terminator\".\n",
      "2.  **Tokenization**: Tokenization is the process of splitting an input sentence into individual tokens. In this example, the `Tokenizer` class uses regular expressions to split the sentence into words.\n",
      "3.  **Recursive Descent Parsing**: The parser employs a recursive descent approach to analyze each token in the sentence according to the specified grammar rules.\n",
      "\n",
      "**How the Parser Works**\n",
      "\n",
      "Here's a step-by-step explanation of how the parser works:\n",
      "\n",
      "1.  **Initialization**: The `Parser` class is initialized with a reference to the grammar and a `Tokenizer` instance.\n",
      "2.  **Sentence Tokenization**: The input sentence is tokenized into individual words using the `Tokenizer`.\n",
      "3.  **Recursive Parsing**: The parser starts parsing from the first token in the sentence.\n",
      "4.  **Grammar Rule Matching**: For each token, the parser checks if it matches any of the grammar rules defined in the grammar.\n",
      "5.  **Rule Application**: If a match is found, the corresponding value is extracted from the rule and used to create a new node in the syntax tree.\n",
      "6.  **Recursive Recursion**: The parser recursively calls itself on each remaining token in the sentence to continue parsing.\n",
      "7.  **Syntax Tree Construction**: As the parser analyzes each token, it constructs a syntax tree representing the parsed structure of the input sentence.\n",
      "\n",
      "**Example Usage and Output**\n",
      "\n",
      "The code includes an example usage that demonstrates how to define a simple grammar with two rules (one for greetings and one for terminators) and parse an input sentence using the parser. The output shows the root node of the constructed syntax tree, which represents the parsed value \"greeting\".\n",
      "\n",
      "Overall, this Python code provides a basic implementation of a syntactic parser that can be extended to support more complex grammar rules and parsing scenarios.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Define the first prompt template\n",
    "prompt_template1 = PromptTemplate.from_template(\n",
    "    template='You are an experienced scientist in NLP and Deep Learning Engineer. Write a function in Python that implements the concept of {concept}.'\n",
    ")\n",
    "# Create an LLMChain using the first model and the prompt template\n",
    "chain1 = LLMChain(llm=llm, prompt=prompt_template1)\n",
    "\n",
    "# Define the second prompt template\n",
    "prompt_template2 = PromptTemplate.from_template(\n",
    "    template='Given the Python function {function}, describe it as detailed as possible.'\n",
    ")\n",
    "# Create another LLMChain using the second model and the prompt template\n",
    "chain2 = LLMChain(llm=llm, prompt=prompt_template2)\n",
    "\n",
    "# Combine both chains into a SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "\n",
    "# Invoke the overall chain with the concept \"linear regression\"\n",
    "output = overall_chain.invoke('syntactic parser')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating React Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda\\envs\\pytorch\\lib\\site-packages\\langsmith\\client.py:351: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain.agents import Tool, AgentExecutor, initialize_agent, create_react_agent\n",
    "from langchain.tools import DuckDuckGoSearchRun, WikipediaQueryRun\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Initialize the ChatOpenAI model (gpt-4-turbo-preview) with a temperature of 0. Utilize gpt-3.5-turbo if you use the free plan\n",
    "# llm = ChatOpenAI(model_name='gpt-4-turbo-preview', temperature=0)\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:3b\", use_gpu=True, max_tokens=2048, temperature=0.5, top_p=0.95)\n",
    "\n",
    "# Define a template for answering questions\n",
    "template = '''\n",
    "Answer the following questions in English as best you can.\n",
    "Questions: {q}\n",
    "'''\n",
    "\n",
    "# Create a PromptTemplate object from the template\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "\n",
    "# Pull the react prompt from the hub\n",
    "prompt = hub.pull('hwchase17/react')\n",
    "\n",
    "# displaying information about the react prompt\n",
    "# print(type(prompt))\n",
    "# print(prompt.input_variables)\n",
    "# print(prompt.template)\n",
    "\n",
    "\n",
    "# Create tools for the agent\n",
    "\n",
    "# 1. Python REPL Tool (for executing Python code)\n",
    "python_repl = PythonREPLTool()\n",
    "python_repl_tool = Tool(\n",
    "    name='Python REPL',\n",
    "    func=python_repl.run,\n",
    "    description='Useful when you need to use Python to answer a question. You should input Python code.'\n",
    ")\n",
    "\n",
    "# 2. Wikipedia Tool (for searching Wikipedia)\n",
    "api_wrapper = WikipediaAPIWrapper()\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "wikipedia_tool = Tool(\n",
    "    name='Wikipedia',\n",
    "    func=wikipedia.run,\n",
    "    description='Useful for when you need to look up a topic, country, or person on Wikipedia.'\n",
    ")\n",
    "\n",
    "# 3. DuckDuckGo Search Tool (for general web searches)\n",
    "search = DuckDuckGoSearchRun()\n",
    "duckduckgo_tool = Tool(\n",
    "    name='DuckDuckGo Search',\n",
    "    func=search.run,\n",
    "    description='Useful for when you need to perform an internet search to find information that another tool can\\'t provide.'\n",
    ")\n",
    "\n",
    "# Combine the tools into a list\n",
    "tools = [python_repl_tool, wikipedia_tool, duckduckgo_tool]\n",
    "\n",
    "# Create a react agent with the ChatOpenAI model, tools, and prompt\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "\n",
    "# Initialize the AgentExecutor\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    max_iterations=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: Wikipedia\n",
      "Action Input: 'Napoleon Bonaparte early life\u001b[0m\u001b[33;1m\u001b[1;3mPage: Lucien Bonaparte\n",
      "Summary: Lucien Bonaparte, 1st Prince of Canino and Musignano (born Luciano Buonaparte; 21 May 1775 – 29 June 1840), was a French politician and diplomat of the French Revolution and the Consulate. He served as Minister of the Interior from 1799 to 1800 and as the president of the Council of Five Hundred in 1799.\n",
      "The third surviving son of Carlo Bonaparte and his wife Letizia Ramolino, Lucien was the younger brother of Napoleon Bonaparte. As president of the Council of Five Hundred, he was one of the participants of the Coup of 18 Brumaire that brought Napoleon to power in France.\n",
      "\n",
      "Page: Napoleon III\n",
      "Summary: Napoleon III (Charles-Louis Napoléon Bonaparte; 20 April 1808 – 9 January 1873) was the first president of France from 1848 to 1852, and the last monarch of France as the second Emperor of the French from 1852 until he was deposed on 4 September 1870.\n",
      "Prior to his reign, Napoleon III was known as Louis Napoleon Bonaparte. He was born in Paris as the son of Louis Bonaparte, King of Holland (r. 1806–1810), and Hortense de Beauharnais. Napoleon I was Louis Napoleon's paternal uncle, and one of his cousins was the disputed Napoleon II. It would only be two months following his birth that he, in accordance to Napoleon I's dynastic naming policy, would be bestowed the name of Charles-Louis Napoleon, however, shortly after, Charles was very quick to be removed from his name. Louis Napoleon Bonaparte was the first and only president of the French Second Republic, elected in 1848. He seized power by force in 1851 when he could not constitutionally be re-elected. He later proclaimed himself Emperor of the French and founded the Second Empire, reigning until the defeat of the French Army and his capture by Prussia and its allies at the Battle of Sedan in 1870.\n",
      "Napoleon III was a popular monarch who oversaw the modernization of the French economy and filled Paris with new boulevards and parks. He expanded the French colonial empire, made the French merchant navy the second largest in the world, and personally engaged in two wars. Maintaining leadership for 22 years, he was the longest-reigning French head of state since the fall of the Ancien Régime, although his reign would ultimately end upon his surrender to Otto Von Bismarck and Wilhelm I on September 2nd, 1870.\n",
      "Napoleon III commissioned a grand reconstruction of Paris carried out by prefect of the Seine, Baron Georges-Eugène Haussmann. He expanded and consolidated the railway system throughout the nation and modernized the banking system. Napoleon promoted the building of the Suez Canal and established modern agriculture, which ended famines in France and made the country an agricultural exporter. He negotiated the 1860 Cobden–Chevalier Free Trade Agreement with Britain and similar agreements with France's other European trading partners. Social reforms included giving French workers the right to strike, the right to organize, and the right for women to be admitted to a French university.\n",
      "In foreign policy, Napoleon III aimed to reassert French influence in Europe and around the world. In Europe, he allied with Britain and defeated Russia in the Crimean War (1853–1856). His regime assisted Italian unification by defeating the Austrian Empire in the Second Italian War of Independence and later annexed Savoy and Nice through the Treaty of Turin as its deferred reward. At the same time, his forces defended the Papal States against annexation by Italy. He was also favourable towards the 1859 union of the Danubian Principalities, which resulted in the establishment of the United Principalities of Moldavia and Wallachia. Napoleon doubled the area of the French colonial empire with expansions in Asia, the Pacific, and Africa. On the other hand, the intervention in Mexico, which aimed to create a Second Mexican Empire under French protection, ended in total failure.\n",
      "From 1866, Napoleon had to face the mounting power of Prussia as its minister president Otto von Bi\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: What was the name of Napoleon Bonaparte's first cousin?\n",
      "\n",
      "Thought:Action: Wikipedia\n",
      "Action Input: 'Napoleon Bonaparte cousin\u001b[0m\u001b[33;1m\u001b[1;3mPage: Pierre Napoléon Bonaparte\n",
      "Summary: Prince Pierre-Napoléon Bonaparte (11 October 1815 – 7 April 1881) was a French nobleman, revolutionary and politician, the son of Lucien Bonaparte and his second wife Alexandrine de Bleschamp. He was a nephew of Napoleon I, Joseph Bonaparte, Elisa Bonaparte, Louis Bonaparte, Pauline Bonaparte, Caroline Bonaparte and Jérôme Bonaparte.\n",
      "\n",
      "Page: Napoléon Louis Bonaparte\n",
      "Summary: Napoléon-Louis Bonaparte (11 October 1804 – 17 March 1831) was King of Holland for less than two weeks in July 1810 as Louis II (Dutch: Lodewijk II). He was a son of Louis Bonaparte (King Louis I) and Queen Hortense. His father was the younger brother of Napoleon I of France who ruled the Napoleonic Kingdom of Holland from 1806 to 1810. His mother was the daughter of Josephine de Beauharnais, Napoleon's first wife. His younger brother, Louis-Napoléon, became Emperor of the French in 1852 as Napoleon III.\n",
      "\n",
      "Page: Napoleon II\n",
      "Summary: Napoleon II (Napoléon François Joseph Charles Bonaparte; 20 March 1811 – 22 July 1832) was the disputed Emperor of the French for a few weeks in 1815. He was the son of Emperor Napoleon I and Empress Marie Louise, daughter of Emperor Francis I of Austria. Napoleon II had been Prince Imperial of France and King of Rome since birth. After the fall of his father, he lived the rest of his life in Vienna and was known in the Austrian court as Franz, Duke of Reichstadt for his adult life (from the German version of his second given name, along with a title his grandfather granted him in 1818).  He was posthumously given the nickname L'Aiglon (\"the Eaglet\").\n",
      "When Napoleon I tried to abdicate on 4 April 1814, he said that his son would rule as emperor. However, the coalition victors refused to acknowledge his son as successor, and Napoleon I was forced to abdicate unconditionally some days later. Although Napoleon II never actually ruled France, he was briefly the titular Emperor of the French after the second fall of his father. He died of tuberculosis at the age of 21.\n",
      "His cousin, Louis-Napoléon Bonaparte, founded the Second French Empire in 1852 and ruled as Emperor Napoleon III.\u001b[0m\u001b[32;1m\u001b[1;3mThe name of Napoleon Bonaparte's first cousin was Prince Pierre Napoléon Bonaparte.\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:\u001b[32;1m\u001b[1;3mThe correct response is:\n",
      "\n",
      " Thought: Question: What was the name of Napoleon Bonaparte's first cousin?\n",
      "\n",
      " Thought:Action: Wikipedia\n",
      " Action Input: 'Napoleon Bonaparte cousin'\n",
      " Observation: Page: Pierre Napoléon Bonaparte\n",
      " Summary: Prince Pierre-Napoléon Bonaparte (11 October 1815 – 7 April 1881) was a French nobleman, revolutionary and politician, the son of Lucien Bonaparte and his second wife Alexandrine de Bleschamp. He was a nephew of Napoleon I, Joseph Bonaparte, Elisa Bonaparte, Louis Bonaparte, Pauline Bonaparte, Caroline Bonaparte and Jérôme Bonaparte.\n",
      "\n",
      " Page: Napoléon Louis Bonaparte\n",
      " Summary: Napoléon-Louis Bonaparte (11 October 1804 – 17 March 1831) was King of Holland for less than two weeks in July 1810 as Louis II (Dutch: Lodewijk II). He was a son of Louis Bonaparte (King Louis I) and Queen Hortense. His father was the younger brother of Napoleon I of France who ruled the Napoleonic Kingdom of Holland from 1806 to 1810. His mother was the daughter of Josephine de Beauharnais, Napoleon's first wife. His younger brother, Louis-Napoléon, became Emperor of the French in 1852 as Napoleon III.\n",
      "\n",
      " Page: Napoleon II\n",
      " Summary: Napoleon II (Napoléon François Joseph Charles Bonaparte; 20 March 1811 – 22 July 1832) was the disputed Emperor of the French for a few weeks in 1815. He was the son of Emperor Napoleon I and Empress Marie Louise, daughter of Emperor Francis I of Austria. Napoleon II had been Prince Imperial of France and King of Rome since birth. After the fall of his father, he lived the rest of his life in Vienna and was known in the Austrian court as Franz, Duke of Reichstadt for his adult life (from the German version of his second given name, along with a title his grandfather granted him in 1818).  He was posthumously given the nickname L'Aiglon (\"the Eaglet\").\n",
      "\n",
      "When Napoleon I tried to abdicate on 4 April 1814, he said that his son would rule as emperor. However, the coalition victors refused to acknowledge his son as successor, and Napoleon I was forced to abdicate unconditionally some days later. Although Napoleon II never actually ruled France, he was briefly the titular Emperor of the French after the second fall of his father. He died of tuberculosis at the age of 21.\n",
      "\n",
      "His cousin, Louis-Napoléon Bonaparte, founded the Second French Empire in 1852 and ruled as Emperor Napoleon III.\n",
      "\n",
      " Thought: The name of Napoleon Bonaparte's first cousin was Prince Pierre Napoléon Bonaparte.\u001b[0m\u001b[33;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\u001b[32;1m\u001b[1;3mI see what you're trying to do. You want me to add a new \"Action:\" and \"Observation:\" section after the original \"Thought:\" and provide a response.\n",
      "\n",
      "Here is the revised version:\n",
      "\n",
      "Thought: The name of Napoleon Bonaparte's first cousin was Prince Pierre Napoléon Bonaparte\u001b[0mInvalid Format: Missing 'Action Input:' after 'Action:'\u001b[32;1m\u001b[1;3mYou are correct that I should have added an \"Action:\" and \"Observation:\" section to provide a clear response. Here is the revised version:\n",
      "\n",
      "Thought: The name of Napoleon Bonaparte's first cousin was Prince Pierre Napoléon Bonaparte\n",
      "Action Input: Search for more information on the subject\u001b[0m\" and \"Observation:\" section to provide a clear response. Here is the revised version:\n",
      "\n",
      "Thought: The name of Napoleon Bonaparte's first cousin was Prince Pierre Napoléon Bonaparte is not a valid tool, try one of [Python REPL, Wikipedia, DuckDuckGo Search].\u001b[32;1m\u001b[1;3mYou're correct that I should have added an \"Action Input:\" and provided a clear response. Here's the revised version:\n",
      "\n",
      "Thought: The name of Napoleon Bonaparte's first cousin was Prince Pierre Napoléon Bonaparte\n",
      "Action Input: Search for more information on the subject\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:\u001b[32;1m\u001b[1;3mThank you for pointing out the format issues and providing a clear example of how to revise the response.\n",
      "\n",
      "Here is the revised version:\n",
      "\n",
      "Thought: The name of Napoleon Bonaparte's first cousin was Prince Pierre Napoléon Bonaparte\n",
      "Action Input: Search for more information on the subject\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:\u001b[32;1m\u001b[1;3mI see what you're trying to do. You want me to add a new \"Action:\" and \"Observation:\" section after the original \"Thought:\" and provide a clear response.\n",
      "\n",
      "Here is the revised version:\n",
      "\n",
      " Thought: Question: What was the name of Napoleon Bonaparte's first cousin?\n",
      "\n",
      "Thought: Action Input: Search for more information on the subject\u001b[0m\" and \"Observation:\" section after the original \"Thought:\" and provide a clear response.\n",
      "\n",
      "Here is the revised version:\n",
      "\n",
      " Thought: Question: What was the name of Napoleon Bonaparte's first cousin?\n",
      "\n",
      "Thought: is not a valid tool, try one of [Python REPL, Wikipedia, DuckDuckGo Search].\u001b[32;1m\u001b[1;3mIt seems like you've tried to revise your response multiple times. Here's a revised version with a clear and concise format:\n",
      "\n",
      " Thought: Question: What was the name of Napoleon Bonaparte's first cousin?\n",
      " \n",
      "Action Input: Research on Napoleon Bonaparte's family members\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# question = 'Generate the first 20 numbers in the Fibonacci series.'\n",
    "# question = 'Who is the current prime minister of the UK?'\n",
    "\n",
    "question = 'Tell me about Napoleon Bonaparte early life in 4 sentences'\n",
    "output = agent_executor.invoke({\n",
    "    'input': prompt_template.format(q=question)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer the following questions in English as best you can.\n",
      "Questions: Tell me about Napoleon Bonaparte early life in 4 sentences\n",
      "\n",
      "Agent stopped due to iteration limit or time limit.\n"
     ]
    }
   ],
   "source": [
    "print(output['input'])\n",
    "print(output['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving chat sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_22720\\2460789804.py:36: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  chain = LLMChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:3b\", use_gpu=True, max_tokens=1024, temperature=0.7, top_p=0.9)\n",
    "\n",
    "def view_text(texts):\n",
    "    paragraphs = texts.split('\\n')\n",
    "    for paragraph in paragraphs:\n",
    "        lines = textwrap.wrap(paragraph, width=100)\n",
    "        for line in lines:\n",
    "            print(line)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# 2. Create memory \n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history',\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# 3. add  MessagesPlaceholder(variable_name='messages') to the prompt\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"content\", \"chat_history\"],\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a chatbot having a conversation with a human.\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"), # Where the memory will be stored.\n",
    "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 4. Add the memory to the chain\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "while True:\n",
    "    content = input('Your prompt: ')\n",
    "    if content.lower() in ['quit', 'exit', 'bye']:\n",
    "        print('Goodbye!')\n",
    "        break\n",
    "    \n",
    "    response = chain.invoke({'content': content})\n",
    "    print(response['content'])\n",
    "    # print(response['chat_history'])\n",
    "    print('')\n",
    "    print(view_text(response['text']))\n",
    "    print('-' * 50)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving chat sections in RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hello! How can I assist you today?\n",
      "Chatbot:  What is Corpus Reader?\n",
      "A Corpus Reader is a utility object in NLTK (Natural Language Toolkit) that provides a way to\n",
      "quickly create corpora and associate them with readers. It is a generic reader that can be used to\n",
      "read various types of corpus formats, such as plain-text documents, part-of-speech tagged corpora,\n",
      "and chunked corpora, among others.\n",
      "\n",
      "\n",
      "Chatbot:  What is NLTK?\n",
      "A Corpus Reader in NLTK is an object that can access and read corpora (collections of texts). The\n",
      "capabilities of a Corpus Reader include:\n",
      "* Providing information about the corpus structure\n",
      "* Allowing you to create corpora from plain text data\n",
      "* Associating readers with corpora\n",
      "* Giving hints on how to customize a CorpusReader for application-specific purposes\n",
      "Some examples of utility readers provided by NLTK include:\n",
      "- PlaintextCorpusReader (for plain-text documents)\n",
      "- TaggedCorpusReader (for simple part-of-speech tagged corpora)\n",
      "- BracketParseCorpusReader (for parenthesis-delineated parse trees)\n",
      "- ChunkedCorpusReader (for chunked and optionally tagged corpora formatted with parentheses)\n",
      "- TwitterCorpusReader (for corpora of tweets in JSON format)\n",
      "These readers can be used to quickly create corpora, which are essential for Natural Language\n",
      "Processing (NLP) tasks.\n",
      "\n",
      "\n",
      "Chatbot:  What is Gensim?\n",
      "According to the provided context, a Corpus Reader is an interface that enables programmers to read\n",
      "one or more documents from a corpus into memory. It provides various functionalities, including:\n",
      "1. Listing the absolute paths of all documents in the corpus.\n",
      "2. Opening each document with the correct encoding.\n",
      "3. Allowing access to metadata separately.\n",
      "In addition to these basic functionalities, Corpus Readers can also handle compressed corpora (such\n",
      "as Zip files) and provide extensions for reading Gzip or Bzip compression.\n",
      "The primary purpose of a Corpus Reader is to facilitate the analysis and processing of large\n",
      "collections of natural language data by providing a standardized interface for accessing and working\n",
      "with these corpora.\n",
      "\n",
      "\n",
      "Chatbot:  Exlcuding the context, what is Gensim?\n",
      "Gensim is a Python library used for topic modeling, document similarity analysis, and other natural\n",
      "language processing (NLP) tasks. It provides an efficient and convenient way to work with text data\n",
      "by offering various vectorization techniques, including TF-IDF, Latent Dirichlet Allocation (LDA),\n",
      "and more.\n",
      "\n",
      "\n",
      "Chatbot:  What is language?\n",
      "I don't know.\n",
      "\n",
      "\n",
      "Chatbot:  What is NetworkX?\n",
      "NetworkX is a Python library for creating and analyzing complex networks. It provides an efficient\n",
      "and flexible way to represent and manipulate network structures, including graphs, networks, and\n",
      "other types of relationships between objects.\n",
      "With NetworkX, you can create and edit graphs using various methods, such as adding nodes, edges,\n",
      "and attributes, and then analyze the structure and properties of the graph using various algorithms\n",
      "and functions.\n",
      "In the context of the provided code, NetworkX is used to create a graph that represents the synsets\n",
      "(set of synonyms) for a given list of words. The `graph_synsets` function creates an empty graph\n",
      "with the specified depth, and then adds edges between nodes representing different synsets using the\n",
      "wordnet corpus.\n",
      "\n",
      "\n",
      "Chatbot:  Using information in the DB, what is feature analysis?\n",
      "I don't know. The provided context doesn't explicitly define what \"feature analysis\" is, but it does\n",
      "mention terms like \"feature extraction\", which seems related to the concept of analyzing or\n",
      "processing features in a dataset. However, without further information, I'm unable to provide a\n",
      "definitive answer on what feature analysis specifically entails.\n",
      "\n",
      "\n",
      "Chatbot:  Using data in the Pinecone DB, what is Tokenization?\n",
      "Tokenization is the process of breaking down sentences or texts into individual tokens, which are\n",
      "strings of characters (such as words, punctuation marks, or special characters) that represent\n",
      "semantic information.\n",
      "\n",
      "\n",
      "Chatbot:  How many chapters in Applied Text Analysis with Python?\n",
      "I don't know.\n",
      "\n",
      "\n",
      "Chatbot:  Using data in Pinecone DB, show me how many chapters in Applied Text Analysis with Python?\n",
      "I don't know. The provided context does not mention the number of chapters in the book \"Applied Text\n",
      "Analysis with Python\".\n",
      "\n",
      "\n",
      "Chatbot:  Using provided data, what is Scikit-Learn?\n",
      "Scikit-Learn is an extension of SciPy (a scikit) that provides machine learning algorithms and tools\n",
      "for data modeling. It exposes a wide array of model families through a single, user-friendly\n",
      "interface, allowing users to easily train and utilize various models for tasks such as\n",
      "classification, regression, clustering, and more.\n",
      "\n",
      "\n",
      "Chatbot:  What is Yellowbrick?\n",
      "In the context of data visualization and machine learning, a Yellowbrick refers to a suite of visual\n",
      "diagnostic tools for analyzing and interpreting machine learning workflows. It extends the Scikit-\n",
      "Learn API, providing intuitive visualizations of feature selection, model-ling, and hyperparameter\n",
      "tuning to help steer the model selection process and find effective models for text data.\n",
      "\n",
      "\n",
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "system_template = '''\n",
    "Use the following pieces of context to answer the user's question.\n",
    "If you don't find the answer in the provided context, just respond \"I don't know.\"\n",
    "---------------\n",
    "Context: ```{context}```\n",
    "'''\n",
    "\n",
    "user_template = '''\n",
    "Question: ```{question}```\n",
    "'''\n",
    "\n",
    "messages= [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(user_template)\n",
    "]\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "# Create a conversational retrieval chain with memory\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=docsearch.as_retriever(),\n",
    "    memory=memory,\n",
    "    chain_type='stuff',  # Specify the chain type\n",
    "    combine_docs_chain_kwargs={'prompt': qa_prompt},\n",
    "    verbose=False  # Set to True to enable verbose logging for debugging\n",
    ")\n",
    "\n",
    "def ask_question(q, chain):\n",
    "    result = chain.invoke({'question': q})\n",
    "    return result\n",
    "\n",
    "print(\"Chatbot: Hello! How can I assist you today?\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in ['exit', 'quit', 'bye']:\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "    else:\n",
    "        # Use the conversational retrieval chain to get the response\n",
    "        response = ask_question(user_input, qa_chain)\n",
    "        \n",
    "        # Retrieve and print the chat history\n",
    "        print(\"Chatbot: \", user_input)\n",
    "        view_text(response['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# loading PDF, DOCX and TXT files as LangChain Documents\n",
    "def load_document(file):\n",
    "    import os\n",
    "    _, extension = os.path.splitext(file)\n",
    "\n",
    "    if extension == '.pdf':\n",
    "        from langchain_community.document_loaders import PyPDFLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = PyPDFLoader(file)\n",
    "    elif extension == '.docx':\n",
    "        from langchain_community.document_loaders import Docx2txtLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = Docx2txtLoader(file)\n",
    "    elif extension == '.txt':\n",
    "        from langchain_community.document_loaders import TextLoader\n",
    "        loader = TextLoader(file)\n",
    "    else:\n",
    "        print('Document format is not supported!')\n",
    "        return None\n",
    "\n",
    "    data = loader.load()\n",
    "    return data\n",
    "\n",
    "# splitting data in chunks\n",
    "def chunk_data(data, chunk_size=256, chunk_overlap=20):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# create embeddings using OpenAIEmbeddings() and save them in a Chroma vector store\n",
    "def create_embeddings(chunks):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")  # 512 works as well\n",
    "    # vector_store = Chroma.from_documents(chunks, embeddings)\n",
    "\n",
    "    # if you want to use a specific directory for chromadb\n",
    "    vector_store = Chroma.from_documents(chunks, embeddings, persist_directory='./application_with_langchain/mychroma_db_test')\n",
    "    return vector_store\n",
    "\n",
    "def calculate_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-3-small')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    # check prices here: https://openai.com/pricing\n",
    "    # print(f'Total Tokens: {total_tokens}')\n",
    "    # print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.00002:.6f}')\n",
    "    return total_tokens, total_tokens / 1000 * 0.00002\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading D:/Online_Learning/Practical_DL/doc/1910.13461v1.pdf\n"
     ]
    }
   ],
   "source": [
    "data = load_document('D:/Online_Learning/Practical_DL/doc/1910.13461v1.pdf')\n",
    "chunks = chunk_data(data, chunk_size=1000)\n",
    "tokens, embedding_cost = calculate_embedding_cost(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = create_embeddings(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'Ask a question about the content of your file: What is MNLI? Search in the DB'\n",
    "\n",
    "def ask_and_get_answer(vector_store, q, k=3):\n",
    "    from langchain.chains import RetrievalQA\n",
    "\n",
    "    llm = ChatOllama(model=\"llama3.2:3b\", use_gpu=True, max_tokens=2048, temperature=0.5, top_p=0.95)\n",
    "    retriever = vector_store.as_retriever()\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "    answer = chain.invoke(q)\n",
    "    return answer['result']\n",
    "\n",
    "answer = ask_and_get_answer(vector_store, q, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Creating Index with Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import pinecone\n",
    "import streamlit as st\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file):\n",
    "    import os\n",
    "    name, extension = os.path.splitext(file)\n",
    "\n",
    "    if extension == '.pdf':\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = PyPDFLoader(file)\n",
    "    elif extension == '.docx':\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = Docx2txtLoader(file)\n",
    "    elif extension == '.txt':\n",
    "        from langchain.document_loaders import TextLoader\n",
    "        loader = TextLoader(file)\n",
    "    else:\n",
    "        print('Document format is not supported!')\n",
    "        return None\n",
    "\n",
    "    data = loader.load()\n",
    "    return data\n",
    "\n",
    "\n",
    "# splitting data in chunks\n",
    "def chunk_data(data, chunk_size=1000, chunk_overlap=20):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=30, separator=\"\\n\")\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Insert to Pinecone index\n",
    "def insert_or_fetch_embeddings(index_name, chunks):\n",
    "    # importing the necessary libraries and initializing the Pinecone client\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    # loading from existing index\n",
    "    if (index_name in pc.list_indexes().names()):\n",
    "        print(f'Index {index_name} already exists. Loading embeddings ... ', end='')\n",
    "        print('Ok')\n",
    "    else:\n",
    "        # creating the index and embedding the chunks into the index \n",
    "        print(f'Creating index {index_name} and embeddings ...', end='')\n",
    "\n",
    "        # creating a new index\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=384,\n",
    "            metric='cosine',\n",
    "            spec=ServerlessSpec(\n",
    "                cloud=\"aws\",\n",
    "                region=\"us-east-1\"\n",
    "        ) \n",
    "        )\n",
    "\n",
    "        PineconeVectorStore.from_documents(chunks, embeddings, index_name=index_name)\n",
    "\n",
    "        # processing the input documents, generating embeddings using the provided `OpenAIEmbeddings` instance,\n",
    "        # inserting the embeddings into the index and returning a new Pinecone vector store object. \n",
    "        \n",
    "        print('Ok')\n",
    "\n",
    "    vector_store = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "def delete_pinecone_index(index_name='all', exclude_indexes=['documents-embeddings-index']):\n",
    "    if index_name == 'all':\n",
    "        indexes = [i for i in pc.list_indexes().names() if i not in exclude_indexes]\n",
    "        print('Deleting all indexes ... ')\n",
    "        for index in indexes:\n",
    "            print(index)\n",
    "            pc.delete_index(index)\n",
    "        print('Ok')\n",
    "    else:\n",
    "        print(f'Deleting index {index_name} ...', end='')\n",
    "        pc.delete_index(index_name)\n",
    "        print('Ok')\n",
    "\n",
    "    \n",
    "def ask_and_get_answer(vector_store, q, k=5):\n",
    "    llm = ChatOllama(model=\"llama3.2:3b\", use_gpu=True, max_tokens=2048, temperature=0.5, top_p=0.95)\n",
    "    retriever = vector_store.as_retriever(search_kwargs={'k': k})\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "    answer = chain.invoke(q)\n",
    "    return answer['result']\n",
    "\n",
    "\n",
    "# calculate embedding cost using tiktoken\n",
    "def calculate_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-3-small')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    # check prices here: https://openai.com/pricing\n",
    "    # print(f'Total Tokens: {total_tokens}')\n",
    "    # print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.00002:.6f}')\n",
    "    return total_tokens, total_tokens / 1000 * 0.00002\n",
    "\n",
    "\n",
    "# clear the chat history from streamlit session state\n",
    "def clear_history():\n",
    "    if 'history' in st.session_state:\n",
    "        del st.session_state['history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the API key and index name from environment variables\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "INDEX_NAME = os.getenv(\"INDEX_NAME\")\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading D:/Online_Learning/Practical_DL/doc/1910.13461v1.pdf\n",
      "Total Tokens: 11139\n",
      "Total cost (USD): 0.00022278\n"
     ]
    }
   ],
   "source": [
    "file_path = 'D:/Online_Learning/Practical_DL/doc/1910.13461v1.pdf'\n",
    "data = load_document(file_path)\n",
    "chunks = chunk_data(data, chunk_size=1000)\n",
    "tokens, embedding_cost = calculate_embedding_cost(chunks)\n",
    "print(\"Total Tokens:\", tokens)\n",
    "print(\"Total cost (USD):\", embedding_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index test-file-1 and embeddings ...Ok\n",
      "Finish chunking and embedding the file.\n",
      "<langchain_pinecone.vectorstores.PineconeVectorStore object at 0x000001E000EFC1C0>\n"
     ]
    }
   ],
   "source": [
    "index_name = 'test-file-1'\n",
    "vector_store = insert_or_fetch_embeddings(index_name=index_name, chunks=chunks)\n",
    "\n",
    "print(\"Finish chunking and embedding the file.\")\n",
    "print(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, MNLI stands for Multi-Genre Natural Language Inference (NLI).\n",
      "It's a bitext classification task where the goal is to predict whether one sentence entails another.\n",
      "The model fine-tunes by concatenating two sentences with an appended EOS token and passing them\n",
      "through both the BART encoder and decoder.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = 'Using provided data. What is MNLI?'\n",
    "answer = ask_and_get_answer(vector_store, q, k=3)\n",
    "view_text(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes ... \n",
      "test-file-1\n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "def delete_pinecone_index(index_name='all', exclude_indexes=['documents-embeddings-index']):\n",
    "    if index_name == 'all':\n",
    "        indexes = [i for i in pc.list_indexes().names() if i not in exclude_indexes]\n",
    "        print('Deleting all indexes ... ')\n",
    "        for index in indexes:\n",
    "            print(index)\n",
    "            pc.delete_index(index)\n",
    "        print('Ok')\n",
    "    else:\n",
    "        print(f'Deleting index {index_name} ...', end='')\n",
    "        pc.delete_index(index_name)\n",
    "        print('Ok')\n",
    "\n",
    "delete_pinecone_index(index_name='all', exclude_indexes=['documents-embeddings-index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading D:/Online_Learning/Practical_DL/doc/1910.13461v1.pdf\n"
     ]
    }
   ],
   "source": [
    "data = load_document('D:/Online_Learning/Practical_DL/doc/1910.13461v1.pdf')\n",
    "chunks = chunk_data(data, chunk_size=1000)\n",
    "tokens, embedding_cost = calculate_embedding_cost(chunks)\n",
    "index_name = 'uae-test2-index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index uae-test2-index and embeddings ...Ok\n"
     ]
    }
   ],
   "source": [
    "# Insert to Pinecone index\n",
    "def insert_or_fetch_embeddings(index_name, chunks):\n",
    "    # Initialize the model\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"WhereIsAI/UAE-Large-V1\")\n",
    "\n",
    "    # Check if the index already exists\n",
    "    if index_name in pc.list_indexes().names():\n",
    "        print(f'Index {index_name} already exists. Loading embeddings ... ', end='')\n",
    "        print('Ok')\n",
    "    else:\n",
    "        # Create the index and embed the chunks into the index\n",
    "        print(f'Creating index {index_name} and embeddings ...', end='')\n",
    "\n",
    "        # Create a new index\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=1024,\n",
    "            metric='cosine',\n",
    "            spec=ServerlessSpec(\n",
    "                cloud=\"aws\",\n",
    "                region=\"us-east-1\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        PineconeVectorStore.from_documents(chunks, embeddings, index_name=index_name)\n",
    "\n",
    "        print('Ok')\n",
    "\n",
    "    # Initialize the Pinecone vector store\n",
    "    vector_store = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "\n",
    "    return vector_store\n",
    "\n",
    "vector_store = insert_or_fetch_embeddings(index_name, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vector_store' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mvector_store\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vector_store' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
